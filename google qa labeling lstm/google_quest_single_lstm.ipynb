{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import copy\n",
    "from functools import partial\n",
    "import itertools\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import joblib\n",
    "from numba import cuda\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly_express as px\n",
    "import scipy\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先用fasttext.pkl预训练出来的词向量，然后w2vec进行训练，训练完之后再放入lstm之中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个模型操作的神奇之处在于使用了fasttext+w2vec+lstm的相应的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "gpu_no = '0' # or '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.compat.v1.Session(config=config))\n",
    "#设置程序按需占用gpu显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "SAMPLE_SUBMISSION_PATH = '/home/huarui/competitiondata/google-qa-labeling-lstm/sample_submission.csv'\n",
    "#获取对应的sample_submission.csv\n",
    "USE_DIR = '/home/huarui/competitiondata/google-qa-labeling-lstm/universal-sentence-encoder-qa/'\n",
    "EMBEDDING_PATH = '/home/huarui/competitiondata/google-qa-labeling-lstm/fasttext.pkl'\n",
    "\n",
    "TRAIN_PATH = '/home/huarui/competitiondata/google-qa-labeling-lstm/train.csv'\n",
    "TEST_PATH = '/home/huarui/competitiondata/google-qa-labeling-lstm/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "num_targets = 30\n",
    "target_names = [\n",
    "    'question_asker_intent_understanding',\n",
    "    'question_body_critical',\n",
    "    'question_conversational',\n",
    "    'question_expect_short_answer',\n",
    "    'question_fact_seeking',\n",
    "    'question_has_commonly_accepted_answer',\n",
    "    'question_interestingness_others',\n",
    "    'question_interestingness_self',\n",
    "    'question_multi_intent',\n",
    "    'question_not_really_a_question',\n",
    "    'question_opinion_seeking',\n",
    "    'question_type_choice',\n",
    "    'question_type_compare',\n",
    "    'question_type_consequence',\n",
    "    'question_type_definition',\n",
    "    'question_type_entity',\n",
    "    'question_type_instructions',\n",
    "    'question_type_procedure',\n",
    "    'question_type_reason_explanation',\n",
    "    'question_type_spelling',\n",
    "    'question_well_written',\n",
    "    'answer_helpful',\n",
    "    'answer_level_of_information',\n",
    "    'answer_plausible',\n",
    "    'answer_relevance',\n",
    "    'answer_satisfaction',\n",
    "    'answer_type_instructions',\n",
    "    'answer_type_procedure',\n",
    "    'answer_type_reason_explanation',\n",
    "    'answer_well_written']\n",
    "\n",
    "train_df['question'] = train_df['question_title'] + ' ' + train_df['question_body']\n",
    "test_df['question'] = test_df['question_title'] + ' ' + test_df['question_body']\n",
    "#注意是question中的title与body进行融合\n",
    "n_splits = 5\n",
    "n_epochs = 9\n",
    "batch_size = 128\n",
    "\n",
    "max_q_len = 512\n",
    "max_a_len = 512\n",
    "\n",
    "updates_per_epoch = 100\n",
    "mu = 0.9\n",
    "\n",
    "embed_size = 300\n",
    "max_features = 60000\n",
    "\n",
    "seed = 1029\n",
    "device = torch.device('cuda')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "#将不同形式的单词转换为同一形式\n",
    "#比如create和created都得到creat\n",
    "lc = LancasterStemmer()\n",
    "sb = SnowballStemmer('english')\n",
    "#snowballstemmer是一款非常瘦小的语言转换库，支持15种语言\n",
    "#'english'为其中一种语言\n",
    "#这里面都使用的nltk,nltk是python很强大的第三方库，可以很方便的\n",
    "#完成很多NLP的任务，包括分词、词性标注、命名实体标注ner及句法分析\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "#过滤掉所有的警告信息\n",
    "random.seed(seed)\n",
    "#上面的seed = 1029，所以使用random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#确定性能的变化是来自模型还是数据集的变化，或者仅仅是\n",
    "#一些新的随机样本点带来的结果\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "#获取对应的fasttest.pkl的相应的文件内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   qa_id                                     question_title  \\\n",
      "0      0  What am I losing when using extension tubes in...   \n",
      "1      1  What is the distinction between a city and a s...   \n",
      "2      2  Maximum protusion length for through-hole comp...   \n",
      "3      3              Can an affidavit be used in Beit Din?   \n",
      "4      5       How do you make a binary image in Photoshop?   \n",
      "\n",
      "                                       question_body question_user_name  \\\n",
      "0  After playing around with macro photography on...               ysap   \n",
      "1  I am trying to understand what kinds of places...      russellpierce   \n",
      "2  I'm working on a PCB that has through-hole com...          Joe Baker   \n",
      "3  An affidavit, from what i understand, is basic...         Scimonster   \n",
      "4  I am trying to make a binary image. I want mor...            leigero   \n",
      "\n",
      "                                  question_user_page  \\\n",
      "0         https://photo.stackexchange.com/users/1024   \n",
      "1           https://rpg.stackexchange.com/users/8774   \n",
      "2  https://electronics.stackexchange.com/users/10157   \n",
      "3       https://judaism.stackexchange.com/users/5151   \n",
      "4  https://graphicdesign.stackexchange.com/users/...   \n",
      "\n",
      "                                              answer answer_user_name  \\\n",
      "0  I just got extension tubes, so here's the skin...           rfusca   \n",
      "1  It might be helpful to look into the definitio...     Erik Schmidt   \n",
      "2  Do you even need grooves?  We make several pro...      Dwayne Reid   \n",
      "3  Sending an \"affidavit\" it is a dispute between...    Y     e     z   \n",
      "4  Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n",
      "\n",
      "                                    answer_user_page  \\\n",
      "0         https://photo.stackexchange.com/users/1917   \n",
      "1           https://rpg.stackexchange.com/users/1871   \n",
      "2  https://electronics.stackexchange.com/users/64754   \n",
      "3       https://judaism.stackexchange.com/users/4794   \n",
      "4  https://graphicdesign.stackexchange.com/users/...   \n",
      "\n",
      "                                                 url   category  ...  \\\n",
      "0  http://photo.stackexchange.com/questions/9169/...  LIFE_ARTS  ...   \n",
      "1  http://rpg.stackexchange.com/questions/47820/w...    CULTURE  ...   \n",
      "2  http://electronics.stackexchange.com/questions...    SCIENCE  ...   \n",
      "3  http://judaism.stackexchange.com/questions/551...    CULTURE  ...   \n",
      "4  http://graphicdesign.stackexchange.com/questio...  LIFE_ARTS  ...   \n",
      "\n",
      "  answer_helpful  answer_level_of_information  answer_plausible  \\\n",
      "0       1.000000                     0.666667          1.000000   \n",
      "1       0.888889                     0.555556          0.888889   \n",
      "2       0.777778                     0.555556          1.000000   \n",
      "3       0.833333                     0.333333          0.833333   \n",
      "4       1.000000                     0.666667          1.000000   \n",
      "\n",
      "   answer_relevance  answer_satisfaction  answer_type_instructions  \\\n",
      "0          1.000000             0.800000                       1.0   \n",
      "1          0.888889             0.666667                       0.0   \n",
      "2          1.000000             0.666667                       0.0   \n",
      "3          1.000000             0.800000                       0.0   \n",
      "4          1.000000             0.800000                       1.0   \n",
      "\n",
      "   answer_type_procedure  answer_type_reason_explanation  answer_well_written  \\\n",
      "0               0.000000                        0.000000             1.000000   \n",
      "1               0.000000                        0.666667             0.888889   \n",
      "2               0.333333                        1.000000             0.888889   \n",
      "3               0.000000                        1.000000             1.000000   \n",
      "4               0.000000                        1.000000             1.000000   \n",
      "\n",
      "                                            question  \n",
      "0  What am I losing when using extension tubes in...  \n",
      "1  What is the distinction between a city and a s...  \n",
      "2  Maximum protusion length for through-hole comp...  \n",
      "3  Can an affidavit be used in Beit Din? An affid...  \n",
      "4  How do you make a binary image in Photoshop? I...  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "$$$$$$$$$$$$$$$$$$$$$\n",
      "   qa_id                                     question_title  \\\n",
      "0     39  Will leaving corpses lying around upset my pri...   \n",
      "1     46         Url link to feature image in the portfolio   \n",
      "2     70  Is accuracy, recoil or bullet spread affected ...   \n",
      "3    132     Suddenly got an I/O error from my external HDD   \n",
      "4    200  Passenger Name - Flight Booking Passenger only...   \n",
      "\n",
      "                                       question_body question_user_name  \\\n",
      "0  I see questions/information online about how t...              Dylan   \n",
      "1  I am new to Wordpress. i have issue with Featu...                Anu   \n",
      "2  To experiment I started a bot game, toggled in...             Konsta   \n",
      "3  I have used my Raspberry Pi as a torrent-serve...           robbannn   \n",
      "4  I have bought Delhi-London return flights for ...               Amit   \n",
      "\n",
      "                                  question_user_page  \\\n",
      "0       https://gaming.stackexchange.com/users/64471   \n",
      "1    https://wordpress.stackexchange.com/users/72927   \n",
      "2       https://gaming.stackexchange.com/users/37545   \n",
      "3  https://raspberrypi.stackexchange.com/users/17341   \n",
      "4       https://travel.stackexchange.com/users/29089   \n",
      "\n",
      "                                              answer answer_user_name  \\\n",
      "0  There is no consequence for leaving corpses an...        Nelson868   \n",
      "1  I think it is possible with custom fields.\\n\\n...            Irina   \n",
      "2  You do not have armour in the screenshots. Thi...   Damon Smithies   \n",
      "3  Your Western Digital hard drive is disappearin...      HeatfanJohn   \n",
      "4  I called two persons who work for Saudia (tick...    Nean Der Thal   \n",
      "\n",
      "                                   answer_user_page  \\\n",
      "0      https://gaming.stackexchange.com/users/97324   \n",
      "1   https://wordpress.stackexchange.com/users/27233   \n",
      "2      https://gaming.stackexchange.com/users/70641   \n",
      "3  https://raspberrypi.stackexchange.com/users/1311   \n",
      "4      https://travel.stackexchange.com/users/10051   \n",
      "\n",
      "                                                 url    category  \\\n",
      "0  http://gaming.stackexchange.com/questions/1979...     CULTURE   \n",
      "1  http://wordpress.stackexchange.com/questions/1...  TECHNOLOGY   \n",
      "2  http://gaming.stackexchange.com/questions/2154...     CULTURE   \n",
      "3  http://raspberrypi.stackexchange.com/questions...  TECHNOLOGY   \n",
      "4  http://travel.stackexchange.com/questions/4704...     CULTURE   \n",
      "\n",
      "                            host  \\\n",
      "0       gaming.stackexchange.com   \n",
      "1    wordpress.stackexchange.com   \n",
      "2       gaming.stackexchange.com   \n",
      "3  raspberrypi.stackexchange.com   \n",
      "4       travel.stackexchange.com   \n",
      "\n",
      "                                            question  \n",
      "0  Will leaving corpses lying around upset my pri...  \n",
      "1  Url link to feature image in the portfolio I a...  \n",
      "2  Is accuracy, recoil or bullet spread affected ...  \n",
      "3  Suddenly got an I/O error from my external HDD...  \n",
      "4  Passenger Name - Flight Booking Passenger only...  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "print('$$$$$$$$$$$$$$$$$$$$$')\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\", \"couldn't\": \"could not\",\n",
    "                 \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\",\n",
    "                 \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                 \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\",\n",
    "                 \"i'd\": \"I had\", \"i'll\": \"I will\", \"i'm\": \"I am\", \"isn't\": \"is not\",\n",
    "                 \"it's\": \"it is\", \"it'll\": \"it will\", \"i've\": \"I have\", \"let's\": \"let us\",\n",
    "                 \"mightn't\": \"might not\", \"mustn't\": \"must not\", \"shan't\": \"shall not\",\n",
    "                 \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\",\n",
    "                 \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n",
    "                 \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n",
    "                 \"they've\": \"they have\", \"we'd\": \"we would\", \"we're\": \"we are\",\n",
    "                 \"weren't\": \"were not\", \"we've\": \"we have\", \"what'll\": \"what will\",\n",
    "                 \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                 \"where's\": \"where is\", \"who'd\": \"who would\", \"who'll\": \"who will\",\n",
    "                 \"who're\": \"who are\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                 \"won't\": \"will not\", \"wouldn't\": \"would not\", \"you'd\": \"you would\",\n",
    "                 \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
    "                 \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\": \" will\", \"tryin'\": \"trying\"}\n",
    "\n",
    "\n",
    "def replace_typical_misspell(text: str) -> str:\n",
    "    misspell_re = re.compile('(%s)' % '|'.join(misspell_dict.keys()))\n",
    "    #re.compile函数用于编译正则表达式，生成一个pattern对象\n",
    "    #中间的%s代表赋值用法,赋值的内容为'|'.join(misspell_dict.keys())\n",
    "    #正则表达式中的'|'表示或，这里代表的含义为能够匹配第一个keys()\n",
    "    #或第二个keys()或第三个keys()依次类推\n",
    "    \n",
    "    #''.join([a,b]),将[a,b]字符串进行连接\n",
    "    def replace(match):\n",
    "        return misspell_dict[match.group(0)]\n",
    "    #match.group(0)就是匹配正则表达式的整体结果\n",
    "    #misspell_dict[match.group(0)]是匹配正则式之后进行替换的结果字符串\n",
    "\n",
    "    return misspell_re.sub(replace, text)\n",
    "    #1.先去replace之中找寻对应的替换规则，对text字符串\n",
    "    #使用replace函数进行替换\n",
    "    #2.进入replace函数之中以后，再使用misspell_dict[match.group(0)]\n",
    "    #这里面调用match.group(0)之时使用上面的misspell_re进行正则表达式定义\n",
    "    #3.正则表达式匹配出来之后，使用misspell_dict找寻替换的对应内容\n",
    "    #然后返回最外面的misspell_re.sub替换层，将text所有内容都进行替换\n",
    "    \n",
    "    #!!!这里的replace函数调用的就是外面的内容misspell_re对应的正则\n",
    "    #替换的内容\n",
    "\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
    "          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
    "          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n",
    "          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n",
    "          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n",
    "          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n",
    "          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text)\n",
    "    for punct in puncts + list(string.punctuation):\n",
    "        if punct in text:\n",
    "            text = text.replace(punct, f' {punct} ')\n",
    "    #这里本身字符串为'你好!nihao?'，经过replace替换之后\n",
    "    #对应的字符串为'你好 ! nihao ?' ,这里的{punct}内容实际上就是前面\n",
    "    #标注的punct的内容，使用f正则表达式之后，中间的内容插入了空格\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_numbers(text: str) -> str:\n",
    "    return re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = replace_typical_misspell(text)\n",
    "    text = clean_text(text)\n",
    "    text = clean_numbers(text)\n",
    "    text = text.strip()\n",
    "    #text.strip()去除字符串中相应的空格\n",
    "    return text\n",
    "#预处理措施很到位，去除标点符号，对应的时态以及相应的语态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['question', 'answer']\n",
    "train_texts = train_df[text_columns].applymap(preprocess_text).values\n",
    "#preprocess_text为上面刚刚定义过的函数内容\n",
    "#applymap:将函数作用于DataFrame中的所有元素\n",
    "\n",
    "test_texts = test_df[text_columns].applymap(preprocess_text).values\n",
    "all_texts = list(itertools.chain(*train_texts, *test_texts))\n",
    "#你想在多个对象执行相同的操作，但是这些对象在不同的容器中，\n",
    "#你希望代码在不失可读性的情况下避免写重复的循环\n",
    "#也就是说这里的train_texts和test_texts形成了一个共同的容器\n",
    "#放在all_texts之中进行相同的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what am i losing when using extension tubes instead of a macro lens  ?   after playing around with macro photography on  -  the  -  cheap   (  read  :   reversed lens  ,   rev  .   lens mounted on a straight lens  ,   passive extension tubes  )    ,   i would like to get further with this  .   the problems with the techniques i used is that focus is manual and aperture control is problematic at best  .   this limited my setup to still subjects   (  read  :   dead insects  )   now  ,   as spring is approaching  ,   i want to be able to shoot live insects  .   i believe that for this  ,   autofocus and settable aperture will be of great help  .  \n",
      "\n",
      "so  ,   one obvious but expensive option is a macro lens   (  say  ,   ef  mm macro  )   however  ,   i am not really interested in yet another prime lens  .   an alternative is the electrical extension tubes  .  \n",
      "\n",
      "except for maximum focusing distance  ,   what am i losing when using tubes   (  coupled with a fine lens  ,   say ef   -     /     .     )   instead of a macro lens  ?\n",
      "i just got extension tubes  ,   so here  '  s the skinny  .  \n",
      "\n",
      "\n",
      "    .    .    .  what am i losing when using tubes  .    .    .    ?  \n",
      "\n",
      "\n",
      "a very considerable amount of light  !    increasing that distance from the end of the lens to the sensor can cut your light several stops  .    combined with the fact that you will usually shoot stopped down   -   expect to need to increase your iso considerably  .  \n",
      "\n",
      "the fact the macro  '  s are usually considered very very sharp  ,   although i believe that    -   mm    .    is supposed to be quite sharp  .  \n",
      "\n",
      "the ultra low distortion typical of many macros  .  \n",
      "\n",
      "i would not worry too much about the bokeh since the dof will still be quite limited  .  \n",
      "\n",
      "coupled on my  mm  ,   a full  mm  '  ish extension tube results in a dof of about a couple inches in front of the lens  .    on my    -     ,   its probably around    -    feet in front of the lens to about a foot in front of the lens  .\n",
      "what is the distinction between a city and a sprawl  /  metroplex  .    .    .   between downtown and a commercial district  ?   i am trying to understand what kinds of places the spam values on p   refer to in the  th edition main book for shadowrun  .  \n",
      "\n",
      "per p    ,   a sprawl is a plex  ,   a plex is a   \"  metropolitan complex  ,   short for metroplex  \"    .   per google a metroplex is   \"   a very large metropolitan area  ,   especially one that is an aggregation of two or more cities  \"    .    a city downtown and sprawl downtown would tend to have similar densities  ,   but for some reason the sprawl   (  which includes suburbs  ?    )   has a higher spam zone noise rating   (  p    )    .    similarly  ,   I had think of a downtown as being more dense and noisy   (  e  .  g  .   office buildings and street vendors  )   than a commercial district  ,   e  .  g  .   an outdoor mall  .    the noise ratings make me think that i am thinking about this incorrectly  .   what is a better way of thinking of them  ?\n",
      "it might be helpful to look into the definition of spam zone  :  \n",
      "\n",
      "  (  p  .     )   spam zone  :   an area flooded with invasive and  /  or viral ar advertising  ,   causing noise  .  \n",
      "\n",
      "because a metroplex has so many marketing targets  ,   it seems a safe assumption that marketers would drown the plex with spam  .   spam from the less dense areas would bleed into the urban cores  .   a smaller city with less urban  /  suburban territory surrounding it ostensibly would not have as much spam  .\n",
      "maximum protusion length for through  -  hole component pins I am working on a pcb that has through  -  hole components on both sides of the board  .   the   \"  top  \"   side of the board is mounted flush to a delrin plastic block   (  the only top  -  side component is a gas sensor that is fed air samples through hose fittings in the plastic block  )    .  \n",
      "\n",
      "the flush mounting means that i have to add grooves to the plastic block to accommodate the soldered pins of the bottom  -  side components  .   assuming a standard    .     \"   thickness fr  board  ,   how deep do i need to make the grooves in the plastic block  ?   the only thing i could find is this nasa workmanship standard that states    .   mm to    .   mm  ,   but I am not sure if that will always hold true  .\n",
      "do you even need grooves  ?    we make several products using through  -  hole components that are intended to mount using vhb double  -  sided foam tape  .    the boards are    .     \"   thick double  -  sided with pth and we use a table  -  top vertical belt sander to bring the component leads almost flush with the solder mask  .    in other words  ,   the solder mask is not touched by the sand paper but the leads are all sanded flat and sitting just proud of the solder mask  .  \n",
      "\n",
      "this works well for small boards  .  \n",
      "\n",
      "for what it is worth  ,   there are commercial machines available that use a rotary saw blade to do the same thing  .    the board is held horizontal in a mounting   /   clamping system on the base and the saw motor is vertical on a sliding x  -  y mechanism  .    the saw blade simply cuts all of the leads almost flush with the board surface  .    \n",
      "\n",
      "this system is suited for boards of all sizes but especially for those boards larger than can be handled easily to be sanded with the belt sander  .  \n",
      "\n",
      "also note that these techniques are suitable only for pc boards with plated  -  through holes  .\n",
      "can an affidavit be used in beit din  ?   an affidavit  ,   from what i understand  ,   is basically a signed document given by a witness to be used as evidence in a trial  ,   without the witness themselves needing to take a stand  .  \n",
      "\n",
      "can an affidavit be used in beit din  ?   or must witnesses take the stand in person for their testimony to count  ?  \n",
      "\n",
      "  (  in case I am misunderstanding what exactly an affidavit is  ,   simply treat it as a signed document by a witness with their testimony  .    )\n",
      "sending an   \"  affidavit  \"   it is a dispute between rashi and rabbeinu tam  .  \n",
      "\n",
      "devarim    :     :  \n",
      "\n",
      "\n",
      "  לא יקום עד אחד באיש לכל עון ולכל חטאת בכל חטא אשר יחטא על פי שני עדים או על פי שלשה עדים יקום דבר\n",
      "\n",
      "\n",
      "rashi  :  \n",
      "\n",
      "\n",
      "  ולא שיכתבו עדותם באגרת וישלחו לבית דין\n",
      "  \n",
      "  and not that they write their testimony in a letter and send it to beis din\n",
      "\n",
      "\n",
      "tosefos bava basra  a   (  continued from  b  )    :  \n",
      "\n",
      "\n",
      "  ועוד אומר ר  \"  י ששמע מן ר  \"  ת שנוהגים לשלח העדים עדותם באיגרת לב  \"  ד וחשיב עדות והא דדרשינן בספרי  .   מפיהם ולא מפי כתבם לא אתא אלא למעוטי דוקא אלם שאינו בר הגדה אבל ראוי להגדה אין הגדה מעכבת בו \n",
      "  \n",
      "  r  \"  i said that he heard from rabbeinu tam that the custom is to send testimony by a letter and it is considered   [  valid  ]   testimony  .    and that which it expounds in the sifre   \"  from their mouths and not from their writing  \"   is only coming to exclude a mute who is not able to speak  ,   but someone who is able to speak does not need to speak  .  \n",
      "\n",
      "\n",
      "rambam concludes it is not allowed  ,   but in monetary law the chachomim enacted that it would be accepted in order to not prohibit the ability of people to secure loans   (  hilchos edus    :     )  \n",
      "\n",
      "\n",
      "  דין תורה שאין מקבלין עדות  ,   לא בדיני ממונות ולא בדיני נפשות  ,   אלא מפי העדים  :    שנאמר   \"  על פי שניים עדים  \"     (  דברים יז  ,  ו  )    -    -  מפיהם  ,   ולא מכתב ידן  .    אבל מדברי סופרים שחותכין דיני ממונות בעדות שבשטר  ,   אף על פי שאין העדים קיימין  ,   כדי שלא תנעול דלת בפני לווין  .\n",
      "how do you make a binary image in photoshop  ?   i am trying to make a binary image  .   i want more than just the look of the image to be black  /  white  ,   but i want the actual file to be a binary file  .   every pixel should be either black  ,   or white  .   \n",
      "\n",
      "i do not just want a monochrome image  .   i cannot have varying shades of gray  ,   every pixel needs to be black or white  .  \n",
      "\n",
      "is this possible  ?   i looked under image   >   mode but nothing there seems to indiciate a binary style image  .\n",
      "check out image trace in adobe illustrator  .   \n",
      "\n",
      "i like using python and pil  ,   however  .  \n",
      "\n",
      "from pil import image\n",
      "image  _  file   =   image  .  open  (    \"  myimage  .  bmp  \"    )   \n",
      "image  _  file   =   image  _  file  .  convert  (    '     '    )     #   convert\n",
      "image  _  file  .  save  (   aresult  .  bmp  '    )\n",
      "column grouping with title in datatables i am creating an html table with jquery  '  s datatables plug  -  in  .   i would like to know if there is a way to group a number of columns together with a title which describes what the grouping represents  .   \n",
      "\n",
      "in my specific case  ,     of my columns will display an address   (  street  ,   city  ,   state  )    .   I had like them to have an additional border around just those columns with a title that indicates to the end  -  user that those   columns represent the address  .  \n",
      "\n",
      "I am considering just changing the background color of those   columns which will do the job but I am wanting the address title to be displayed  .   I have seen that rows can be grouped together and sections divided by separators  ;   however  ,   i have not yet come across anything that makes it possible to group columns together  .\n",
      "to show group headers see this example   (  build the   &  lt  ;  thead  &  gt  ;   element using colspan and rowspan  )    .  \n",
      "\n",
      "to show borders around you group build css classes with border  -  left and border  -  right and use the columns  .  classname option for the first and last column in your group  .\n",
      "core file size with ulimit my question is probably not related to ubuntu in particular  ,   but since my desktop running this os  ,   i came to this forum  .  \n",
      "\n",
      "i am trying to change the core file size using ulimit   -  c command as follows  :  \n",
      "\n",
      "  $   ulimit   -  a\n",
      "core file size            (  blocks  ,     -  c  )    \n",
      "data seg size             (  kbytes  ,     -  d  )   unlimited\n",
      "scheduling priority               (    -  e  )    \n",
      "file size                 (  blocks  ,     -  f  )   unlimited\n",
      "pending signals                   (    -  i  )    \n",
      "max locked memory         (  kbytes  ,     -  l  )    \n",
      "max memory size           (  kbytes  ,     -  m  )   unlimited\n",
      "open files                        (    -  n  )    \n",
      "pipe size              (    bytes  ,     -  p  )    \n",
      "posix message queues       (  bytes  ,     -  q  )    \n",
      "real  -  time priority                (    -  r  )    \n",
      "stack size                (  kbytes  ,     -  s  )    \n",
      "cpu time                 (  seconds  ,     -  t  )   unlimited\n",
      "max user processes                (    -  u  )    \n",
      "virtual memory            (  kbytes  ,     -  v  )   unlimited\n",
      "file locks                        (    -  x  )   unlimited\n",
      "\n",
      "\n",
      "changing the limitation  :  \n",
      "\n",
      "  $   ulimit   -  c unlimited\n",
      "\n",
      "\n",
      "observing the result  :  \n",
      "\n",
      "  $   ulimit   -  a\n",
      "core file size            (  blocks  ,     -  c  )   unlimited\n",
      "data seg size             (  kbytes  ,     -  d  )   unlimited\n",
      "scheduling priority               (    -  e  )    \n",
      "file size                 (  blocks  ,     -  f  )   unlimited\n",
      "pending signals                   (    -  i  )    \n",
      "max locked memory         (  kbytes  ,     -  l  )    \n",
      "max memory size           (  kbytes  ,     -  m  )   unlimited\n",
      "open files                        (    -  n  )    \n",
      "pipe size              (    bytes  ,     -  p  )    \n",
      "posix message queues       (  bytes  ,     -  q  )    \n",
      "real  -  time priority                (    -  r  )    \n",
      "stack size                (  kbytes  ,     -  s  )    \n",
      "cpu time                 (  seconds  ,     -  t  )   unlimited\n",
      "max user processes                (    -  u  )    \n",
      "virtual memory            (  kbytes  ,     -  v  )   unlimited\n",
      "file locks                        (    -  x  )   unlimited\n",
      "\n",
      "\n",
      "indeed the limit is changed  .  \n",
      "however  ,   when i open another terminal and check the value  ,   i still see zero value in core file size  .  \n",
      "\n",
      "questions  :  \n",
      "\n",
      "\n",
      "are changes made using ulimit command affect only current process  ,   i  .  e  .   in this case the bash  ?  \n",
      "i launch a program from shell as a fore  -   or  -  background process  .   does the ulimit change apply for new process   ?  \n",
      "how can i make that all user processes are affected with this configuration   ?\n",
      "ulimit is a shell builtin  ,   and thus only affects the current shell  ,   and processes started by that shell  :  \n",
      "\n",
      "  $   type ulimit\n",
      "ulimit is a shell builtin\n",
      "\n",
      "\n",
      "from man ulimit  :  \n",
      "\n",
      "the  ulimit  utility  shall  set  or report the file  -  size writing limit\n",
      "imposed on files written by the shell and its child processes   (  files of\n",
      "any  size  may be read  )    .   only a process with appropriate privileges can\n",
      "increase the limit  .  \n",
      "\n",
      "\n",
      "so  ,   yes  ,   child processes are affected  .  \n",
      "\n",
      "to set limits permanently or for all processes  ,   edit   /  etc  /  security  /  limits  .  conf and reboot  .   the examples in the manpage are fairly good  .   you just need to add something like  :  \n",
      "\n",
      "username   -   core unlimited\n",
      "how do you get your steam games to run on ubuntu through wine or something similar  ?   ok  ,   i was kind of surprised that this had not been asked here before  ,   but maybe it is too technical for this site  .   you guys decide  .  \n",
      "\n",
      "I have heard lots of different stories about setting up wine on ubuntu  ,   winetricks  ,   playonlinux etc  .    ,   but never a   '  this is the best way to do it for steam and steam games  '   thread  .  \n",
      "\n",
      "so has anyone had any real success getting their steam games to run on ubuntu through wine or something similar  ?   if so  ,   could we get some specific steps  ?\n",
      "you could try http  :    /    /  transgaming  .  com  /     (  cedega  )    .    i did this in the past and it worked fine  ,   but you have to pay for it   -     :    \\\n",
      "high memory usage windows server  r  on vmware we are running windows server  r  on vmware and are experiencing extremely high memory use when nothing is running  .   the server memory usages slowly creeps up to    -     %    .   the server is configured to use  gb of memory  .   is there some setting we should be using so the server can better manage it is memory usage  .   it is behaving as if there is a memory leak  .\n",
      "well to better assist with your question  :  \n",
      "\n",
      "  -  which vmware product   &  amp  ;   version\n",
      "  -  what is the windows   r  vm running  ?  \n",
      "  -  where are you seeing the high memory usage  ?   windows or your vmware product  ?\n",
      "how do you grapple in dead rising    ?   i just started playing dead rising   on the xbox one  .   i got to the first grapple  ,   and i cannot figure out what I am supposed to do  .   the top of the screen says   \"  perform a gesture to escape a grapple  \"     -   what does   \"  gesture  \"   mean in this context  ?   is it something to do with kinect  ?\n",
      "you can also switch gesture based grapple escapes off in the kinect settings of dead rising     (  along with all the rest of the kinect features  )   so that you have to do qte type button presses instead  .\n",
      "how to compile and install programs from source this is an issue that really limits my enjoyment of linux  .   if the application is not on a repository or if it does not have an installer script  ,   then i really struggle where and how to install an application from source  .  \n",
      "\n",
      "comparatively to windows  ,   it is easy  .   you are   (  pretty much  )   required to use an installer application that does all of the work in a wizard  .   with linux  .    .    .   not so much  .  \n",
      "\n",
      "so  ,   do you have any tips or instructions on this or are there any websites that explicitly explain how  ,   why and where to install linux programs from source  ?\n",
      "i think it is just best to read the documentation coming with that specific program or application that you are wanting to install  .  \n",
      "usually there are readmes  /  readmes inside the tarballs   (  the application source archive which you can usually download  )   or maybe even install files to read and learn about what is the preferred way of installing said application  .  \n",
      "in short  :   rtfm   ;    )\n",
      "sleeping spid blocking other transactions I am really having trouble tracking down some blocking we are experiencing  .  \n",
      "\n",
      "the root blocking spid  '  s status is   '  sleeping  '    ,   the cmd is   '  awaiting command  '    ,   and the sqltext is   '  set transaction isolation level read committed  '    .  \n",
      "\n",
      "when i view the top transactions by blocked transactions count report  ,   the blocking sql statement is   '    -    -    '    .  \n",
      "\n",
      "I have performed a trace on the sql and when the blocking happens tracing the root blocking spid but it has not really led me anywhere  .    the last trace statement is the same as the sqltext above   '  set transaction isolation level read committed  '    .  \n",
      "\n",
      "I have checked all the related sprocs i can find to make sure they have try  /  catch begin tran  /  commit tran  /  rollback tran statements   (  we use sprocs for everything so there are no standalone statements being ran  )    .    this issue just started happening over the last   hours and no one is claiming to have made any changes to the system  .  \n",
      "\n",
      "solution  .    one of our seldomly used sprocs had an error with an insert   (  number of columns did not match  )    ,   but we are still confused on what exactly was happening  .  \n",
      "\n",
      "when looking at all the trace information  ,   the exec statement for this sproc was listed at times  ,   but never just before the block happened on the blockking spid  .    it seemed that when it starting blocking  ,   the trace did not record the execution of it   (  or any of the statements within it either  )    .    however there are other times were the trace did record it is execution and no blocking occurred  .  \n",
      "\n",
      "the sproc error report came from a user  ,   and i was able to find multiple exec statements in traces and run them in ssms  .    no time when i ran them did we have any blocking occur or did they hang  .    they ran as expected   (  the catch block fired and rolled back the transaction after the error  )    .    after resolving the fixing the sproc  ,   we have not seen the issue again  .\n",
      "have you tried using adam machanic  '  s sp  _  whoisactive  ?   there is an option to get the outer command to see if it really is within a proc  .   it could be the application is holding open a transaction instead of committing it  .   try looking at dbcc opentran as well  .\n",
      "verify   $    _  post script will work correctly can someone read through this script really quick and verify that i did not miss anything  .    .    .   I am not getting any errors in my ide so just have to make sure the structure is correct\n",
      "\n",
      "  &  lt  ;    ?  php\n",
      "require  _  once   '    /  usr  /  local  /  cpanel  /   rdparty  /  lib  /  php  /  mail  .  php  '    ;  \n",
      "\n",
      "  $  db  _  server   =     '  localhost  '    ;  \n",
      "  $  db  _  user   =     '    -    -    -    -    -    '    ;  \n",
      "  $  db  _  pass   =     '    -    -    -    -    -    '    ;  \n",
      "\n",
      "  $  dbc   =   mysql  _  connect   (    $  db  _  server  ,     $  db  _  user  ,     $  db  _  pass  )    ;  \n",
      "if   (    !    $  dbc  )     {   \n",
      "    die  (  mysql  _  error  (    )    )    ;  \n",
      "    header   (    '  location  :     /  contact  '    )    ;  \n",
      "    exit  ;  \n",
      "  }  \n",
      "\n",
      "if   (    $    _  post  [    '  contactsent  '    ]     !    =     '  yes  '    )     {  \n",
      "    header   (    '  location  :     /  contact  '    )    ;  \n",
      "    exit  ;  \n",
      "\n",
      "  }   else   {  \n",
      "\n",
      "    if   (  is  _  array  (    $    _  post  )    )     {  \n",
      "        foreach   (    $    _  post as   $  key   =    &  gt  ;     $  value  )     {  \n",
      "              $    _  post  [    $  key  ]     =   mysql  _  real  _  escape  _  string  (  stripslashes  (    $  value  )    )    ;  \n",
      "          }  \n",
      "      }  \n",
      "\n",
      "      $  requesttype        =     $    _  post  [    \"  requesttype  \"    ]    ;  \n",
      "      $  consumerbusiness   =     $    _  post  [    \"  consumerbusiness  \"    ]    ;  \n",
      "      $  globallocation     =     $    _  post  [    \"  globallocation  \"    ]    ;  \n",
      "      $  firstname          =   strtolower  (  str  _  replace  (    \"    '    \"    ,    \"    '    '    \"    ,    $    _  post  [    \"  firstname  \"    ]    )    )    ;  \n",
      "      $  firstname          =   strtoupper  (  substr  (    $  firstname  ,     ,     )    )    .  substr  (    $  firstname  ,     )    ;  \n",
      "      $  lastname           =   strtolower  (  str  _  replace  (    \"    '    \"    ,    \"    '    '    \"    ,    $    _  post  [    \"  lastname  \"    ]    )    )    ;  \n",
      "      $  lastname           =   strtoupper  (  substr  (    $  lastname  ,     ,     )    )    .  substr  (    $  lastname  ,     )    ;  \n",
      "      $  email              =   strtolower  (  str  _  replace  (    \"    '    \"    ,    \"    '    '    \"    ,    $    _  post  [    \"  email  \"    ]    )    )    ;  \n",
      "      $  title              =   strtolower  (  str  _  replace  (    \"    '    \"    ,    \"    '    '    \"    ,    $    _  post  [    \"  title  \"    ]    )    )    ;  \n",
      "      $  title              =   strtoupper  (  substr  (    $  title  ,     ,     )    )    .  substr  (    $  title  ,     )    ;  \n",
      "      $  company            =   strtolower  (  str  _  replace  (    \"    '    \"    ,    \"    '    '    \"    ,    $    _  post  [    \"  company  \"    ]    )    )    ;  \n",
      "      $  company            =   strtoupper  (  substr  (    $  company  ,     ,     )    )    .  substr  (    $  company  ,     )    ;  \n",
      "      $  address            =   strtolower  (  str  _  replace  (    \"    '    \"    ,    \"    '    '    \"    ,    $    _  post  [    \"  address  \"    ]    )    )    ;  \n",
      "      $  address            =   strtoupper  (  substr  (    $  address  ,     ,     )    )    .  substr  (    $  address  ,     )    ;  \n",
      "      $  city               =   strtolower  (  str  _  replace  (    \"    '    \"    ,    \"    '    '    \"    ,    $    _  post  [    \"  city  \"    ]    )    )    ;  \n",
      "      $  city               =   strtoupper  (  substr  (    $  city  ,     ,     )    )    .  substr  (    $  city  ,     )    ;  \n",
      "      $  state              =     $    _  post  [    \"  state  \"    ]    ;  \n",
      "      $  zip                =     $    _  post  [    \"  zip  \"    ]    ;  \n",
      "      $  phone              =     $    _  post  [    \"  phone  \"    ]    ;  \n",
      "      $  f                =     $    _  post  [    \"  f  \"    ]    ;  \n",
      "      $  productdesc        =     $    _  post  [    \"  productdesc  \"    ]    ;  \n",
      "      $  comment            =     $    _  post  [    \"  comment  \"    ]    ;  \n",
      "\n",
      "    if   (    $  globallocation   =    =     \"  canada  \"    )    :  \n",
      "          $  sendto  =    \"  canadainfo  @    -    -    -    -    -    -    .  com  \"    ;  \n",
      "    elseif   (    $  globallocation   =    =     \"  central america  \"    )    :   \n",
      "          $  sendto  =    \"  customer  .  service  @    -    -    -    -    -    -    .  com  .  pa  \"    ;  \n",
      "    elseif   (    $  globallocation   =    =     \"  europe  \"    )    :  \n",
      "          $  sendto  =    \"  marketing  @    -    -    -    -    -    .  uk  \"    ;  \n",
      "    elseif   (    $  globallocation   =    =     \"  mexico  \"    )    :  \n",
      "          $  sendto  =    \"  ventas  @    -    -    -    -    -    -    .  com  .  mx  \"    ;  \n",
      "    else  :  \n",
      "          $  sendto  =    \"  info  @    -    -    -    -    -    -    .  com  \"    ;  \n",
      "    endif  ;  \n",
      "\n",
      "function dbset  (    $  fields  ,     $  source   =   array  (    )    )     {  \n",
      "      $  set  =    '    '    ;  \n",
      "    if   (    !  source  )     $  source   =     &  amp  ;    $    _  post  ;  \n",
      "    foreach   (    $  fields as   $  field  )     {  \n",
      "        if   (  isset  (    $  source  [    $  field  ]    )    )     {  \n",
      "              $  set  .    =    \"    `    $  field  `    =    '    \"    .  mysql  _  real  _  escape  _  string  (    $  source  [    $  field  ]    )    .    \"    '    ,     \"    ;  \n",
      "          }  \n",
      "      }  \n",
      "    return substr  (    $  set  ,      ,     -     )    ;  \n",
      "  }  \n",
      "\n",
      "  /    /   insert into database\n",
      "            mysql  _  select  _  db  (    \"  new  _  contact  \"    ,    $  dbc  )   or die  (    \"  could not select new  _  contact  \"    )    ;  \n",
      "\n",
      "  $  fields   =     explode  (    \"     \"    ,     \"  requesttype consumerbusiness globallocation firstname lastname email title company address city state zip phone f productdesc comment  \"    )    ;  \n",
      "              $  query   =     \"  insert into new  _  contact set   \"    .  dbset  (    $  fields  ,     $    _  post  )    ;  \n",
      "            mysql  _  query  (    $  query  )    ;  \n",
      "\n",
      "  /    /   setup email\n",
      "          $  bodycopy   =     \"  this information was submitted via the   -    -    -    -    -    -    .  com website and sent to you because of the location \n",
      "        identified by the user  .     &  lt  ;  br  &  gt  ;  if this has reached you in error  ,   please forward this email to info  @    -    -    -    -    -    -    .  com  \"    ;  \n",
      "          $  bodycopy  .     \"    &  lt  ;  br  &  gt  ;    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    &  lt  ;  br  &  gt  ;    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "\n",
      "        if   (    $  requesttype   !    =     \"    \"    )          $  bodycopy  .     \"  what kind of information do you need  ?     :     \"     .    $  requesttype  .     \"    &  lt  ;  br  &  gt  ;    \"    ;     \n",
      "        if   (    $  consumerbusiness   !    =     \"    \"    )     $  bodycopy  .     \"  what type of customer or vendor are you  ?     :     \"     .    $  consumerbusiness  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  globallocation   !    =     \"    \"    )       $  bodycopy  .     \"  global location   :     \"     .    $  globallocation  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  company   !    =     \"    \"    )              $  bodycopy  .     \"  company   :     \"     .    $  company  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  firstname   !    =     \"    \"    )            $  bodycopy  .     \"  first name   :     \"     .    $  firstname  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  lastname   !    =     \"    \"    )             $  bodycopy  .     \"  last name   :     \"     .    $  lastname  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  title   !    =     \"    \"    )                $  bodycopy  .     \"  title   :     \"     .    $  title  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  email   !    =     \"    \"    )                $  bodycopy  .     \"  email   :     \"     .    $  email  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  address   !    =     \"    \"    )              $  bodycopy  .     \"  address   :     \"     .    $  address  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  city   !    =     \"    \"    )                 $  bodycopy  .     \"  city   :     \"     .    $  city  .     \"    &  lt  ;  br  &  gt  ;    \"    ;     \n",
      "        if   (    $  state   !    =     \"    \"    )                $  bodycopy  .     \"  state   :     \"     .    $  state  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  zip   !    =     \"    \"    )                  $  bodycopy  .     \"  zip  /  postal code   :     \"     .    $  zip  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  phone   !    =     \"    \"    )                $  bodycopy  .     \"  phone   :     \"     .    $  phone  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  f   !    =     \"    \"    )                  $  bodycopy  .     \"  f   :     \"     .    $  f  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "        if   (    $  productdesc   !    =     \"    \"    )          $  bodycopy  .     \"  upc or product description   :     \"     .    $  productdesc  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "\n",
      "          $  bodycopy  .     \"    &  lt  ;  br  &  gt  ;    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    -    &  lt  ;  br  &  gt  ;    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "\n",
      "        if   (    $  comment   !    =     \"    \"    )              $  bodycopy  .     \"  comments   :     &  lt  ;  br  &  gt  ;    \"     .    $  comment  .     \"    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "\n",
      "          $  bodycopy  .     \"    &  lt  ;  br  &  gt  ;    &  lt  ;  br  &  gt  ;    \"    ;  \n",
      "          $  bodycopy  .     $  ip   =     $    _  server  [    \"  remote  _  addr  \"    ]    ;  \n",
      "\n",
      "  /    /   process email\n",
      "\n",
      "  /    /   mail server info  .    .    .  \n",
      "\n",
      "          $  from   =     $  sendto  ;  \n",
      "          $  to   =     \"  do not reply   &  lt  ;  donotreply  @    -    -    -    -    -    -    &  gt  ;    \"    ;  \n",
      "          $  subject   =     \"  website contact   :     \"     .     $  globallocation  ;  \n",
      "          $  body   =     $  bodycopy  ;  \n",
      "          $  host   =     \"  mail  .    -    -    -    -    -    -    \"    ;  \n",
      "          $  port   =     \"     \"    ;  \n",
      "          $  username   =     \"  donotreply  @    -    -    -    -    -    -    -    \"    ;  \n",
      "          $  password   =     \"    -    -    -    -    -    -    -    \"    ;  \n",
      "\n",
      "          $  headers   =   array   (    '  from  '     =    &  gt  ;     $  from  ,  \n",
      "          '  to  '     =    &  gt  ;     $  to  ,  \n",
      "          '  subject  '     =    &  gt  ;     $  subject  )    ;  \n",
      "          $  smtp   =   mail  :    :  factory  (    '  smtp  '    ,  \n",
      "        array   (    '  host  '     =    &  gt  ;     $  host  ,  \n",
      "           '  auth  '     =    &  gt  ;   true  ,  \n",
      "           '  port  '     =    &  gt  ;     $  port  ,  \n",
      "           '  username  '     =    &  gt  ;     $  username  ,  \n",
      "           '  password  '     =    &  gt  ;     $  password  )    )    ;  \n",
      "\n",
      "          $  mail   =     $  smtp  -    &  gt  ;  send  (    $  to  ,     $  headers  ,     $  body  )    ;  \n",
      "\n",
      "        if   (  pear  :    :  iserror  (    $  mail  )    )     {  \n",
      "        echo  (    \"    &  lt  ;  p  &  gt  ;    \"     .     $  mail  -    &  gt  ;  getmessage  (    )     .     \"    &  lt  ;    /  p  &  gt  ;    \"    )    ;  \n",
      "          }   else   {  \n",
      "        echo  (    \"    &  lt  ;  p  &  gt  ;  message successfully sent  !    &  lt  ;    /  p  &  gt  ;    \"    )    ;  \n",
      "          }  \n",
      "\n",
      "  /    /   make sure db conn is closed\n",
      "        mysql  _  close  (    $  dbc  )    ;  \n",
      "\n",
      "  /    /   redirect to thank you page\n",
      "        header   (    '  location  :     /  index  .  php  ?  option  '    )    ;  \n",
      "        exit  (    )    ;  \n",
      "\n",
      "  }  \n",
      "\n",
      "  ?    &  gt  ;\n",
      "although it will lenghten your code quite a bit  ,   in my opinion it is worth checking if the   $    _  post variables are set   (  using isset  )    ,   to avoid any exceptions  .\n",
      "what is the relation between sensor size and image quality   (  noise  ,   dynamic range  )    ?   I am reading this description on sensor size  :  \n",
      "\n",
      "\n",
      "  digital compact cameras have substantially smaller sensors offering a\n",
      "  similar number of pixels  .   as a consequence  ,   the pixels are much\n",
      "  smaller  ,   which is a key reason for the image quality difference  ,  \n",
      "  especially in terms of noise and dynamic range  .  \n",
      "\n",
      "\n",
      "could you please elaborate on the last sentence  :   what is the relation between sensor size and image quality  ?   in particular  ,   what are the advantages and disadvantages of a small sensor   (  of a compact camera  ,   in contrast to a large sensor of a dslr camera  )    ?\n",
      "a digital image sensor is ultimately a device that uses the photovoltaic or photoconductive properties of photodiodes to convert photons into electrons   (  charge  )    ,   which can later be read out as a saturation value and converted to a digital pixel  .   this is an analog  -  to  -  analog then analog  -  to  -  digital conversion process  .   \n",
      "\n",
      "the key behavior of a photodiode relevant to imaging  ,   converting photons to electrons  ,   improves with total surface area  .   the more surface area  ,   the greater the area to detect photon strikes per photodiode  ,   and the greater the physical material area within which electron charge   (  signal  )   can be collected  .   in other words  ,   larger physical pixel area equates to higher signal ratio  .   the   \"  depth  \"   of a well is ultimately immaterial to modern bayer  -  type cfa sensors  ,   as deeper penetration only really has a filtration effect  .    .    .  the deeper a photon penetrates a photodiode  ,   the more blueshifted light will be filtered out in favor of redshifted light  .   this is due to the response curve of the type of silicon used in photodiodes  .    .    .  which are more sensitive to infrared wavelengths than visible light wavelengths  ,   and more sensitive to visible light wavelengths than ultraviolet and x  -  ray wavelengths  .  \n",
      "\n",
      "finally  ,   being electronic devices  ,   image sensors produce a variety of forms of electronic noise  .   in particular  ,   they are susceptible to a low number of electrons in any given photodiode being generated from the low level of dark current that is always running through the sensor  .   being devices sensitive to electromagnetic frequencies  ,   the intrinsic field of the sensor itself can be affected by strong  ,   nearby devices that emit electromagnetic frequencies of their own   (  if its not shielded properly  )   which can produce banding  .   slight differences in the exact electrical response of each pixel can produce slight variations in how the charge accumulated in a photodiode is read out  ,   and there can be thermally induced forms of noise  .   these forms of noise create a signal floor wherein it becomes difficult or impossible to determine of a digital pixel level is the product of an actual photon capture or due to electronic and thermal noise  .   so long as the image signal is larger than the noise floor  ,   or in other words the signal to noise ratio   (  snr  )   is high  ,   a useful image can be produced  .  \n",
      "\n",
      "all things being equal  .    .    .  and by that  ,   i mean the same number of pixels  ,   the same ultimate sensor design characteristics  ,   the same amount of dark current  ,   the same amount of read noise  .    .    .  a smaller sensor will be noisier than a larger sensor because the larger sensor  ,   with the exact same number of pixels  ,   can have larger surface area for each of those pixels  ,   allowing more electrons to be captured for any given photon strike  .   a larger pixel has a higher maximum saturation point  ,   which allows more total electron charge before the pixel is   \"  full  \"   or totally white  .   that intrinsically increases snr  ,   reducing the impact that electronic noise has on the final image signal  ,   producing less noisy images at exactly the same settings as a smaller sensor  .  \n",
      "\n",
      "dynamic range is the total usable tonal range available from a sensor  .   it is ultimately affected by the amount of electronic noise present in a sensor and the maximum saturation point of the pixels  ,   or the ratio between the mean of electronic noise and the maximum saturation point of each pixel in the sensor  .   again  ,   all things being equal  ,   dynamic range will be better on a larger sensor as the snr  ,   even at low signal levels  ,   is going to be slightly better than a smaller sensor  ,   and at higher signal levels it can be significantly better  .   as is often the case with image sensors these days  ,   increasing pixel size  ,   or for that matter increasing a pixels maximum sensitivity   (  iso  )    ,   has the effect of also increasing the maximum amount of read noise at low iso  .   this is ultimately due to poor control over electronic noise to start with  ,   resulting in higher read noise at minimum iso for larger sensors than for smaller sensors  .   while the increase in read noise is often still minor compared to the increase in maximum saturation point  ,   and therefor does not affect maximum snr much  ,   it can mitigate or eliminate any gains at the sensors minimal snr level  ,   reducing or eliminating any improvement in dynamic range as well  .\n",
      "multiple keys via hkdf   -   whats better  ,   one or two applications of hkdf  -  extract assume for the sake of the question that i have two variable  -  length bit strings  ,   each with   bit cryptographic randomness  ,   and i want to extract two   bit keys via hkdf  -  sha   .  \n",
      "\n",
      "which alternative is better   (  if any  )    ,   and why  ?  \n",
      "\n",
      "\n",
      "use a single hkdf  -  extract on the concatenation of the two strings  ,   and two hkdf  -  expands with different info strings to get two   bit keys  .  \n",
      "use two hkdf  -  extract operations  ,   one for each bit string  ,   and use a single hkdf  -  expand on each to get two   bit keys  .  \n",
      "\n",
      "\n",
      "or in other words  ,   is it better to hkdf  -  extract on a longer ikm string and use multiple hkdf  -  expands  ,   or is it better to use hkdf on independent but shorter ikms  .  \n",
      "\n",
      "my intuition tells me that  ,   if my randomness strings are really as good as i claim  ,   then two independent hkdf extracts are better  ,   but using a single one on the concatenation is  ,   in practice  ,   just as safe  ,   and safer if my input randomness is not actually as good as assumed  ,   so two hkdfs might be more robust in practice  .\n",
      "realistically  ,   it probably does not matter  ,   if all of your premises are accurate  .  \n",
      "\n",
      "if it were me  ,   I had probably concatenate the inputs  ,   then apply a hkdf to the concatenation to derive two keys   -    -   but honestly  ,   it is unlikely to matter  .    this is very unlikely to be the weakest link in your system  .    pick something that is easy to implement and easy to understand  ,   and move on  :   focus your energy on some other aspect of your system  .\n",
      "what filter should i use to avoid overly  -  whitened daylight  images  ?   what filter should i use to avoid whitened images   (  blown highlights  )   caused by intense daylight  ?   at the moment i am only using an uv filter  ,   but it does not solve the whole problem  .  \n",
      "\n",
      "i am a newbie in photography  ,   and I have recently bought a nikon dslr  .\n",
      "if you are indeed referring to your images having too much contrast to allow for both a properly exposed subject and sky  ,   then there are a few options  .  \n",
      "\n",
      "\n",
      "as zak stated  ,   you could use exposure compensation to darken the entire image   (  this might make the non  -  sky portion of the image too dark though  )  \n",
      "use a graduated neutral density filter  .   this will selectively darken one part of an image while leaving the other part alone  .   this is the old  -  school method  .   while it typically limits the transition between the filtered and unfiltered parts to a straight line  ,   it does allow you to fix things at the time of the shot without having to spend time tweaking it on the computer afterwards  .   \n",
      "hdr   (  high dynamic range  )   photography  .   this is the hi  -  tech way to do things  .   this requires that you take multiple shots of the same scene at different exposures and use software to combine them into one image where both the highlights and shadows are well  -  exposed  .\n",
      "create heatmap with pstricks or tikz i have a huge amount of  d  -  coordinates  ,   associated with a value  ,   e  .  g  .    :  \n",
      "\n",
      "  x     |     y     |   value\n",
      "   .         .         .   \n",
      "   .         .         .   \n",
      "   .         .         .   \n",
      "   .         .         .   \n",
      "  .    .    .  \n",
      "\n",
      "\n",
      "the data is not tabular and i want the points between two data  -  points to be interpolated in some way   (  which way is not really clear  ,   yet  )  \n",
      "\n",
      "i want to preset the data as heatmap like this  :  \n",
      "\n",
      "\n",
      "is there any ready  -  to  -  use package for pstricks or tikz to do it  ?\n",
      "there is tikzdevice for r which will generate tikz code for a plot created in r  .   so  ,   if you use r to create your heat map   (  say  ,   using ggplot   '  s geom  _  density d  (    )    )    ,   you also get the tikz code with little effort  .   there is a learning curve  ,   though  .  \n",
      "\n",
      "however  ,   this kind of image should be included as a   (  perhaps high  -  resolution  )   raster image in your document  ,   as the vector version might take a long time to render  .   so you can create a tikz version of the plot  ,   compile it to pdf and then convert to png at the required resolution  /  pixel density  .\n",
      "can you book hotels on a prepaid credit card worldwide  ?   i tend to stay at smaller boutique hotels or local apartments when i visit a city but recently due to some credit card issues i will need to depend on prepaid visa  .    but i noticed when i try to shop online a lot of retailers do not accept prepaid credit cards so I am thinking hotels would be even stricter  .    is there a list somewhere of countries or particular hotel chains than ban prepaid cards  ?   if so  ,   how does one book online or reserve a room without a card  ?    do all hotels worldwide accept cash  ?\n",
      "tl  ;  dr   -   depends  ,   on your pre  -  paid card  ,   the hotel  ,   and how you book the hotel  .  \n",
      "\n",
      "there are a few different charges to consider here  :  \n",
      "\n",
      "\n",
      "pre  -  payment of the room at  /  shortly after booking\n",
      "holding the room on a flexible booking\n",
      "deposit at checkin\n",
      "room charge  ,   meals  ,   drinks  ,   extras etc at checkout\n",
      "\n",
      "\n",
      "with many otas and hotel websites  ,   if you make a non flexible booking  ,   or some kinds of flexible bookings  ,   they will charge your card for the room rate during the booking process  .   with some others  ,   they will send your card details through to the hotel  ,   who will put it through their tills later  .     (  maybe that day  ,   maybe during a weekly sweep  )    .   in order for this pre  -  payment to go through  ,   your card will need to support offline   /   cardholder  -  not  -  present transactions  .   as long as your card advertises itself as   \"  suitable for online shopping  \"   or similar  ,   and as long as the card issuer does not block travel booking  ,   you should be fine  .   speak to your card issuer to be sure  .  \n",
      "\n",
      "alternately  ,   when reserving the room  ,   you might opt for a flexible rate where you pay at checkout  ,   with no pre  -  payment  .   this is typically offered on the hotel  '  s own site  ,   and some otas  .   they will normally ask for a credit card to   \"  hold  \"   the reservation  ,   which would be charged in the event of a no  -  show  ,   but as long as you turn up as planned the card will not be charged  .   these rates are normally a bit more expensive than non  -  flexible pre  -  paid ones  ,   but this style of booking should be fine on any card  .  \n",
      "\n",
      "at checkin  ,   most hotels   (  but not all  )   will want to take some sort of deposit  .   typically this covers any unpaid parts of the room rate  ,   along with expected spend in the restaurant   /   bar   /   etc  .   the details taken would also be used in the event of damage  .   if you have pre  -  paid the room and do not plan to run up extras in the hotel  ,   many hotels would let you not leave a deposit  ,   but not all  .   very small hotels may take a look at you  ,   decide you look fine  ,   and tell you to pay everything at checkout  .   some hotels may want to take something no matter what  .  \n",
      "\n",
      "for the deposit  ,   this is normally done as a pre  -  authorisation  .   not all pre  -  paid card support this  ,   which would be a problem  .   some pre  -  paid cards do support it  ,   but badly  ,   and might take a week or so to release the reserved funds  ,   which could be a problem  .   check with your card issuer about this  .   alternately  ,   many hotels which require a deposit will be willing to take a cash deposit  ,   but they may well want a larger cash deposit than they would normally authorise on a credit card  .     (  they cannot charge extras to the card later for example  )    .   expect to leave a hefty chunk of cash as a deposit  ,   expect checkin to take a while  ,   expect several people to count the money  ,   bills may be checked for counterfeits  ,   and expect a receipt at the end  .   speak with the hotel first to see if they need a deposit  ,   and if they take cash ones  .   as a datapoint  ,   i once got stuck in a checkin queue behind someone doing a cash deposit  ,   he was asked to leave something like   nights room rate as a deposit on a   night stay  .  \n",
      "\n",
      "finally  ,   we have settling up the bill at the end  .   if you pre  -  paid  ,   and did not eat there  ,   you could well find there is nothing to pay  ,   or just a few dollars   /   pounds   /   euros in city taxes  .   alternately  ,   if you did not pre pay   (  or they did not take it  )    ,   ate in the restaurant lots etc  ,   you could have a decent amount to pay here  .   you will be present  ,   so any card which supports in  -  person transactions will be fine for this  .   well  ,   as long as you have the money on it for the bill that is  !   if you did a cash deposit  ,   you should get the choice between taking the bill off that and getting the remainder  ,   or getting the whole deposit back in cash and paying on a card  .  \n",
      "\n",
      "oh  ,   one more possibility   -   charges not noticed at checkout  .   maybe you forgot to tell them about the minibar charges  ,   maybe that morning  '  s breakfast did not go through  ,   something like that  .   if you used a credit card for the deposit  ,   expect them to charge it to that later when they notice  .   if you paid cash  ,   expect to get a bill in the mail some time later  .  \n",
      "\n",
      "can it be done   -   sure  !   can it be done at one specific hotel with one particular kind of pre  -  paid card on one specific rate booked one specific way  ?   depends  .    .    .\n",
      "fiber channel san hooked up to   esxi hosts i have a powervault with   fc modules at  gb  /  s  .   i do not want to jack the san into our    /     /    switch because of the max  gb  /  s speed  .  \n",
      "\n",
      "my question is can i jack both esxi host  '  s fc into the san  '  s fcs and have hv and vmotion  ?\n",
      "the san itself does not need an ethernet connection in this case  ,   all it needs is fc to the esxi hosts  .   the esxi hosts will need gige ethernet  .   during vmotion  ,   the source esxi starts sending machine  -  state to the target esxi host over the vmkernel connection in the virtual  -  switch  .   that is actually a connection between the two esxi hosts over your physical ethernet switch  .   meanwhile  ,   when state is fully transferred control of the vmdk files is passed over to the target server it it goes live  .   so  ,   vmotion requires both fc and ethernet  .  \n",
      "\n",
      "ha   (  what i presume you meant by hv  )   requires vmotion to work  ,   so should be available if vmotion is working  .   \n",
      "\n",
      "unless you meant hardware virtualization  ,   or direct lun presentation  .   that can also work  ,   but is trickier  .   the same volume needs to be presented to both esxi hosts using exactly the same lun number  ,   if it does not have the same lun number that volume will not be visible when the vm is on one or the other esxi host  .\n",
      "how do i remove a facebook app request  ?   unfortunately  ,   because of some fat fingering when browsing the klout website from my ipad i accidentally sent app requests to   friends  .   i am really annoyed by these sort of requests and certainly did not intend to send them to my friends  .  \n",
      "\n",
      "\n",
      "can i view the app requests I have sent on facebook  ?  \n",
      "can i undo the requests that I have sent  ?  \n",
      "can i add a privacy setting to prevent any facebook app   (  klout or otherwise  )   from ever sending these requests in the future  ?\n",
      "there is not a native way with klout  '  s ui that i am aware of  .   your best bet is finding a chrome extension that handles facebook js sdk api calls  ,   then you can do something like this\n",
      "\n",
      "fb  .  api  (    '    /  me  /  apprequests  '    ,   function  (  response  )     {  \n",
      "  var ids   =     [    ]    ;  \n",
      "  for   (  var i  =     ,   l  =  response  .  data  .  length  ;   i  &  lt  ;  l  ;   i  +    +    )     {  \n",
      "    fb  .  api  (    '    /    '     +   response  .  data  [  i  ]    .  id  ,     '  delete  '    ,   \n",
      "      function  (  response  )    {   console  .  log  (    '  cleared request  :    '     +   response  )     }   \n",
      "      )    ;  \n",
      "    console  .  log  (    '  for  :     '     +   response  .  data  [  i  ]    .  to  .  name  )    ;  \n",
      "    }  \n",
      "  }    )    ;  \n",
      "\n",
      "\n",
      "quick and dirty would be to just paste this into your developer console on klout  '  s page  .\n",
      "how often do i need to change my oil in a vw tdi  ?   the manual for my   vw tdi says to change the oil every   months or    ,    miles  .    my dealer says every   months or    ,    miles  .  \n",
      "\n",
      "are they just trying to make twice as much money off my oil changes  ,   or is there a legitimate reason to change the oil twice as often as vw originally printed in the book  ?\n",
      "passat tdi  .   have an oil analysis done and quit speculating  .   i change at    .    to  k as that is when the additive package was low on my last analysis  .\n",
      "does this statement make any sense  ?   i am asking this question completely out of curiosity  .   the other day  ,   my roommate  ,   by mistake  ,   used   '  light year  '   as a unit of time instead of distance  .   when i corrected him   (  pedantic  ,   much  )    ,   he said the following  :  \n",
      "\n",
      "  \"  units are relative  .   and according to fourier transforms  ,   units can be changed so light year is a unit of time  .    \"  \n",
      "\n",
      "that got me thinking and i read up fourier transforms on wikipedia but could not find anything about using a unit in one domain as a unit for another measurement  .   i do agree that units   (  particularly  ,   base units are relative  .   eg  :   the meter  )    ,   but does his statement make any sense  ?  \n",
      "\n",
      "edit\n",
      "thank you everyone for all the answers  .   it is not so much to in it in or prove a point as it is to understand the concept better  .   anyways this is his response after i showed him this thread  .   any comments would be appreciated  .  \n",
      "\n",
      "his response  :  \n",
      "nevermind  ,   for the first time i accept i was wrong  .   but using lightyears to measure time is possible  .   my example did not make sense bacause i was wrong when i meantioned that I am still measuring dist  .   if you have a signal in time domain and   .    .    .  take the ft  ,   i get a signal which does not have to be in frequency domain  .   clarify this to the guy who posted last  .   now the new signal is in a domain defined by me and so is its units  .   this signal although not equal to the original signal  ,   still represents that if ya take an inverse ft  .   so  ,   the idea of time will still be there  .   now coming back to our case  :   lightyears here is not the lightyears you are used to read when dealing with distance  .   it represents time  .\n",
      "this does not make much sense  :   light year is in any case a unit of distance  .  \n",
      "\n",
      "what is common is to use   \"  reduced units  \"    ,   for examples units where   $  c  =     $     (  speed of light  )   or   $  h  =     \\  pi  $    .   but in these cases the opposite would happen  :   you would say   \"  year  \"   to mean a distance  .   or for example you say   \"  has a mass of xyz mev  \"   instead of   \"    $  mev  /  c  ^     $    \"    .  \n",
      "\n",
      "\n",
      "\n",
      "about the fourier transform  :   this allow to go from the so  -  called   \"  time domain  \"     (  even if   \"  time  \"   is not always the usual time  )   to the   \"  frequency domain  \"   involving   .    .    .   frequencies  .  \n",
      "\n",
      "but as you can see this cannot change the definition of light  -  year  .\n",
      "servers  ,   high availability and faster response i recently bought a second webserver because i worry about hardware failure of my old server  .   now that i have that second server i wish to do a little more then just have one server standby and replicate all day  .   as long as it is there i might as well get some advantage our of it   !  \n",
      "\n",
      "i have a website powered by ubuntu    .     ,   nginx  ,   php  -  fpm  ,   apc  ,   mysql   (     .     )   and couchdb  .  \n",
      "\n",
      "im currently testing configurations where i can achieve failover and make good use of the extra harware for faster responses   /   distributed load  .  \n",
      "\n",
      "the setup i am testing nowinvolves heartbeat for ip failover and two identical servers  .   of the two servers only one has a public ip adress  .   if one server crashes the other server takes over the public ip adress  .   on an incoming request nginx forwards the request tot php  -  fpm to either server a of server b   (     /    if both servers are alive  )    .   once the request has been send to php  -  fpm both servers look at localhost for the mysql server  .   i use master  -  master mysql replication for this  .   the file system is synced with lsyncd  .  \n",
      "\n",
      "this works pretty well but im reading it is discouraged by the   (  mysql  )   community  .  \n",
      "\n",
      "another option i could think of is to use one server as a mysql master and one server as a web  /  php server  .   the servers would still sync their filesystem  ,   would still run the same duplicate software   (  nginx  ,  mysql  )   but master slave mysql replication could be used  .   as long as bother servers are alive i could just prefer nginx to listen to ip a and mysql to ip b  .   if one server is down  ,   the other server could take over the task of the other server  ,   simply by ip switching  .   \n",
      "\n",
      "but im completely new at this so i would greatly value your expert advice  .   is either of the two setups any good   ?   if you have any thoughts on this please let me know   !  \n",
      "\n",
      "ps  ,   virtualisation  ,   hosting on different locations or active  /  passive setups are not solutions im looking for  .   i find virtual server either too slow or too expensive  .   i already have a passive failover on another location  .   but in case of a crash i found the site was still unreachable for too long due to dns caching  .\n",
      "we do it on all our customer ha boxes and our magento microcloudtm  /   configurations  ,   so that the fail over system  (  s  )   is not idle  .   look into drbd  ,   linux vserver and heartbeat  .   \n",
      "\n",
      "read the following for an insight into it  .  \n",
      "\n",
      "http  :    /    /  linux  -  vserver  .  org  /  getting  _  high  _  with  _  lenny\n",
      "\n",
      "caveat\n",
      "but as always  ,   we advise not trying to do this unless you have substantial experience  ,   as you are more likely to end up with a more unreliable configuration  ,   that is harder to debug and if you end up in a split brain situation  ,   more likely to have data loss   .    .    .   than a plain old single server  .\n",
      "why does string theory have such a huge landscape  ?   i was browsing through foundations of space and time  ,   a compilation of essays on various theories of quantum gravity  .   the following passage in the introduction intrigued me  :  \n",
      "\n",
      "\n",
      "  each compactification leads to a different vacuum state  .    .    .    .   at least one state should describe our universe in its entirety  .    .    .    .   the enormous number   (    ~     ^    at last count  )   of solutions  ,   with no perturbative mechanism to select mechanism to select among them  ,   leads some critics to question the predictive power of the theory  .    .  even more worrying is that  ,   while the theory is perturbatively finite order by order  ,   the perturbation series does not seem to converge  .  \n",
      "\n",
      "\n",
      "i do not know anything about string theory and so i could not make head or tails this  .   all i know is that   ~    $     ^    {     }    $      is a very large number  .   \n",
      "\n",
      "\n",
      "what exactly is a   '  solution  '   in string theory  ?   is it a spacetime metric of some sort or the terms of a s  -  matrix of some sort  ?    \n",
      "why are there so many   '  solutions  '    ?   \n",
      "i thought string theory was supposed to be finite  ,   why do perturbative series still diverge  ?  \n",
      "is there any experimental technique to limit the number of   '  solutions  '    ?    \n",
      "will experimental techniques be able to pinpoint a solution within present day string theorists  '   lifetimes too  ?   if not  ,   how long will it take before we can experimentally probe these things  ?   \n",
      "are string theorists completely relaxed about these issues  ?   or are they in anguish  ?\n",
      "\"  each compactification leads to a different vacuum state  .    .    .    .   at least\n",
      "  one state should describe our universe in its entirety  .    .    .    .   the\n",
      "  enormous number   (    ~     ^    at last count  )   of solutions  ,   with no\n",
      "  perturbative mechanism to select mechanism to select among them  ,   leads\n",
      "  some critics to question the predicitive power of the theory  .    .  even\n",
      "  more worrying is that  ,   while the theory is perturubatively finite\n",
      "  order by order  ,   the pertrubation series does not seem to converge  .    \"  \n",
      "\n",
      "\n",
      "ok  .    \n",
      "\n",
      "\n",
      "  i do not know anything about string theory and so i could not make head\n",
      "  or tails this  .   all i know is that   ~     ^    is a very large number  .  \n",
      "\n",
      "\n",
      "that  ,   it is  .        \n",
      "\n",
      "\n",
      "  what exactly is a   '  solution  '   in string theory  ?   is it a spacetime\n",
      "  metric of some sort or the terms of a s  -  matrix of some sort  ?    \n",
      "\n",
      "\n",
      "so  ,   we have a non  -  peturbative definition of m  -  theory and string theories on ads space through the ads  /    .  cft correspondence  .   now  ,    these are   or    -  dimensional  .            to get rid of the extra   or   dimensions  ,   you need to compactify it on a    -  dimensional or    -  dimensional manifold  .   \n",
      "\n",
      "a particularly convinient compactifications of    -  dimensional m  -  theory is on   $  g  (     )    $    -  holonomy manifolds  .         particularly convinient compactifications of    -  dimensional string theories  ,   such as type he  ,   are on   $  su  (     )    $    -  holonomy calabi  -  yau manifolds  .        of course  ,   it is not necessary  ;   e  .  g  .   if the world happens to be something with   $    \\  mathcal n  =     $   supersymmetry  ,   as opposed to                             $    \\  mathcal n  =     $    .               \n",
      "\n",
      "\n",
      "  why are there so many   '  solutions  '    ?    \n",
      "\n",
      "\n",
      "because there are lots of these manifolds  !         \n",
      "\n",
      "\n",
      "  i thought string theory was supposed to be finite  ,   why do perturbative\n",
      "  series still diverge  ?    \n",
      "\n",
      "\n",
      "uh  .    .    .   yes  .   but it is renormaliable  .   and there are non  -  peturbative definitions in ads spacetime  .       \n",
      "\n",
      "\n",
      "  is there any experimental technique to limit the number of\n",
      "    '  solutions  '    ?    will experimental techniques be able to pinpoint a\n",
      "  solution within present day string theorists  '   life      times too  ?   if\n",
      "  not  ,   how long will it take before we can experimentally probe these\n",
      "  things  ?  \n",
      "\n",
      "\n",
      "in principle  ,   it is possible  .      but in anyone  '  s lifetime  .    .    .   do you know how big   $     ^    {     }     $   is  ?     %   see this  .     \n",
      "\n",
      "\n",
      "  are string theorists completely relaxed about these issues  ?   or are\n",
      "  they in anguish  ?      \n",
      "\n",
      "\n",
      "there is the branch of string phenomenology that attempts  to find the correct vacua  .    .    .\n",
      "what are the benefits of owning a physical book  ?   i have seen this question about updates of the d  &  amp  ;  d  th edition books  ,   and it got me thinking  .  \n",
      "since i got my kindle i have not read a single paper novel  ;   they have fewer drawbacks compared to digital copies than rpg rulebooks  .  \n",
      "\n",
      "\n",
      "dead  -  tree types have some benefits like looking good on a bookshelf  ,   but any ebook reader weighs less with   novels than the usual hard  -  cover book  .   \n",
      "if you want to look for the damage of ares alpha  ,   even with a half  -  decent tablet it takes less than   seconds  .    \n",
      "digital copies do not get worn  ,   they never get unwanted earmarks  ,   but you can bookmark them  .  \n",
      "rulebooks do get updates  ,   and unless you are willing to take a pen to your book  ,   your hard copies will never contain them  .   the pdfs can be edited and resent to the buyers  .   \n",
      "even better is the wotc approach with the ddi  ,   you can look up any monster or item or   (  almost any  )   rule  ,   in the most recent form  ,   for   years at the cost of seven books  .   \n",
      "\n",
      "\n",
      "i think this is the way to go  ,   even considering the horribly slow character builder  .   although i must admit good illustration can help build the athmosphere  .  \n",
      "\n",
      "so what am i missing  ?   why are people buying rpg rulebooks in paper format  ?   why are books even published  ,   i do not need to know if feats are supposed to be on the right page and skills on the left  ,   i just want a list of them  ,   filterable any way i want  .   \n",
      "\n",
      "is this just a necessary part of earning money  ?   i understand that pdfs are copied illegally  ,   but the compendium is not  .\n",
      "as others have said   -   there is definitely something to be said about tactile navigation  .   \n",
      "\n",
      "while digital formats   (  assuming they are text  -  parseable  )   can be searched  ,   if you do not know the specific spelling or the specific term  ,   they can be difficult to parse by hand  .   quadruply so if the publisher did not provide bookmarks to the different chapters  .     (  which is really annoying  ,   imo  .    )  \n",
      "\n",
      "by contrast  ,   with a physical copy  ,   you can pick up a general sense of where the desired content is physically located fairly quickly  .   for example  :   combat rules are towards the middle  .   spell lists are towards the end  .   character classes are near the beginning  .   the more familiar one is with the book  ,   the quicker this is to process   (  and more accurate one tends to get  )    .  \n",
      "\n",
      "additionally   -   i stare at a digital monitor all day at work  ;   then for most of the evening  .   so if i actually have to read something  ,   I will opt for ink on paper  ,   just to save my eyes that itty little bit  .     :    )   \n",
      "\n",
      "  (  kindles are awesome  ,   but they can be slow  ;   plus  ,   some books have art work and  /  or tables that i am unsure would translate over very well  .    )\n",
      "i teleported too high on my minecraft server my friends own a minecraft server and i was playing around with commands when i teleported to y   just because i wanted to see how high maps could go  .   that was a big mistake  .   now every time i go on the server i crash  .   \n",
      "\n",
      "i do not have access to the server files and i do not have the owner  '  s email or phone number or anything and I am the only one that plays on the server  .   i know that one solution is to make a new minecraft character but that would involve me to pay  .   \n",
      "\n",
      "is there some way i can delete my player information or change the location without access to the server files  ?   i do not want to leave the server because I have been building a castle for   months now and i do not want to give up that easy  .\n",
      "yer  ,   you are basically screwed   :  i  .   even though you do not have contact to the owner maybe try and get a friend to go online and get them to teleport you to the ground or get them to contact the owner  ,   as only he will be able to fix this  .\n",
      "how can i write html and send as an email  ?   i want to send html email from my outlook or yahoo  ,   gmail  ,   hotmail  .   i was not able to find any such option where i could write html which will then be rendered at the receiver  '  s end  .   how can i do that  ?\n",
      "html in email clients is a very very thorny problem because most of them do not just embed a browser  ,   but implement random subsets of the html and css  .   and there is a lot of email clients  .  \n",
      "\n",
      "if you really do need to do this  ,   you may look at professional solutions from mailing list marketing companies  .   it is a known problem in that space and there are services  ,   tutorials   (  example  )   and checklists for that  .   here is a service from mailchimp  .   most of the providers offer something similar  .\n",
      "why is the potential function defined differently in physics and calculus  ?   i am very familiar with the concept of a potential function  ,   and potential energy  ,   from calculus  -  based physics  .  \n",
      "\n",
      "for instance  ,   if we have the familiar force field   $    \\  mathbf  {  f  }     =     -  mg   \\    ,    \\  mathbf  {  j  }    $    ,   then a potential function is given by   $  u   =   mgy   +   c  $    .     (  since potential energy is relative  ,   we have an infinite number of potential functions  .    )  \n",
      "\n",
      "notice that the gradient of the potential function is the negative of the force field  :     $    $    \\  nabla u   =     \\  nabla  (  mgy   +   c  )     =   mg   \\    ,    \\  mathbf  {  j  }     =     -    \\  mathbf  {  f  }    .    $    $  \n",
      "\n",
      "that was perfectly fine with me  .   but now in vector calculus  ,   i am reading that the potential function   $  f  $   of a vector function   $    \\  mathbf  {  f  }    $   is such that   $    \\  nabla f   =     \\  mathbf  {  f  }    $    .   a negative sign appears to have been lost when migrating from physics to calculus  .  \n",
      "\n",
      "it seems confusing to call   $  f  $   a   \"  potential function  \"    ,   since it cannot be interpreted as potential energy in the real world  .   so why is the calculus nomenclature as it is   (  i  .  e  .    ,   why not call this something else and then say the potential function is the negative of it  )    ?\n",
      "recall where the negative sign comes from in physics   -    -   it is simply due to your coordinate system and point of view  .   the difference is analogous to the difference between work done by gravity and work done on gravity  .\n",
      "what is cold iron actually  ?   it came up in dresden files  ,   but is not limited to that game  ,   you can find the term in dnd as well  .   i would like to know what it means  .   \n",
      "\n",
      "if you look for cold iron on wikipedia  ,   you only get iron  :   \n",
      "  \"  cold iron is a poetic and archaic term for iron  .    \"  \n",
      "this would imply everything made mostly from fe is cold iron  .   clearly  ,   this is not the case  ,   in every game cold iron is something special  ,   the every day sword is not made out of it  .   \n",
      "\n",
      "the dresden files rulebook is not very specific about it  :  \n",
      "\n",
      "\n",
      "  something that anyone could reasonably get access to  ,   but usually doesn ’ t carry on them   (  like cold iron  )   page    .  \n",
      "\n",
      "\n",
      "what is cold iron  ?  \n",
      "how do i create cold iron  ?  \n",
      "how do i get cold iron  ?     \n",
      "\n",
      "to make the question easier to understand  ,   compare cold iron to holy water  .   you know how it is different from usual water  ,   you know how you get it or create it  .\n",
      "based on the events of summer knight cold iron is  ,   in fact  ,   just iron  .    dresden\n",
      "\n",
      "\n",
      "   kills aurora with hundreds of pixies wielding common hobby knives with plastic casings  .    the book specifically mentions how the cold iron of the knife blades makes the relatively minor cuts deadly to the summer lady  .  \n",
      "\n",
      "\n",
      "referencing your quote  ,   how much steel do you have on you right now  ?    sure  ,   some people still carry a leatherman or a pocket knife but most do not in the us these days  .    however  ,   how easy is it to get one  ?    in summer knight dresden specifically mentions needing to\n",
      "\n",
      "\n",
      "   stop at walmart prior to the climatic battle but not what he needs  ,  \n",
      "\n",
      "\n",
      "which is saved for the big reveal at the stone table  .\n",
      "relationship between user story  ,   feature  ,   and epic  ?   as someone whose still new to agile  ,   I am not sure i completely understand the relationship or difference between a user story  ,   feature  ,   and epic  .  \n",
      "\n",
      "according to this question  ,   a feature is a collection of stories  .    one of the answers suggest that a feature is actually an epic  .    \n",
      "\n",
      "so are features and epics considered the same thing  ,   which is basically a collection of related user stories  ?  \n",
      "\n",
      "our project manager insists that there is a hierarchical structure  :  \n",
      "\n",
      "epic   -    >   features   -    >   user stories\n",
      "\n",
      "  .    .    .   basically all user stories must fall within this structure  .    therefore all user stories must fall under an umbrella feature and all features must fall under an epic  .  \n",
      "\n",
      "to me  ,   that sounds awkward  .    can someone please clarify how user stories  ,   features  ,   and epics are related  ?    or is there an article that clearly outlines the differences  ?\n",
      "i caution you against applying too rigid a hierarchy to these terms  .    we tried to do that in my previous job  .    twice  .    both attempts were different and both times we found we had unnecessarily limited ourselves  .    the only constant was the definition of a user story  .    from a planning perspective  ,   a story is the basic building block of a project  .    the larger terms   (  epic  ,   feature  ,   etc  .    )   are effectively just tags  .    tags are an easy way to allow a story to exist as part of multiple epics and multiple features at the same time  .    it is not worth the mental effort to be more strict than that  .  \n",
      "\n",
      "tags work for stack exchange and they can work for you too  .\n",
      "how do different tissue culture matrices affect background in fluorescent microscopy  ?   in response to my previous question  ,   I have been reading up a little bit on poly  -  d  -  lysine  ,   collagen i  ,   collagen iv  ,   laminin  ,   and other tissue culture coatings that promote cell adhesion  .   I have always assumed that anything other than standard tc  -  treated plastic or glass would significantly increase background  ,   but perhaps my views on background fluorescence are a little outdated  .   does anybody have experience with these in a fluorescent microscopy  /  high  -  throughput screening environment  ?  \n",
      "\n",
      "specifically in my case  ,   I am looking at endocytosis and trafficking of a labeled protein into the lysosome  .   I am labeling the protein with the ph  -  dependent dye phrodotm from molecular probes  ,   which supposedly has very little fluorescence at neutral ph  ,   but becomes very bright as the ph drops when endocytic vesicles become lysosomes  .   this theoretically means that a final wash step is not needed  ,   but with a matrix coating on the plates I am worried about background  .  \n",
      "\n",
      "so  ,   what is the current thinking as far as background fluorescence of the various tc matrices is concerned  ?   does the background come from the matrix itself  ,   or by the fluorescent dye becoming adsorbed to it  ?   is it wavelength  -  dependent  ?   fortunately i may not be stuck with my poorly  -  adhering cells  ,   and i may not need supplemental matrix at all in the end  ,   but i still want a better understanding of how it works  .\n",
      "extracellular matrix   (  ecm  )   fluoresces  ,   especially collagen and laminin  .   the maximum is in the dapi and fitc channels and the fluorescence becomes weaker towards longer wavelengths  .   however  ,   since the coat on the tc flasks is very thin  ,   i would not expect this to be a problem  .   the best thing is just to try it  .   there is also a quite famous document available which might be of help  :  \n",
      "\n",
      "autofluorescence  :   causes and cures\n",
      "tips for finding spam links injected into the  _  content I am working on a client  '  s site and i noticed that posts have a hidden   &  lt  ;  div  &  gt  ;   filled to spam links to dick pills  ,   etc  .   hoping to get lucky  ,   i searched for some of the keywords in the database tables  ,   but found no matches  .   i also searched the code in all the files  ,   and also found no matches  .  \n",
      "\n",
      "i know that wordpress hacks can be very tricky to remove  ,   and they go to great lengths to make them hard to find  .   but perhaps there are some   \"  usual suspects  \"   that i could check  ,   or maybe some tell  -  tale signs i could look for  .   \n",
      "\n",
      "I am not asking for anyone to solve this specific hack  .   I am just looking for advice on where   (  in general  )   to look  .  \n",
      "\n",
      "in case it is useful  ,   here  '  s the unauthorized   &  lt  ;  div  &  gt  ;   which is injected right before the close of the first   &  lt  ;    \\  p  &  gt  ;    :  \n",
      "\n",
      "  &  lt  ;  div id  =    '  hidemeya  '    &  gt  ;   at that requires looking for how you http  :    /    /  www  .  cialis  .  com   &  lt  ;  a href  =    \"  http  :    /    /  wwxcashadvancecom  .  com  /    \"   title  =    \"  want   $     ?   visit our site  .    \"    &  gt  ;  want   $     ?   visit our site  .    &  lt  ;    /  a  &  gt  ;   sign any of money  .   visit our secure bad creditors that cialis levitra sales viagra   &  lt  ;  a href  =    \"  http  :    /    /  www   .  c viagra   .  com  /    \"   title  =    \"  viagra australia online  \"    &  gt  ;  viagra australia online  &  lt  ;    /  a  &  gt  ;   payday lenders know otherwise  .   but the black you stay on discount price levitra   &  lt  ;  a href  =    \"  http  :    /    /  www   .  x cialis   .  com  /    \"   title  =    \"  what is impotence in men  \"    &  gt  ;  what is impotence in men  &  lt  ;    /  a  &  gt  ;   duty to their lives  .   citizen at one online or after receiving their research viagra online   &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  www  .  viagra  .  com  \"    &  gt  ;  www  .  viagra  .  com  &  lt  ;    /  a  &  gt  ;   to just take just wait until monday  .   this specifically relates to shop every pay stubs get viagra without prescription   &  lt  ;  a href  =    \"  http  :    /    /  www   .   cialis   .  com  /    \"   title  =    \"  cialis overnight delivery  \"    &  gt  ;  cialis overnight delivery  &  lt  ;    /  a  &  gt  ;   and only used or faxing required  .   we will turn double checked by some small business loans viagra for woman   &  lt  ;  a href  =    \"  http  :    /    /  www   .  x cialis   .  com  /    \"   title  =    \"  cialis india  \"    &  gt  ;  cialis india  &  lt  ;    /  a  &  gt  ;   sites that works the business before approval  .   living paycheck went out cash there would generate levitra   &  lt  ;  a href  =    \"  http  :    /    /  www   .  a viagra   .  com  /    \"   title  =    \"  viagra cialis levitra  \"    &  gt  ;  viagra cialis levitra  &  lt  ;    /  a  &  gt  ;   the scheduled maturity day method  .   own a short application on when money also buy cialis online   &  lt  ;  a href  =    \"  http  :    /    /  buy kamagra  .  com  /    \"   title  =    \"  kamagra  \"    &  gt  ;  kamagra  &  lt  ;    /  a  &  gt  ;   plenty of personal initial limits  .   even those loans quick because lenders realize http  :    /    /  cialis  -  ca  -  online  .  com   &  lt  ;  a href  =    \"  http  :    /    /  levitra au  .  com  /    \"   title  =    \"  levitrafroaustraila  \"    &  gt  ;  levitrafroaustraila  &  lt  ;    /  a  &  gt  ;   you notice a payday advance  .   a loan applications are more common thanks http  :    /    /  www  .  cialis au  .  com  /     &  lt  ;  a href  =    \"  http  :    /    /  buy  -   cialis  .  com  /    \"   title  =    \"  cialis  \"    &  gt  ;  cialis  &  lt  ;    /  a  &  gt  ;   to only apply online website  .   third borrowers will use your paycheck to levitra online pharmacy   &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  viagra online purchase  \"    &  gt  ;  viagra online purchase  &  lt  ;    /  a  &  gt  ;   utilize these individuals can cover  .   often there must also referred to ensure online pharmacy viagra usa   &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  viagra effectiveness  \"    &  gt  ;  viagra effectiveness  &  lt  ;    /  a  &  gt  ;   you with financial expenses  .   thanks to checking account also merchant cash loan wwwwviagracom  .  com   &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  levitra viagra cialis  \"    &  gt  ;  levitra viagra cialis  &  lt  ;    /  a  &  gt  ;   comparison to state or from there  .   at that they pay them in mere viagra   &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  cheapest generic viagra  \"    &  gt  ;  cheapest generic viagra  &  lt  ;    /  a  &  gt  ;   seconds and to comprehend  .   if a repossession or limited to see if approved www  .  cashadvances  .  com    |   apply for a cash advance online  !     &  lt  ;  a href  =    \"  http  :    /    /  www   .   cialis   .  com  /    \"   title  =    \"  cialis dosage  \"    &  gt  ;  cialis dosage  &  lt  ;    /  a  &  gt  ;   the risks associated at your current address  .   second borrowers should not start and struggle http  :    /    /  www  .  cashadvance  .  com   &  lt  ;  a href  =    \"  http  :    /    /  levitra  -  online  -  ca  .  com  /    \"   title  =    \"  levitra for sale  \"    &  gt  ;  levitra for sale  &  lt  ;    /  a  &  gt  ;   at least a button  .   thanks to send the benefits of everyday living cheapest viagra order online   &  lt  ;  a href  =    \"  http  :    /    /  www   .   cialis   .  com  /    \"   title  =    \"  tadalafil  \"    &  gt  ;  tadalafil  &  lt  ;    /  a  &  gt  ;   from being foreclosed on its benefits  .   finally you get help rebuild the original loan buy cialis viagra   &  lt  ;  a href  =    \"  http  :    /    /  viagra online  .  com  /    \"   title  =    \"  viagra without prescription  \"    &  gt  ;  viagra without prescription  &  lt  ;    /  a  &  gt  ;   can really only to surprises  .   bank loans out you will take http  :    /    /  wviagracom  .  com  /     &  lt  ;  a href  =    \"  http  :    /    /  www   .   cialis   .  com  /    \"   title  =    \"  erectile dysfunction supplements  \"    &  gt  ;  erectile dysfunction supplements  &  lt  ;    /  a  &  gt  ;   the conditions are a  .   bills might provide an unexpected car cialis uk suppliers   &  lt  ;  a href  =    \"  http  :    /    /  kamagra  -  ca  -  online  .  com  /    \"   title  =    \"  kamagra  \"    &  gt  ;  kamagra  &  lt  ;    /  a  &  gt  ;   broke a repayment length  .   third borrowers repay because payday industry has the results http  :    /    /  www  .  buy levitra  .  com  /     &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  viagra recreational use  \"    &  gt  ;  viagra recreational use  &  lt  ;    /  a  &  gt  ;   by the middle man and check process  .   after verifying your question with dignity and credit cards www  .  levitra  .  com   &  lt  ;  a href  =    \"  http  :    /    /  www   .  b viagra   .  com  /    \"   title  =    \"  overnight viagra delivery  \"    &  gt  ;  overnight viagra delivery  &  lt  ;    /  a  &  gt  ;   or drive to secure loan online  .   most people for dollars you between bad and free cialis   &  lt  ;  a href  =    \"  http  :    /    /  viagra au  .  com  /    \"   title  =    \"  http  :    /    /  viagra au  .  com  /    \"    &  gt  ;  http  :    /    /  viagra au  .  com  /    &  lt  ;    /  a  &  gt  ;   instead these applicants is available  .   social security against your payday the larger sums buying viagra online   &  lt  ;  a href  =    \"  http  :    /    /  payday online  .  com  /    \"   title  =    \"  direct lenders installment loans no credit check  \"    &  gt  ;  direct lenders installment loans no credit check  &  lt  ;    /  a  &  gt  ;   of gossip when working telephone calls  .   face it provides hour payday industry levitra online   &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  buy viagra now  \"    &  gt  ;  buy viagra now  &  lt  ;    /  a  &  gt  ;   has high credit score  ?   within minutes during your best score range from http  :    /    /  cashadvance online  .  com   &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  viagra dosage instructions  \"    &  gt  ;  viagra dosage instructions  &  lt  ;    /  a  &  gt  ;   fees if there for them most  .   to avoid paperwork you in crisis arise from wwwpaydayloancom  .  com   |   online payday loans application form  !     &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  super active viagra  \"    &  gt  ;  super active viagra  &  lt  ;    /  a  &  gt  ;   online from paying the bank  ?   funds will know to throwing your cash advance no credit check   &  lt  ;  a href  =    \"  http  :    /    /  orderviagrauaonline  .  com  /    \"   title  =    \"  viagara online  \"    &  gt  ;  viagara online  &  lt  ;    /  a  &  gt  ;   finances there that purse  .   companies realize that asks for which can become cialis online   &  lt  ;  a href  =    \"  http  :    /    /  www   .   viagra   .  com  /    \"   title  =    \"  sublingual viagra  \"    &  gt  ;  sublingual viagra  &  lt  ;    /  a  &  gt  ;   eligible to paycheck some lenders  .   medical bills that be much easier than actually need only online cash advance   &  lt  ;  a href  =    \"  http  :    /    /  cashadvance online  .  com  \"   title  =    \"  online cash advance  \"    &  gt  ;  online cash advance  &  lt  ;    /  a  &  gt  ;   your funds via the freedom you out  .      &  lt  ;    /  div  &  gt  ;    &  lt  ;  script type  =    '  text  /  javascript  '    &  gt  ;  if  (  document  .  getelementbyid  (    '  hidemeya  '    )     !    =   null  )    {  document  .  getelementbyid  (    '  hidemeya  '    )    .  style  .  visibility   =     '  hidden  '    ;  document  .  getelementbyid  (    '  hidemeya  '    )    .  style  .  display   =     '  none  '    ;    }    &  lt  ;    /  script  &  gt  ;     &  lt  ;    /  p  &  gt  ;\n",
      "in general  ,   the best place to look is in your theme folders  ,   specifically the main theme and in the index  .  php file  .   then the footer and header files  .  \n",
      "\n",
      "also  ,   check your modified dates and start with the most recently modified  .   especially if there are several that were all modified around the same date  /  time  .  \n",
      "\n",
      "I have seen  ,   and had to fix  ,   this problem several times on people  '  s servers  .   the false data is normally loaded via a script inserted into the php files of the template  .  \n",
      "\n",
      "first  ,   you should definitely read over the wordpress faq for dealing with a hacked site  .  \n",
      "\n",
      "common entry points for gaining the   \"  access  \"   required to pull the injection off are outdated themes and  /  or plugins  .   it is best to run production servers with only the one active theme on the server  ,   as well as removing unused plugins and replacing outdated active plugins  .  \n",
      "\n",
      "there are a few scripts out there that you can upload to your server to help you find infected files so you can replace  ,   clean or delete them  .     (  links listed below  )  \n",
      "\n",
      "again  ,   try looking at file modified dates and check out ones you have not modified  /  installed recently yet have a recent date on the  .  \n",
      "\n",
      "\n",
      "look for bad guys   :   I have used this one  .   it takes a bunch of configuration to root out all the false positives   .    .    .   but it can at least help you get a general idea of which files to check out  .  \n",
      "wordfence offers the ability to scan your files with their free plugin  ,   i cannot personally endorse it though  ,   since i have not used it myself  .  \n",
      "https  :    /    /  www  .  wordfence  .  com  /  docs  /  how  -  to  -  clean  -  a  -  hacked  -  wordpress  -  site  -  using  -  wordfence  /  \n",
      "sucuri is linked on the wordpress forums frequently and they have a plugin for scanning as well  ,   but once again  .    .    .   have not personally used it  .   but I am a hands  -  on kind of person  .\n",
      "photo icons have generic picture only when going to a jpg photo folder os c drive and opening it to view photos in an icon mode the icons only show a generic picture  .   if i click on the icon then the actual photo comes up  .   it seems like this problem just started happening  .   in the past i would open a folder and view icons and the photos were all there to view at once  .   i do not believe i made any intentinal changes  .   thanks for the help  .\n",
      "i had this problem with my new installation of windows    .     .  \n",
      "\n",
      "after i had installed most of the regular windows applications i was using in windows    ,   i started on a getting  -  acquainted tour of the new os and its   \"  modernui  \"  apps  .  \n",
      "\n",
      "the generic file type icons displayed in the photos app really puzzled me  ,   but a lot of internet searching finally led to the solution  :   any installation of a recent version of the open source office suite   \"  libreoffice  \"   changes the windows registry of common graphics file types in an undesirable way  ,   so that they are treated as a document rather than a picture  .  \n",
      "\n",
      "there is a very long thread about the problems and solutions at http  :    /    /  answers  .  microsoft  .  com  /  en  -  us  /  windows  /  forum  /  windows   _     -  pictures  /  windows  -     -  photos  -  app  -  does  -  not  -  show  -  any  -  photos  /   b bc  -  d d  -   d   -  afe   -  c a cdecd   ?  page  =     &  amp  ;  msgid  =   cdf   -  aab   -   a b  -   aa  -  e f d b fc\n",
      "\n",
      "the simplest solution presented was a registry file which has to be reapplied every time libreoffice is upgraded  .   the file must contain the following text and be named with a   .  reg suffix  ,   then double click on it to load it into the registry  .   you may have to log out and back in  ,   or even restart the computer  -    -  i cannot remember which i did to get it to work  .  \n",
      "\n",
      "windows registry editor version    .   \n",
      "\n",
      "  [  hkey  _  local  _  machine  \\  software  \\  microsoft  \\  windows  \\  currentversion  \\  explorer  \\  kindmap  ]  \n",
      "  \"    .  wpg  \"    =    \"  picture  \"  \n",
      "  \"    .  dxf  \"    =    \"  picture  \"  \n",
      "  \"    .  emf  \"    =    \"  picture  \"  \n",
      "  \"    .  eps  \"    =    \"  picture  \"  \n",
      "  \"    .  met  \"    =    \"  picture  \"  \n",
      "  \"    .  pct  \"    =    \"  picture  \"  \n",
      "  \"    .  wmf  \"    =    \"  picture  \"  \n",
      "  \"    .  bmp  \"    =    \"  picture  \"  \n",
      "  \"    .  cdr  \"    =    \"  picture  \"  \n",
      "  \"    .  cmx  \"    =    \"  picture  \"  \n",
      "  \"    .  gif  \"    =    \"  picture  \"  \n",
      "  \"    .  jpg  \"    =    \"  picture  \"  \n",
      "  \"    .  jpeg  \"    =    \"  picture  \"  \n",
      "  \"    .  jpe  \"    =    \"  picture  \"  \n",
      "  \"    .  pbm  \"    =    \"  picture  \"  \n",
      "  \"    .  pcx  \"    =    \"  picture  \"  \n",
      "  \"    .  pgm  \"    =    \"  picture  \"  \n",
      "  \"    .  png  \"    =    \"  picture  \"  \n",
      "  \"    .  ppm  \"    =    \"  picture  \"  \n",
      "  \"    .  psd  \"    =    \"  picture  \"  \n",
      "  \"    .  ras  \"    =    \"  picture  \"  \n",
      "  \"    .  svg  \"    =    \"  picture  \"  \n",
      "  \"    .  tga  \"    =    \"  picture  \"  \n",
      "  \"    .  tif  \"    =    \"  picture  \"  \n",
      "  \"    .  tiff  \"    =    \"  picture  \"  \n",
      "  \"    .  xbm  \"    =    \"  picture  \"  \n",
      "  \"    .  xpm  \"    =    \"  picture  \"  \n",
      "  \"    .  pcd  \"    =    \"  picture  \"  \n",
      "\n",
      "\n",
      "i hope this helps  ,   but i am not   (  yet  )   sufficiently familiar with windows internals to explain the problem and the solution presented here  .\n",
      "more than   resistors in parallel most people know the formula for the total resistance of parallel resistors  :  \n",
      "\n",
      "  \\    $     \\  dfrac  {     }    {  r  _  t  }     =     \\  dfrac  {     }    {  r  _     }     +     \\  dfrac  {     }    {  r  _     }     +     {    }    .    .    .    {    }     +     \\  dfrac  {     }    {  r  _  n  }     \\    $  \n",
      "\n",
      "if there are only   resistors  ,   that can be easily rearranged to solve for rt  :  \n",
      "\n",
      "  \\    $     {  r  _  t  }     =     \\  dfrac  {    (  r  _      \\  cdot r  _     )    }    {    (  r  _      +   r  _     )    }     \\    $  \n",
      "\n",
      "is there a safe way to do that for n resistors  ?\n",
      "this is not an answer to your question but rather  ,   additional information that may   (  or may not  )   be helpful in thinking about this kind of problem  .  \n",
      "\n",
      "when i teach introductory circuit classes  ,   i always emphasize the notion of duality which  ,   when mastered  ,   can give you deep insight into many fundamental   \"  rules  \"   of circuit analysis  .  \n",
      "\n",
      "the idea is that if you know the answer for  ,   say  ,   a series circuit  ,   you can take the dual of the result and get a correct answer for a seemingly very different problem  .  \n",
      "\n",
      "so  ,   here is short list of circuit duals  :  \n",
      "\n",
      "\n",
      "  voltage   -   current\n",
      "  \n",
      "  resistance   -   conductance\n",
      "  \n",
      "  inductance   -   capacitance\n",
      "  \n",
      "  impedance   -   admittance\n",
      "  \n",
      "  series   -   parallel\n",
      "  \n",
      "  thevenin   -   norton\n",
      "\n",
      "\n",
      "there are others but these will do most of the time  .  \n",
      "\n",
      "ohm  '  s law is usually written as  :  \n",
      "\n",
      "  \\    $  v   =   i r   \\    $  \n",
      "\n",
      "to take the dual  ,   replace all the variables in the above equation with their duals  :  \n",
      "\n",
      "the dual of ohm  '  s law  :  \n",
      "\n",
      "  \\    $  i   =   vg   \\    $  \n",
      "\n",
      "where   \\    $  g   =     \\  dfrac  {     }    {  r  }     \\    $  \n",
      "\n",
      "recall that for resistors in series  ,   resistances add  ,   so that the equivalent resistance is just the sum  .  \n",
      "\n",
      "consider the dual of this  ,   conductances in parallel  .  \n",
      "\n",
      "from the principle of duality  ,   parallel conductances add just as series resistances  .    so  ,   if you have   conductances in parallel  ,   the equivalent conductance is  :  \n",
      "\n",
      "  \\    $  g  _    {  eq  }     =   g  _      +   g  _      +   g  _      \\    $  \n",
      "\n",
      "now  ,   convert back to resistance  :  \n",
      "\n",
      "  \\    $  r  _    {  eq  }     =     \\  dfrac  {     }    {  g  _    {  eq  }    }     =     \\  dfrac  {     }    {  g  _      +   g  _      +   g  _     }     =     \\  dfrac  {     }    {    \\  frac  {     }    {  r  _     }    +    \\  frac  {     }    {  r  _     }     +    \\  frac  {     }    {  r  _     }    }    \\    $  \n",
      "\n",
      "in other words  ,   the equivalent resistance of   \\    $  n  \\    $   parallel resistors is the reciprocal of the sum of the reciprocals  .  \n",
      "\n",
      "this is the origin of your first formula  .\n",
      "c  #   test assembly i am new to c  #   and am having a particularly difficult time figuring out how test assemblies work  .   this is my solution directory   (  with a single project myproject inside of it  )    :  \n",
      "\n",
      "mysolution  /  mysolutionfile  .  sln       -    -   my solution file\n",
      "mysolution  /  packages                 -    -   my packages directory\n",
      "mysolution  /  myproject  /  properties     -    -   my project properties\n",
      "\n",
      "mysolution  /  myproject  /  src                      -    -   my source code\n",
      "                      /  src  /  myclass   .  cs\n",
      "                      /  src  /  myclass   .  cs\n",
      "\n",
      "mysolution  /  myproject  /  test                     -    -   my nunit tests\n",
      "                      /  test  /  myclass test  .  cs\n",
      "                      /  test  /  myclass test  .  cs\n",
      "\n",
      "mysolution  /  myproject  /  themainclass  .  cs\n",
      "\n",
      "\n",
      "now i can run all the tests in the test folder from visual studio  .   however i want to run the tests from a ci system like teamcity  ,   in which case i need an assembly path  .   how do i generate just the test folder into an assembly to be run  .  \n",
      "\n",
      "do i need to add to the main class a method that runs each test in the mysolution  /  test folder  ?\n",
      "i think you need to read about projects and solutions  :   http  :    /    /  msdn  .  microsoft  .  com  /  en  -  us  /  library  /  ee   .  aspx\n",
      "\n",
      "and after that it all becomes more clear  :   have one solution  ,   inside of that solution create a project for your application and a project for your unit  -  tests  .   in test  -  project add reference to the testing framework of your choice and a reference to your application  -  project  .   \n",
      "\n",
      "this way your application does not know about your tests and compiled into one assembly  .   at the same time your tests depend on your application  ,   but compiled into another assembly  ,   which can be used by your test  -  runner gui  /  ci or whatever else you use  .  \n",
      "\n",
      "and to answer your next question  ,   for test  -  project you need to choose project type of   \"  library  \"     (  console application will work as well if you like  )\n",
      "is there a general term for a single note or a chord  ?   take a passage like this  :  \n",
      "\n",
      "\n",
      "\n",
      "fill in the blank  :   each of these boxes denote a   _    _    _    _    _    _  \n",
      "\n",
      "is there a single general term for these that is better than note or chord  ?   or maybe there is a term for   \"  anything that has a duration  \"   that also encompasses rests  ?   it seems like enough of a fundamental concept that it should have a name  .\n",
      "for formal  ,   technical purposes   (  e  .  g  .   when discussing musical audiation and other aspects of musical cognition  )   the terms   \"  acoustic event  \"   or   \"  notated event  \"   or   \"  vertical event  \"   is pretty much standard terminology within psychology of music for referring broadly to any individual single tone or simultaneosly experienced combination of tones   (  i  .  e  .   an individual chord  )   within a passage or composition  .   an acoustic event can also include a silence  .   really useful all  -  inclusive terms  ,     (  though not yet in  musicians  '   common parlance  )    !  \n",
      "\n",
      "so  ,   I had  use the term   \"  notated event  \"      (  or the more generalized   \"  vertical event  \"    )   to fill in the blank in the op  '  s question  .\n",
      "how can i login to a website from terminal  ?   as a student of computer science  ,   i am now learning to work with command line  .  \n",
      "i need  to use brute force to login to a website  ,   knowing the username and that the password is an integer in  a certain range  .   this is what i have so far but it does not seem to work  .   \n",
      "\n",
      " for i in   {     .    .     }    ;   do curl http  :    /    /  admin  :    $  i  @  mywebsite  .  com  /  link  ;   done\n",
      "\n",
      "\n",
      "how can i proceed  ?\n",
      "what your routine is lacking is some way to exit once the correct i is found  .  \n",
      "for that you can look at the exit code given back from curl  :  \n",
      "\n",
      "for i in   {     .    .     }  \n",
      "do \n",
      "  curl http  :    /    /  admin  :    $  i  @  mywebsite  .  com  /  link\n",
      "  if   [     \"    $    ?    \"     -  eq     ]    ;   then\n",
      "    echo found   \"    $  i  \"  \n",
      "    break\n",
      "  fi\n",
      "done\n",
      "\n",
      "\n",
      "the exit value of curl is   when everything is correct and can be checked directly after the program stops by inspecting the special variable   $    ?    .  \n",
      "use man curl and search for exit code to see all the different things curl can tell you with its exit code  .\n",
      "export error with addon im trying to export a model for the euro truck simulator   game with the blender scs addon and i get the following error  ,   any help  ?  \n",
      "\n",
      "traceback   (  most recent call last  )    :  \n",
      "  file   \"  c  :    \\  program files  \\  blender foundation  \\  blender  \\     .     \\  scripts  \\  addons  \\  io  _  scene\n",
      "  _  scs  \\    _    _  init  _    _    .  py  \"    ,   line    ,   in execute\n",
      "    error   =   export  _  scs  .  save  (  filepath  ,   origin  _  path  ,   root  _  object  ,   self  .  copy  _  textur\n",
      "es  ,   int  (  self  .  pmg  _  version  )    )  \n",
      "  file   \"  c  :    \\  program files  \\  blender foundation  \\  blender  \\     .     \\  scripts  \\  addons  \\  io  _  scene\n",
      "  _  scs  \\  export  _  scs  .  py  \"    ,   line    ,   in save\n",
      "    status  ,   ob   =   export  _  pmd  .  save  (  exportpath  ,   originpath  ,   root  _  ob  ,   copy  _  textures  )  \n",
      "\n",
      "  file   \"  c  :    \\  program files  \\  blender foundation  \\  blender  \\     .     \\  scripts  \\  addons  \\  io  _  scene\n",
      "  _  scs  \\  export  _  pmd  .  py  \"    ,   line    ,   in save\n",
      "    pmd  .  write  (  f  ,   exportpath  ,   originpath  ,   copy  _  tex  )  \n",
      "  file   \"  c  :    \\  program files  \\  blender foundation  \\  blender  \\     .     \\  scripts  \\  addons  \\  io  _  scene\n",
      "  _  scs  \\  export  _  pmd  .  py  \"    ,   line    ,   in write\n",
      "    ret  _  ob   =   exp  _  mat  .  write  (  copy  _  tex  )  \n",
      "  file   \"  c  :    \\  program files  \\  blender foundation  \\  blender  \\     .     \\  scripts  \\  addons  \\  io  _  scene\n",
      "  _  scs  \\  export  _  mat  .  py  \"    ,   line    ,   in write\n",
      "    for option in self  .    _    _  options  .  keys  (    )    :  \n",
      "attributeerror  :     '  str  '   object has no attribute   '  keys  '  \n",
      "\n",
      "location  :     &  lt  ;  unknown location  &  gt  ;    :    -   \n",
      "\n",
      "location  :     &  lt  ;  unknown location  &  gt  ;    :    -\n",
      "the addon seems to be broken due to api changes in blender  ,   either use the addons bug tracker or try an older version of blender that is compatible  .   according to the video tutorials version    .    was used  .\n",
      "java final variables changes at the execution time i do not know the reason fo that  .   maybe you could help me\n",
      "\n",
      "so code here creating a frame with   sliders  .  \n",
      "\n",
      "public class myframe extends jframe   {  \n",
      "\n",
      "\n",
      "imagepanel imagepanel  ;  \n",
      "final int minimum   =      ;  \n",
      "final int maximum   =      ;  \n",
      "final int numberofspheres   =      ;  \n",
      "final int numberofscales   =      ;  \n",
      "myrandomaccessfile file  ;  \n",
      "\n",
      "final string  [    ]   s   =     {    \"  друзья и окружение  \"    ,  \n",
      "          \"  отношения  \"    ,     \"  карьера и бизнес  \"    ,  \n",
      "          \"  финансы  \"    ,     \"  духовность и творчество  \"    ,  \n",
      "          \"  личностный рост  \"    ,     \"  яркость жизни  \"    ,     \"  здоровье и спорт  \"    }    ;  \n",
      "\n",
      "private final color  [    ]   colors   =     {  color  .  red  ,   color  .  orange  ,   color  .  yellow  ,   color  .  green  ,   color  .  blue  ,   color  .  pink  ,   color  .  magenta  ,   color  .  dark  _  gray  }    ;  \n",
      "\n",
      "private final int  [    ]   array   =     {     ,      ,      ,      ,      ,      ,      ,      }    ;  \n",
      "\n",
      "public myframe   (    )     {  \n",
      "\n",
      "    settitle  (    \"  wheel of life  \"    )    ;  \n",
      "\n",
      "    myslider  [    ]   sliders   =   new myslider  [  numberofspheres  ]    ;  \n",
      "    jbutton savebutton   =   new jbutton  (    \"  save  \"    )    ;  \n",
      "    myactionlistener listener    =   new myactionlistener  (    )    ;  \n",
      "    savebutton  .  addactionlistener  (  listener   )    ;  \n",
      "\n",
      "    file   =   new myrandomaccessfile  (    )    ;  \n",
      "\n",
      "      /    /  string s    =     \"                   \"    ;  \n",
      "      /    /  array   =   stringtointarray  (  s   )    ;  \n",
      "    array  [     ]     =      ;  \n",
      "\n",
      "    jlabel  [    ]   labels   =   new jlabel  [  numberofspheres  ]    ;  \n",
      "    imagepanel   =   new imagepanel  (  colors  ,   array  )    ;  \n",
      "    system  .  out  .  println  (  array  [     ]    )    ;  \n",
      "    jpanel mainpanel   =   new jpanel  (    )    ;  \n",
      "    jpanel  [    ]   sliderpanels   =   new jpanel  [  numberofspheres  ]    ;  \n",
      "    jpanel mainsliderpanel   =   new jpanel  (  new gridlayout  (     ,      ,      ,      )    )    ;  \n",
      "    mychangelistener listener   =   new mychangelistener  (    )    ;  \n",
      "\n",
      "    for   (  int i   =      ;   i   &  lt  ;   numberofspheres  ;   i  +    +    )     {  \n",
      "        sliders  [  i  ]     =   new myslider  (  s  [  i  ]    )    ;  \n",
      "        sliders  [  i  ]    .  addchangelistener  (  listener  )    ;  \n",
      "        labels  [  i  ]     =   new jlabel  (  s  [  i  ]    )    ;  \n",
      "        labels  [  i  ]    .  setforeground  (  colors  [  i  ]    )    ;  \n",
      "        labels  [  i  ]    .  setfont  (  new font  (    \"  droid sans  \"    ,   font  .  bold  ,      )    )    ;  \n",
      "        sliderpanels  [  i  ]     =   new jpanel  (    )    ;  \n",
      "        sliders  [  i  ]    .  setminimum  (  minimum  )    ;  \n",
      "        sliders  [  i  ]    .  setmaximum  (  maximum  )    ;  \n",
      "        system  .  out  .  print  (  array  [  i  ]    )    ;  \n",
      "        sliders  [  i  ]    .  setvalue  (     )    ;  \n",
      "        sliders  [  i  ]    .  setmajortickspacing  (     )    ;  \n",
      "        sliders  [  i  ]    .  setminortickspacing  (    (  int  )      .     )    ;  \n",
      "        sliders  [  i  ]    .  setpaintlabels  (  true  )    ;  \n",
      "        sliders  [  i  ]    .  setpaintticks  (  true  )    ;  \n",
      "        sliderpanels  [  i  ]    .  setlayout  (  new gridlayout  (     ,      ,      ,      )    )    ;  \n",
      "        sliderpanels  [  i  ]    .  add  (  sliders  [  i  ]    )    ;  \n",
      "        sliderpanels  [  i  ]    .  add  (  labels  [  i  ]    )    ;  \n",
      "        mainsliderpanel  .  add  (  sliderpanels  [  i  ]    )    ;  \n",
      "\n",
      "      }  \n",
      "\n",
      "    mainpanel  .  setlayout  (  new borderlayout  (    )    )    ;  \n",
      "    mainpanel  .  add  (  imagepanel  ,   borderlayout  .  center  )    ;  \n",
      "    mainpanel  .  add  (  mainsliderpanel  ,   borderlayout  .  east  )    ;  \n",
      "    mainpanel  .  add  (  savebutton  ,   borderlayout  .  south  )    ;  \n",
      "\n",
      "    add  (  mainpanel  )    ;  \n",
      "  }  \n",
      "\n",
      "\n",
      "and the output is this\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "how can final variable change its value at the execution time  ,   what the hell  ?    ?    ?  \n",
      "actual value of variable is depend on value that i writing at sliders  [  i  ]    .  setvalue  (     )    ;   but i do not know how exactly  .    .    .  \n",
      "\n",
      "and i tried to set a watchpoint for this variable  .    .    .   does not working  .   one time its zero and next time program stop in this for loop value is    ,     or whatever  .    .    .\n",
      "setting final to array does not mean that u cant add or change element value  ,   but it means now you can assign another array reference to this variable  .   but you can add   ,   update or remove the elements in that array\n",
      "difference between busses i think i am confusing the difference between some of the of busses  ,   such as ide  ,   sata  ,   usb  ,   and pci  .    what is the relationship between all four  ,   how are they connected to each other  ?   from what i read it seems like pci connects them together as well as to the cpu  ,   but it is not clear  .    any help would be greatly appreciated  .   i am cross referencing this post with another i made about the linux commands to browse them  .    http  :    /    /  unix  .  stackexchange  .  com  /  questions  /     /  ide  -  and  -  pci  -  bus  -  commands\n",
      "the interrelationship of the different busses is roughly as follows  :  \n",
      "\n",
      "                                                 /   sata\n",
      "cpu   =    &  gt  ;   northbridge   =    &  gt  ;   pci bus   =    &  gt  ;   southbridge   =    &  gt  ;    ide\n",
      "                                                 \\   usb\n",
      "\n",
      "\n",
      "where the northbridge and southbridge are names given to the two main controller chips inside a pc  .  \n",
      "\n",
      "ide and sata both perform the same job but through different physical media   -   they are for attaching hard drives etc  .  \n",
      "\n",
      "ide is   \"  integrated device electronics  \"     -   also known as   \"  ata  \"   or   \"  atapi  \"     (  ata peripheral interface  )    .  \n",
      "\n",
      "sata is   \"  serial ata  \"     -   the same ata protocol but serial instead of parallel  .  \n",
      "\n",
      "usb is a serial communications bus which can communicate with any number of devices  ,   not just hard drives and other storage devices  .    it speaks a completely different protocol to the ata family  .  \n",
      "\n",
      "pci   (  and the derivatives pcie  ,   etc  )   are much closer to the cpu and generally provides much more direct access to the cpu  .  \n",
      "\n",
      "edit  :  \n",
      "\n",
      "you can see how everything is connected together in windows through the device manager set to view devices by connection  :\n",
      "flushing coolant after driving only with water in if i drive my truck for   min with just water in my system  ,   would it be equivalent to just flushing it stationary   (  the proper way to flush it  )    ?   iow  ,   if i just drive around with a clean water refill in the coolant system instead of keeping the truck in place and then draining  ,   would it be an okay way to flush  ?\n",
      "it would make no difference if you are standing still or driving the vehicle  .   you want to ensure you have the heater wide open when you do it to ensure you are getting the old fluid from the heater core as well as the engine  .   driving the vehicle around will probably allow the process to happen a little faster  ,   as you engine will get up to operating temperature faster  .   the real thing you are trying to accomplish is to get the thermostat open to allow everything to circulate  .\n",
      "how does critical strike chance stack  ?   i was wondering how critical strike chance stacks  ?   say i buy a phanton dancer with its    %   crit chance  .   now if i buy another phanton dancer  ,   what is my critical chance  ?\n",
      "they stack additively  ,   so just add up the crit chance  .      pd will increase your total crit chance by   +     %    .    the  nd pd will increase your total crit chance by   +     %    ,   so   +     %   crit chance total from   pd  '  s  .\n",
      "directory  .  getfiles string gives an error i am creating a website using asp  .  net and c  #  \n",
      "i need to access all files one by one in a directory  .   so i wrote the following code  .  \n",
      "\n",
      "    string  [    ]   ssimgs   =   directory  .  getfiles  (    \"  images  /  movies  \"    )    ;  \n",
      "    label   .  text   =   ssimgs  [     ]    ;  \n",
      "    label   .  text   =   ssimgs  [     ]    ;  \n",
      "\n",
      "\n",
      "but it gives an error  .   what is the correct way to do this  ?   thanks in advance  .\n",
      "there are two potential issues here  .  \n",
      "\n",
      "\n",
      "if you doing have using system  .  io  ;   in your file  ,   the directory class will not be found by the compiler  .  \n",
      "the way you are specifying your path   (    \"  images  /  movies  \"    )   will be relative to the current working directory  ,   which is likely not the proper directory  .    you should use server  .  mappath or the path class to build a full path to the proper folder  ,   so you can specify the correct folder with a full  ,   absolute path  .\n",
      "discontinuity at the edge of chebychev window i am using chebychev window for its narrow main lobe  .   the problem of chebychev window is that it has discontinuities at the edge  ,   and it seems that taylor window solves this issue  .   \n",
      "\n",
      "more detail  :  \n",
      "http  :    /    /  de  .  mathworks  .  com  /  help  /  signal  /  ref  /  taylorwin  .  html\n",
      "http  :    /    /  en  .  wikipedia  .  org  /  wiki  /  window  _  function  #  dolph  .  e   .     .   chebyshev  _  window\n",
      "\n",
      "I have searched around but i cannot find any information on how to implement a taylor window  .   any information on taylor window or suggestions on fixing this issue of edge discontinuities would be very appreciated  .\n",
      "a little googling came up with this reference  ,   which indicates that the impulse response for a taylor window is  :  \n",
      "\n",
      "  $    $  \n",
      "h  [  n  ]     =       +       \\  sum  _    {  m  =     }    ^    {    \\  tilde  {  n  }    -     }   f  _  m   \\  cos  \\  left  (    \\  frac  {     \\  pi m  }    {  n  }     \\  left  (  n  -    \\  frac  {  n  }    {     }    +    \\  frac  {     }    {     }    \\  right  )    \\  right  )  \n",
      "  $    $  \n",
      "\n",
      "  $    \\  tilde  {  n  }    $   is a parameter for controlling how many equal  -  height sidelobes there are in the window  .     $  f  _  m  $   is a parameter that is related to the maximum sidelobe height  ;   the references given in matt l  '  s answer give more detail on how it is calculated  .\n",
      "silverlight  :   returning value from synchronous method using rx i am writing a simple silverlight application and wcf service  .  \n",
      "i want to create a synchronous method that return a value  .  \n",
      "the method itself  ,   call an asynchronous method from wcf services  .   after i call asynchronous method  ,   i want to get it value  ,   and return to sender  .  \n",
      "i hear that rx can solve this kind of problem  .  \n",
      "\n",
      "this is my code   :  \n",
      "\n",
      "    private void btncreate  _  click  (  object sender  ,   routedeventargs e  )  \n",
      "      {  \n",
      "        string myresult   =   getmybook  (  txtbookname  .  text  )    ;  \n",
      "        messagebox  .  show  (    \"  result  \\  n  \"     +   myresult  )    ;  \n",
      "          /    /   myresult will be use for another purpose here  .    .  \n",
      "      }  \n",
      "\n",
      "      /    /   i want this method can be called anywhere  ,   as long as the caller still in the same namespace  .  \n",
      "    public string getmybook  (  string bookname  )  \n",
      "      {  \n",
      "        servo  .  servoclient svc   =   new servoclient  (    )    ;  \n",
      "        string returnvalue   =     \"    \"    ;  \n",
      "\n",
      "        var o   =   observable  .  fromeventpattern  &  lt  ;  getbookcompletedeventargs  &  gt  ;    (  svc  ,     \"  getbookcompleted  \"    )    ;  \n",
      "        o  .  subscribe  (  \n",
      "            b   =    &  gt  ;   returnvalue   =   b  .  eventargs  .  result\n",
      "              )    ;  \n",
      "\n",
      "        svc  .  getbookasync  (  bookname  )    ;  \n",
      "        return returnvalue  ;  \n",
      "      }  \n",
      "\n",
      "\n",
      "when i click btncreate  ,   myresult variable still empty  .   is that something wrong with my code  ?   or maybe i am just do not understand with rx concept  ?   i am new to rx  .  \n",
      "\n",
      "my goal is   :   i need to get the result   (  myresult variable  )   from asynchronous method  ,   and then used in later code  .\n",
      "remember that getbookasync returns immediately  ,   and will return the value stored in returnvalue  .   when the data arrives returnvalue will be out of scope  ,   and by then btncreate will have finished  .  \n",
      "\n",
      "u could use await on the getbookasync  ,   so that it will wait for the data to arrive before continuing  .    do not forget that would mean u also need the async on the method  .  \n",
      "\n",
      "not a great example or use of either rx or await  ,   but trying is how we learn  !\n",
      "how to grep words in a file  ?   \n",
      "  possible duplicate  :  \n",
      "  regex for   &  ldquo  ;  or  &  rdquo  ;   in grep  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "how can i grep for lines with either   '  disable  '   or   '  enable  '   in my file  ?  \n",
      "\n",
      "i tried \n",
      "   $   grep     \"  disable  |  enable  \"   fail  .  log \n",
      "\n",
      "but that shows up nothing  .\n",
      "try   \"  grep   \"  disable  \\    |  enable  \"   fail  .  log\n",
      "\n",
      "the backslash turns the   |   into the   \"  or  \"   metacharacter  ,   otherwise you are grepping for the literal   '  disable  |  enable  \"\n",
      "get term weight in node object for ordering with entityfieldquery is it posiible to expose taxonomy term weight in node object   ?  \n",
      "\n",
      "i need to sort nodes using entityfieldquery by term reference field  ,   but weight is not exposed to node object so i can say for example \n",
      "\n",
      "  -    &  gt  ;  fieldorderby  (    '  field  _  custom  _  terms  '    ,     '  weight  '    ,     '  asc  '    )    ;  \n",
      "\n",
      "\n",
      "and sort nodes by weight of terms  .\n",
      "no  ,   this is not possible  .   you can only order on actual field values  ,   the term weight would require a join to a non  -  field table and efq is not capable of doing that  .  \n",
      "\n",
      "you will have to create a manual sql query to do this  .   note that drupal by default maintains the node   &  lt  ;    -    >   term associations in the taxonomy  _  index table  ,   that one is easier and more reliable to query than a field data table  .\n",
      "why is my cv    %   complete  ?   on careers  ,   when filling out my cv  ,   it is listed as being    %   complete  .    could someone tell me why this is  ?\n",
      "%   means that you are    %   better than the average programmer  .    congratulations  .\n",
      "symfony   and twig custom error page i give up  .  \n",
      "\n",
      "the problem is with the overrided twig   template  .   it is overrided in a default way by creating an error   .  html  .  twig under the   /  app  /  resources  /  twigbundle  /  views directory  .  \n",
      "\n",
      "the template itself does not contain any irregular or complex logic  :   just a layout with some translated text   (    |  trans  )   and a menu with several links  .  \n",
      "\n",
      "the problem is  ,   that i cannot get app  .  user object   (  returned as null  )   or current app  .  request  .  locale   (  always returned as default locale  )   inside this template  .  \n",
      "\n",
      "i have even tried to override the twig exception controller and dump a current locale   (  request  :    :  getlocale  (    )    )   or get user   -   the results are the same   -   default locale and null for user  .  \n",
      "\n",
      "then i decided to dig deeper and found a dozens of listeners   (  locale listeners  ,   exception liteners  ,     .    .    .    )   and tried to debug  /  fix  /  test there  ,   but i did not proceed any further  .  \n",
      "\n",
      "by the way  ,   i have overrided the   error page too  ,   and everything is fine there  .   well i guess that when   error   (  exception  )   is thrown  ,   the symfony has already set up user  /  locale  /  etc  ,   because it already has got to the target   (  controller  /  action  )   and other listeners been already executed  .   but   error   (  notfoundhttpexception  )   is being thrown before symfony targets the action  .    .    .  \n",
      "\n",
      "some words about project  :   symfony    .     .     ,   doctrine  ,   gedmo extensions  /  stof bundle  ,   jms i n routing bundle  .  \n",
      "\n",
      "symfony version  :   v   .     .   \n",
      "jms i n routing bundle  :      .     .   \n",
      "\n",
      "appreciate your help  .\n",
      "that is the way i do it   (  at least for   templates  )    :  \n",
      "\n",
      "in app  /  config  /  routing  .  yml append  :  \n",
      "\n",
      "  #  always in last position\n",
      "  #    -    -    -    -    -    -    -    -    -    -    -    -    &  gt  ;  \n",
      "nonexistent  _  route  :  \n",
      "    path  :         /    {  url  }  \n",
      "    defaults  :     {     _  controller  :   acmedemobundle  :  default  :  wrongroute  }  \n",
      "    requirements  :  \n",
      "        url  :     \"    .    +    \"      \n",
      "  #    &  lt  ;    -    -    -    -    -    -    -    -    -    -    -  \n",
      "\n",
      "\n",
      "the controller  :  \n",
      "\n",
      "namespace acme  \\  demobundle  \\  controller  ;  \n",
      "\n",
      "use symfony  \\  bundle  \\  frameworkbundle  \\  controller  \\  controller  ;  \n",
      "use symfony  \\  component  \\  httpkernel  \\  exception  \\  notfoundhttpexception  ;  \n",
      "\n",
      "class defaultcontroller extends controller\n",
      "  {  \n",
      "      .    .    .  \n",
      "\n",
      "    public function wrongrouteaction  (    $  url  )  \n",
      "      {  \n",
      "          $  user   =     $  this  -    &  gt  ;  get  (    '  security  .  context  '    )    -    &  gt  ;  gettoken  (    )    -    &  gt  ;  getuser  (    )    ;  \n",
      "\n",
      "        return   $  this  -    &  gt  ;  render  (    '  twigbundle  :  exception  :  error   .  html  .  twig  '    ,   array  (    \"  user  \"     =    &  gt  ;     $  user  ,     \"  url  \"     =    &  gt  ;     $  url  )    )    ;  \n",
      "\n",
      "      }  \n",
      "  }  \n",
      "\n",
      "\n",
      "and in your twig template app  /  resources  /  twigbundle  /  views  /  exception  /  error   .  html  .  twig you can access   {    {   user   }    }\n",
      "thunderbird    .     :   inbox size is  gb on disk  :   how do i reduce it  ?   mozilla thunderbird    .     :   i have set thunderbird never to delete a message that is on disk  .    .    .  thus  ,   after four short years  ,   i have a  gb inbox file  .   thunderbird needs about   minutes to read it  ,   and even then i cannot compact it  .   anyone have some suggestions  ?\n",
      "there is definitely a  gb limit on windows due to windows limitations which means you will have problem with individual thunderbird mail folders that are larger than  gb  .  \n",
      "\n",
      "and i thought the  gb limit existed on mac and linux as well   (  so i am curious as to how emgee can have a thunderbird folder that is  gb  !   emgee  :   perhaps you are referring to a unified folder being  gb but your individual folders are   &  lt  ;    gb  ?    )  \n",
      "\n",
      "nick  '  s suggestion   (  i  .  e  .   move to new  ,   multiple thunderbird folders each of which is   &  lt  ;   gb  .   please clarify emgee  )   should work  .   more info with a complete procedure   (  change   \"  sent  \"   to   \"  inbox  \"    )    :  \n",
      "http  :    /    /  getsatisfaction  .  com  /  mozilla  _  messaging  /  topics  /  version  _     _     _     _  still  _  has  _  the  _  missing  _  sent  _  message  _  bug  #  reply  _\n"
     ]
    }
   ],
   "source": [
    "for  i  in  range(100):\n",
    "    print(all_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_vocab !!!\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(texts: List[str], max_features: int = 100000) -> Dict[str, Dict]:\n",
    "    print('build_vocab !!!')\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(text.split())\n",
    "    \n",
    "    filename = 'counter.txt'\n",
    "    with  open(filename,'w') as file_object:\n",
    "        file_object.write(str(counter))\n",
    "    vocab = {}\n",
    "    vocab['token2id'] = {\n",
    "        token: _id + 1 for _id, (token, count) in\n",
    "        enumerate(counter.most_common(max_features))}\n",
    "    #counter保留单词相应出现的次数，\n",
    "    #vocab['token2id']对于相应数值进行标号，\n",
    "    #开头标1，接着标2，依次类推)\n",
    "    #这里的_id+1就是为开头位置的'<PAD>'保留相应的位置\n",
    "    \n",
    "    #most_common:出现最多次数的词，counter.most_common(60000)\n",
    "    #找出出现次数最多的60000个对应的单词\n",
    "    vocab['token2id']['<PAD>'] = 0\n",
    "    vocab['token2id']['<UNK>'] = len(vocab['token2id'])\n",
    "    vocab['id2token'] = {v: k for k, v in vocab['token2id'].items()}\n",
    "    vocab['word_freq'] = {\n",
    "        **{'<PAD>': 0, '<UNK>': 0},\n",
    "        **dict(counter.most_common(max_features)),\n",
    "    }\n",
    "    #这样操作的意义在于'<PAD>':0以及'<UNK>':0放在最前面\n",
    "    #后面为接下来的数组依次往下走\n",
    "    filename = 'vocab.txt'\n",
    "    with  open(filename,'w')  as  file_object:\n",
    "        file_object.write(str(vocab))\n",
    "    return vocab\n",
    "vocab = build_vocab(itertools.chain(*train_texts, *test_texts), max_features)\n",
    "#最终vocab由3个部分组成 ['token2id'],['id2token'],['word_freq']三个部分组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python动态参数的两种形式:$*args$和$**kwarg$,这里面关键是一个和两个星号的区别，而不是args和kwargs在名字上的区别。\n",
    "!!!注意:动态参数，必须放在所有的位置参数和默认参数后面!!!\n",
    "~~~\n",
    "def  func(name,age,sex='male',*args,**kwargs):\n",
    "    pass\n",
    "~~~\n",
    "有时候我们传入一个列表，本意是希望将列表中的所有元素都当做参数传递进去，这里直接将$['a','b','c']$看做一个整体了，\n",
    "怎么办?\n",
    "其实只需要在调用时前面加一个$*$号，就能实现将列表中的每个元素传递进去了\n",
    "~~~\n",
    "def  func(*args):\n",
    "    \"表示接收任意个数量的参数，调用时会将实际参数\n",
    "    打包为一个元组传入实参\"\n",
    "    for  i  in  args:\n",
    "        print  i\n",
    "        \n",
    "func(*['a','b','c'])\n",
    "参数依次放入\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts: List[str],\n",
    "             token2id: Dict[str, int],\n",
    "             max_len: int = 200) -> List[List[int]]:\n",
    "    print('tokenize = !!!')\n",
    "    def text2ids(text, token2id, max_len):\n",
    "        return [\n",
    "            token2id.get(token, len(token2id) - 1)\n",
    "            for token in text.split()[:max_len]]\n",
    "    #get(token,len(token2id)-1)中的后面一个内容len(token2id)-1为\n",
    "    #对应的默认值\n",
    "    #将text中的每一个单词进行切割分离，如果未出现的话对应内容为\n",
    "    #len(token2id)-1\n",
    "    #如果出现的话找寻出单词对应的id编号，传入的参数为vocab['token2id']\n",
    "    \n",
    "    tokenized = [\n",
    "        text2ids(text, token2id, max_len)\n",
    "        for text in texts]\n",
    "    \n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(embedding_path: str, word_index: Dict[str, int]) -> np.ndarray:\n",
    "#embedding_path之中放入的是fasttext.pkl的对应的文件\n",
    "    embeddings_index = joblib.load(embedding_path)\n",
    "    #这个从路径中读出对应参数的操作很墨迹\n",
    "    print('load_embedding = !!!')\n",
    "    print('embeddings_index type = ')\n",
    "    for  i,(k,v)  in  enumerate(embeddings_index.items()):\n",
    "        if  i  in  range(0,10):\n",
    "            print(k)\n",
    "            print('###')\n",
    "            print(v.shape)\n",
    "            print('$$$')\n",
    "        else:\n",
    "            break\n",
    "    #从fasttext.pkl中读出来的都是300维的向量，这点\n",
    "    #很重要\n",
    "    \n",
    "    # word_index = tokenizer.word_index\n",
    "    # 这里面的embedding_path对应的为相应的权重文本\n",
    "    nb_words = min(max_features + 2, len(word_index))\n",
    "    print('nb_words = %d'%nb_words)\n",
    "    #nb_words = 46472\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    for key, i in word_index.items():\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        #embeddings_index中获取word为得到相应的vector\n",
    "        #数组对应内容\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        #ps = PorterStemmer()\n",
    "        #将不同形式的单词转换为同一形式\n",
    "        #比如create和created都得到creat\n",
    "        #lc = LancasterStemmer()\n",
    "        #\n",
    "        #sb = SnowballStemmer('english')\n",
    "        #这里面定义了三种对应的形式对单词进行切分\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        #分别通过不同的转化方式去提取出单词中的相应的矩阵\n",
    "    #这里矩阵的内容返回的也是\n",
    "    return embedding_matrix\n",
    "\n",
    "def w2v_fine_tune(all_texts: List[str], vocab: Dict, embedding_matrix: np.ndarray) -> np.ndarray:\n",
    "    print('w2v_fine_tune = !!!')\n",
    "    model = Word2Vec(min_count=1, workers=1, iter=3, size=300)\n",
    "    #min_count可以对字典做截断，词频少于min_count次数的单词会被\n",
    "    #丢弃掉，默认值为5\n",
    "    #workers:用于控制训练的并行数\n",
    "    #size:是指特征向量的维度，默认值为100\n",
    "    model.build_vocab_from_freq(vocab['word_freq'])\n",
    "    #获取对应的词频字典(负采样优先选中词频较高的单词)\n",
    "    idxmap = np.array(\n",
    "        [vocab['token2id'][w] for w in model.wv.index2entity])\n",
    "    #model.wv.index2entity(输出经过映射的词名称)\n",
    "    #model.wv['sky']输出这个词的向量映射\n",
    "    #这里面的'token2id'的对应内容为出现词的对应编号\n",
    "    #频率高的编号,所以idxmap为所有在Word2Vec中出现过的词\n",
    "    #对应的编号的内容\n",
    "    \n",
    "    model.wv.vectors[:] = embedding_matrix[idxmap]\n",
    "    #model.wv.vectors的类型为ndarray类型\n",
    "    print('model.wv.vectors = ***')\n",
    "    print(model.wv.vectors.shape)\n",
    "    #model.wv.vectors = (47460,300)\n",
    "    #model.wv.vectors对应的是词向量\n",
    "    print('***')\n",
    "    model.trainables.syn1neg[:] = embedding_matrix[idxmap]\n",
    "    #model.trainables.syn1neg = (47460,300)\n",
    "    #注意这里数组为syn1neg,中间对应的数值为1!!!\n",
    "    #model.trainables.syn1neg的类型为ndarray类型\n",
    "    print('model.trainables.synlneg = ###')\n",
    "    print(model.trainables.syn1neg.shape)\n",
    "    #这里面的内容应该维syn1neg[:]的对应数组\n",
    "    print('###')\n",
    "    \n",
    "    print('before train ***')\n",
    "    print('model.wv.vectors = ***')\n",
    "    print(model.wv.vectors[idxmap[0:10]])\n",
    "    print('model.trainables.syn1neg = ***')\n",
    "    print(model.trainables.syn1neg[idxmap[0:10]])\n",
    "    print('###')\n",
    "    #注意model.wv.vectors以及model.trainables.syn1neg这里初始化的时候\n",
    "    #内容都为embedding_matrix中的矩阵\n",
    "    model.train(all_texts, total_examples=len(all_texts), epochs=model.epochs)\n",
    "    #!!!注意这里训练的all_texts的相应的内容既包括问句又包括答句\n",
    "    #相当于问句和答句迭加在一起找词之间的相互关系\n",
    "    print('after train ***')\n",
    "    print('model.wv.vectors = ***')\n",
    "    print(model.wv.vectors[idxmap[0:10]])\n",
    "    print('model.trainables.syn1neg = ***')\n",
    "    print(model.trainables.syn1neg[idxmap[0:10]])\n",
    "    print('###')\n",
    "    embedding_matrix = np.vstack([np.zeros((1, 300)), model.wv.vectors, np.zeros((1, 300))])\n",
    "    #因为这里训练出来的内容为(1,300)维的向量，所以上面和下面都需要添加(1,300)维的向量\n",
    "    #['<PAD>'] = 0,['<UNK>'] = len(vocab['token2id']),这里前后位置补充的是'<PAD>'以及\n",
    "    #'<UNK>'的对应值\n",
    "    \n",
    "    #np.vstack():按垂直方向(行顺序)堆叠数组构成一个新的数组\n",
    "    #堆叠的数组需要具有相同的维度\n",
    "    \n",
    "    #比如a = np.array([[1,2,3]]),b = np.array([[3,4,5]])\n",
    "    #np.vstack((a,b))  \n",
    "    #[[1 2 3]\n",
    "    # [3 4 5]],np.hstack((a,b))为将数组对应横着叠加\n",
    "    #[[1 2 3 3 4 5]]\n",
    "    print('embedding_matrix.shape = ')\n",
    "    print(embedding_matrix.shape)\n",
    "    #embedding_matrix = (47462, 300)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_vocab !!!\n",
      "load_embedding = !!!\n",
      "embeddings_index type = \n",
      "2000000\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      ",\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      "the\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      ".\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      "and\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      "to\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      "of\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      "a\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      "in\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      "is\n",
      "###\n",
      "(300,)\n",
      "$$$\n",
      "nb_words = 47462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1219 00:30:22.926848 140155548452672 base_any2vec.py:974] Processing provided word frequencies\n",
      "I1219 00:30:22.928339 140155548452672 base_any2vec.py:980] collected 47462 different raw word, with total frequency of 2602058\n",
      "I1219 00:30:22.929272 140155548452672 word2vec.py:1647] Loading a fresh vocabulary\n",
      "I1219 00:30:23.052832 140155548452672 word2vec.py:1671] effective_min_count=1 retains 47460 unique words (99% of original 47462, drops 2)\n",
      "I1219 00:30:23.054460 140155548452672 word2vec.py:1677] effective_min_count=1 leaves 2602058 word corpus (100% of original 2602058, drops 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v_fine_tune = !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1219 00:30:23.289059 140155548452672 word2vec.py:1736] deleting the raw counts dictionary of 47462 items\n",
      "I1219 00:30:23.290625 140155548452672 word2vec.py:1739] sample=0.001 downsamples 53 most-common words\n",
      "I1219 00:30:23.292421 140155548452672 word2vec.py:1742] downsampling leaves estimated 1804564 word corpus (69.4% of prior 2602058)\n",
      "I1219 00:30:23.423434 140155548452672 base_any2vec.py:1022] estimated required memory for 47460 words and 300 dimensions: 137634000 bytes\n",
      "I1219 00:30:23.425044 140155548452672 word2vec.py:1888] resetting layer weights\n",
      "I1219 00:30:24.415891 140155548452672 base_any2vec.py:1210] training model with 1 workers on 47460 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.wv.vectors = ***\n",
      "(47460, 300)\n",
      "***\n",
      "model.trainables.synlneg = ###\n",
      "(47460, 300)\n",
      "###\n",
      "before train ***\n",
      "model.wv.vectors = ***\n",
      "[[ 0.0231  0.017   0.0157 ...  0.0744 -0.1118  0.0963]\n",
      " [-0.0282 -0.0557 -0.0451 ... -0.037  -0.0725 -0.0042]\n",
      " [-0.0175 -0.2189  0.0353 ... -0.2846  0.0509  0.0229]\n",
      " ...\n",
      " [-0.062   0.1254  0.1693 ... -0.1727  0.0627 -0.0097]\n",
      " [ 0.0585  0.131  -0.0555 ...  0.1305 -0.0758  0.1905]\n",
      " [-0.2205  0.1434  0.038  ... -0.156   0.1814  0.0072]]\n",
      "model.trainables.syn1neg = ***\n",
      "[[ 0.0231  0.017   0.0157 ...  0.0744 -0.1118  0.0963]\n",
      " [-0.0282 -0.0557 -0.0451 ... -0.037  -0.0725 -0.0042]\n",
      " [-0.0175 -0.2189  0.0353 ... -0.2846  0.0509  0.0229]\n",
      " ...\n",
      " [-0.062   0.1254  0.1693 ... -0.1727  0.0627 -0.0097]\n",
      " [ 0.0585  0.131  -0.0555 ...  0.1305 -0.0758  0.1905]\n",
      " [-0.2205  0.1434  0.038  ... -0.156   0.1814  0.0072]]\n",
      "###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1219 00:30:25.424304 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 3.66% examples, 296337 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:26.435514 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 7.32% examples, 295491 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:27.450618 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 11.14% examples, 293888 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:28.460253 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 15.34% examples, 292836 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:29.462150 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 19.29% examples, 292138 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:30.466277 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 23.15% examples, 292754 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:31.478139 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 26.98% examples, 292855 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:32.486980 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 30.85% examples, 293140 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:33.497547 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 34.50% examples, 292954 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:34.509097 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 38.68% examples, 293033 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:35.516093 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 42.72% examples, 293206 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:36.525188 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 46.69% examples, 293350 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:37.532960 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 50.18% examples, 293295 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:38.549670 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 54.17% examples, 293472 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:39.569534 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 58.09% examples, 293513 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:40.580659 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 62.26% examples, 293387 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:41.592453 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 66.08% examples, 293465 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:42.604050 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 69.57% examples, 293503 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:43.607434 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 73.47% examples, 293554 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:44.612949 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 77.51% examples, 293622 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:45.619519 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 81.37% examples, 293691 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:46.629946 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 85.20% examples, 293797 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:47.643517 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 88.89% examples, 293877 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:48.644924 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 92.66% examples, 293929 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:49.655603 140155548452672 base_any2vec.py:1305] EPOCH 1 - PROGRESS: at 96.49% examples, 293923 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:50.605441 140155548452672 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1219 00:30:50.606573 140155548452672 base_any2vec.py:1346] EPOCH - 1 : training on 14222632 raw words (7697757 effective words) took 26.2s, 293947 effective words/s\n",
      "I1219 00:30:51.616355 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 3.66% examples, 295466 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:52.625017 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 7.27% examples, 293663 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:53.627139 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 11.09% examples, 293672 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:54.640732 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 15.34% examples, 293459 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:55.647294 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 19.39% examples, 293487 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:56.655524 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 23.22% examples, 293682 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:57.670119 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 27.14% examples, 294207 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:30:58.676901 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 30.92% examples, 294172 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:30:59.679612 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 34.68% examples, 294294 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:00.686079 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 38.86% examples, 294429 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:01.704446 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 42.97% examples, 294664 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:02.716330 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 46.77% examples, 294835 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:03.724455 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 50.43% examples, 294828 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:04.731491 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 54.43% examples, 295020 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:05.748568 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 58.41% examples, 295180 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:06.748714 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 62.54% examples, 295304 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:07.761062 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 66.37% examples, 295115 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:08.773864 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 69.90% examples, 295147 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:09.778108 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 73.82% examples, 295253 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:10.788315 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 77.75% examples, 295165 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:11.788914 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 81.71% examples, 295013 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:12.790067 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 85.41% examples, 294850 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:13.802607 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 89.03% examples, 294781 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:14.816866 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 92.79% examples, 294804 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:15.831524 140155548452672 base_any2vec.py:1305] EPOCH 2 - PROGRESS: at 96.66% examples, 294800 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:16.699257 140155548452672 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1219 00:31:16.700665 140155548452672 base_any2vec.py:1346] EPOCH - 2 : training on 14222632 raw words (7698093 effective words) took 26.1s, 295040 effective words/s\n",
      "I1219 00:31:17.713686 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 3.72% examples, 299376 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:18.732557 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 7.42% examples, 298232 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:19.748978 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 11.30% examples, 295969 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:20.756129 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 15.56% examples, 296537 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:21.762463 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 19.73% examples, 297094 words/s, in_qsize 2, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1219 00:31:22.766773 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 23.43% examples, 297543 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:23.773662 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 27.50% examples, 297843 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:24.776389 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 31.34% examples, 297939 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:25.782543 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 35.09% examples, 297675 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:26.794243 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 39.36% examples, 297698 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:27.809045 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 43.42% examples, 297694 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:28.811886 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 47.21% examples, 297711 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:29.822017 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 51.12% examples, 297948 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:30.837638 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 54.99% examples, 298129 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:31.845376 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 59.12% examples, 298272 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:32.861213 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 63.40% examples, 298483 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:33.867516 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 67.03% examples, 298524 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:34.878352 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 70.83% examples, 298522 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:35.879951 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 74.86% examples, 298556 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:36.888366 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 78.70% examples, 298591 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:37.901372 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 82.57% examples, 298638 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:38.914999 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 86.45% examples, 298532 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:39.925794 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 90.21% examples, 298547 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:40.930475 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 94.20% examples, 298560 words/s, in_qsize 1, out_qsize 0\n",
      "I1219 00:31:41.948514 140155548452672 base_any2vec.py:1305] EPOCH 3 - PROGRESS: at 97.84% examples, 298524 words/s, in_qsize 2, out_qsize 0\n",
      "I1219 00:31:42.492548 140155548452672 base_any2vec.py:349] worker thread finished; awaiting finish of 0 more threads\n",
      "I1219 00:31:42.493939 140155548452672 base_any2vec.py:1346] EPOCH - 3 : training on 14222632 raw words (7698164 effective words) took 25.8s, 298509 effective words/s\n",
      "I1219 00:31:42.494921 140155548452672 base_any2vec.py:1382] training on a 42667896 raw words (23094014 effective words) took 78.1s, 295783 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train ***\n",
      "model.wv.vectors = ***\n",
      "[[ 0.0231      0.017       0.0157     ...  0.0744     -0.1118\n",
      "   0.0963    ]\n",
      " [ 0.48918295 -0.11069084 -0.24687009 ...  0.2399514   1.0076635\n",
      "   0.00949595]\n",
      " [-0.0175     -0.2189      0.0353     ... -0.2846      0.0509\n",
      "   0.0229    ]\n",
      " ...\n",
      " [ 1.2186061   0.2925553   0.7809619  ... -1.1349126   0.70017743\n",
      "  -0.99803126]\n",
      " [ 1.4345492  -0.99449205  0.30329013 ...  0.2699531  -0.18112485\n",
      "   0.27332792]\n",
      " [-0.2205      0.1434      0.038      ... -0.156       0.1814\n",
      "   0.0072    ]]\n",
      "model.trainables.syn1neg = ***\n",
      "[[ 0.0063947  -0.07385638  0.01361756 ...  0.06331608 -0.0991794\n",
      "   0.07935498]\n",
      " [ 0.0479171  -0.14804259  0.00637321 ... -0.10388843  0.17515424\n",
      "   0.01787014]\n",
      " [-0.0573205  -0.28986454  0.00773114 ... -0.2850605   0.07366499\n",
      "   0.02994892]\n",
      " ...\n",
      " [ 0.10869126  0.08249416  0.04743605 ... -0.19042307  0.08605625\n",
      "  -0.02372805]\n",
      " [ 0.10886309 -0.09930354 -0.14397602 ... -0.02560676  0.10888562\n",
      "   0.00220892]\n",
      " [-0.23667435  0.06195231  0.02777862 ... -0.1693988   0.20318218\n",
      "   0.00462102]]\n",
      "###\n",
      "embedding_matrix.shape = \n",
      "(47462, 300)\n",
      "train_texts = \n",
      "what am i losing when using extension tubes instead of a macro lens  ?   after playing around with macro photography on  -  the  -  cheap   (  read  :   reversed lens  ,   rev  .   lens mounted on a straight lens  ,   passive extension tubes  )    ,   i would like to get further with this  .   the problems with the techniques i used is that focus is manual and aperture control is problematic at best  .   this limited my setup to still subjects   (  read  :   dead insects  )   now  ,   as spring is approaching  ,   i want to be able to shoot live insects  .   i believe that for this  ,   autofocus and settable aperture will be of great help  .  \n",
      "\n",
      "so  ,   one obvious but expensive option is a macro lens   (  say  ,   ef  mm macro  )   however  ,   i am not really interested in yet another prime lens  .   an alternative is the electrical extension tubes  .  \n",
      "\n",
      "except for maximum focusing distance  ,   what am i losing when using tubes   (  coupled with a fine lens  ,   say ef   -     /     .     )   instead of a macro lens  ?\n",
      "##################################################################\n",
      "what is the distinction between a city and a sprawl  /  metroplex  .    .    .   between downtown and a commercial district  ?   i am trying to understand what kinds of places the spam values on p   refer to in the  th edition main book for shadowrun  .  \n",
      "\n",
      "per p    ,   a sprawl is a plex  ,   a plex is a   \"  metropolitan complex  ,   short for metroplex  \"    .   per google a metroplex is   \"   a very large metropolitan area  ,   especially one that is an aggregation of two or more cities  \"    .    a city downtown and sprawl downtown would tend to have similar densities  ,   but for some reason the sprawl   (  which includes suburbs  ?    )   has a higher spam zone noise rating   (  p    )    .    similarly  ,   I had think of a downtown as being more dense and noisy   (  e  .  g  .   office buildings and street vendors  )   than a commercial district  ,   e  .  g  .   an outdoor mall  .    the noise ratings make me think that i am thinking about this incorrectly  .   what is a better way of thinking of them  ?\n",
      "##################################################################\n",
      "maximum protusion length for through  -  hole component pins I am working on a pcb that has through  -  hole components on both sides of the board  .   the   \"  top  \"   side of the board is mounted flush to a delrin plastic block   (  the only top  -  side component is a gas sensor that is fed air samples through hose fittings in the plastic block  )    .  \n",
      "\n",
      "the flush mounting means that i have to add grooves to the plastic block to accommodate the soldered pins of the bottom  -  side components  .   assuming a standard    .     \"   thickness fr  board  ,   how deep do i need to make the grooves in the plastic block  ?   the only thing i could find is this nasa workmanship standard that states    .   mm to    .   mm  ,   but I am not sure if that will always hold true  .\n",
      "##################################################################\n",
      "can an affidavit be used in beit din  ?   an affidavit  ,   from what i understand  ,   is basically a signed document given by a witness to be used as evidence in a trial  ,   without the witness themselves needing to take a stand  .  \n",
      "\n",
      "can an affidavit be used in beit din  ?   or must witnesses take the stand in person for their testimony to count  ?  \n",
      "\n",
      "  (  in case I am misunderstanding what exactly an affidavit is  ,   simply treat it as a signed document by a witness with their testimony  .    )\n",
      "##################################################################\n",
      "how do you make a binary image in photoshop  ?   i am trying to make a binary image  .   i want more than just the look of the image to be black  /  white  ,   but i want the actual file to be a binary file  .   every pixel should be either black  ,   or white  .   \n",
      "\n",
      "i do not just want a monochrome image  .   i cannot have varying shades of gray  ,   every pixel needs to be black or white  .  \n",
      "\n",
      "is this possible  ?   i looked under image   >   mode but nothing there seems to indiciate a binary style image  .\n",
      "##################################################################\n",
      "column grouping with title in datatables i am creating an html table with jquery  '  s datatables plug  -  in  .   i would like to know if there is a way to group a number of columns together with a title which describes what the grouping represents  .   \n",
      "\n",
      "in my specific case  ,     of my columns will display an address   (  street  ,   city  ,   state  )    .   I had like them to have an additional border around just those columns with a title that indicates to the end  -  user that those   columns represent the address  .  \n",
      "\n",
      "I am considering just changing the background color of those   columns which will do the job but I am wanting the address title to be displayed  .   I have seen that rows can be grouped together and sections divided by separators  ;   however  ,   i have not yet come across anything that makes it possible to group columns together  .\n",
      "##################################################################\n",
      "core file size with ulimit my question is probably not related to ubuntu in particular  ,   but since my desktop running this os  ,   i came to this forum  .  \n",
      "\n",
      "i am trying to change the core file size using ulimit   -  c command as follows  :  \n",
      "\n",
      "  $   ulimit   -  a\n",
      "core file size            (  blocks  ,     -  c  )    \n",
      "data seg size             (  kbytes  ,     -  d  )   unlimited\n",
      "scheduling priority               (    -  e  )    \n",
      "file size                 (  blocks  ,     -  f  )   unlimited\n",
      "pending signals                   (    -  i  )    \n",
      "max locked memory         (  kbytes  ,     -  l  )    \n",
      "max memory size           (  kbytes  ,     -  m  )   unlimited\n",
      "open files                        (    -  n  )    \n",
      "pipe size              (    bytes  ,     -  p  )    \n",
      "posix message queues       (  bytes  ,     -  q  )    \n",
      "real  -  time priority                (    -  r  )    \n",
      "stack size                (  kbytes  ,     -  s  )    \n",
      "cpu time                 (  seconds  ,     -  t  )   unlimited\n",
      "max user processes                (    -  u  )    \n",
      "virtual memory            (  kbytes  ,     -  v  )   unlimited\n",
      "file locks                        (    -  x  )   unlimited\n",
      "\n",
      "\n",
      "changing the limitation  :  \n",
      "\n",
      "  $   ulimit   -  c unlimited\n",
      "\n",
      "\n",
      "observing the result  :  \n",
      "\n",
      "  $   ulimit   -  a\n",
      "core file size            (  blocks  ,     -  c  )   unlimited\n",
      "data seg size             (  kbytes  ,     -  d  )   unlimited\n",
      "scheduling priority               (    -  e  )    \n",
      "file size                 (  blocks  ,     -  f  )   unlimited\n",
      "pending signals                   (    -  i  )    \n",
      "max locked memory         (  kbytes  ,     -  l  )    \n",
      "max memory size           (  kbytes  ,     -  m  )   unlimited\n",
      "open files                        (    -  n  )    \n",
      "pipe size              (    bytes  ,     -  p  )    \n",
      "posix message queues       (  bytes  ,     -  q  )    \n",
      "real  -  time priority                (    -  r  )    \n",
      "stack size                (  kbytes  ,     -  s  )    \n",
      "cpu time                 (  seconds  ,     -  t  )   unlimited\n",
      "max user processes                (    -  u  )    \n",
      "virtual memory            (  kbytes  ,     -  v  )   unlimited\n",
      "file locks                        (    -  x  )   unlimited\n",
      "\n",
      "\n",
      "indeed the limit is changed  .  \n",
      "however  ,   when i open another terminal and check the value  ,   i still see zero value in core file size  .  \n",
      "\n",
      "questions  :  \n",
      "\n",
      "\n",
      "are changes made using ulimit command affect only current process  ,   i  .  e  .   in this case the bash  ?  \n",
      "i launch a program from shell as a fore  -   or  -  background process  .   does the ulimit change apply for new process   ?  \n",
      "how can i make that all user processes are affected with this configuration   ?\n",
      "##################################################################\n",
      "how do you get your steam games to run on ubuntu through wine or something similar  ?   ok  ,   i was kind of surprised that this had not been asked here before  ,   but maybe it is too technical for this site  .   you guys decide  .  \n",
      "\n",
      "I have heard lots of different stories about setting up wine on ubuntu  ,   winetricks  ,   playonlinux etc  .    ,   but never a   '  this is the best way to do it for steam and steam games  '   thread  .  \n",
      "\n",
      "so has anyone had any real success getting their steam games to run on ubuntu through wine or something similar  ?   if so  ,   could we get some specific steps  ?\n",
      "##################################################################\n",
      "high memory usage windows server  r  on vmware we are running windows server  r  on vmware and are experiencing extremely high memory use when nothing is running  .   the server memory usages slowly creeps up to    -     %    .   the server is configured to use  gb of memory  .   is there some setting we should be using so the server can better manage it is memory usage  .   it is behaving as if there is a memory leak  .\n",
      "##################################################################\n",
      "how do you grapple in dead rising    ?   i just started playing dead rising   on the xbox one  .   i got to the first grapple  ,   and i cannot figure out what I am supposed to do  .   the top of the screen says   \"  perform a gesture to escape a grapple  \"     -   what does   \"  gesture  \"   mean in this context  ?   is it something to do with kinect  ?\n",
      "##################################################################\n",
      "train_texts1 = \n",
      "i just got extension tubes  ,   so here  '  s the skinny  .  \n",
      "\n",
      "\n",
      "    .    .    .  what am i losing when using tubes  .    .    .    ?  \n",
      "\n",
      "\n",
      "a very considerable amount of light  !    increasing that distance from the end of the lens to the sensor can cut your light several stops  .    combined with the fact that you will usually shoot stopped down   -   expect to need to increase your iso considerably  .  \n",
      "\n",
      "the fact the macro  '  s are usually considered very very sharp  ,   although i believe that    -   mm    .    is supposed to be quite sharp  .  \n",
      "\n",
      "the ultra low distortion typical of many macros  .  \n",
      "\n",
      "i would not worry too much about the bokeh since the dof will still be quite limited  .  \n",
      "\n",
      "coupled on my  mm  ,   a full  mm  '  ish extension tube results in a dof of about a couple inches in front of the lens  .    on my    -     ,   its probably around    -    feet in front of the lens to about a foot in front of the lens  .\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "it might be helpful to look into the definition of spam zone  :  \n",
      "\n",
      "  (  p  .     )   spam zone  :   an area flooded with invasive and  /  or viral ar advertising  ,   causing noise  .  \n",
      "\n",
      "because a metroplex has so many marketing targets  ,   it seems a safe assumption that marketers would drown the plex with spam  .   spam from the less dense areas would bleed into the urban cores  .   a smaller city with less urban  /  suburban territory surrounding it ostensibly would not have as much spam  .\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "do you even need grooves  ?    we make several products using through  -  hole components that are intended to mount using vhb double  -  sided foam tape  .    the boards are    .     \"   thick double  -  sided with pth and we use a table  -  top vertical belt sander to bring the component leads almost flush with the solder mask  .    in other words  ,   the solder mask is not touched by the sand paper but the leads are all sanded flat and sitting just proud of the solder mask  .  \n",
      "\n",
      "this works well for small boards  .  \n",
      "\n",
      "for what it is worth  ,   there are commercial machines available that use a rotary saw blade to do the same thing  .    the board is held horizontal in a mounting   /   clamping system on the base and the saw motor is vertical on a sliding x  -  y mechanism  .    the saw blade simply cuts all of the leads almost flush with the board surface  .    \n",
      "\n",
      "this system is suited for boards of all sizes but especially for those boards larger than can be handled easily to be sanded with the belt sander  .  \n",
      "\n",
      "also note that these techniques are suitable only for pc boards with plated  -  through holes  .\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "sending an   \"  affidavit  \"   it is a dispute between rashi and rabbeinu tam  .  \n",
      "\n",
      "devarim    :     :  \n",
      "\n",
      "\n",
      "  לא יקום עד אחד באיש לכל עון ולכל חטאת בכל חטא אשר יחטא על פי שני עדים או על פי שלשה עדים יקום דבר\n",
      "\n",
      "\n",
      "rashi  :  \n",
      "\n",
      "\n",
      "  ולא שיכתבו עדותם באגרת וישלחו לבית דין\n",
      "  \n",
      "  and not that they write their testimony in a letter and send it to beis din\n",
      "\n",
      "\n",
      "tosefos bava basra  a   (  continued from  b  )    :  \n",
      "\n",
      "\n",
      "  ועוד אומר ר  \"  י ששמע מן ר  \"  ת שנוהגים לשלח העדים עדותם באיגרת לב  \"  ד וחשיב עדות והא דדרשינן בספרי  .   מפיהם ולא מפי כתבם לא אתא אלא למעוטי דוקא אלם שאינו בר הגדה אבל ראוי להגדה אין הגדה מעכבת בו \n",
      "  \n",
      "  r  \"  i said that he heard from rabbeinu tam that the custom is to send testimony by a letter and it is considered   [  valid  ]   testimony  .    and that which it expounds in the sifre   \"  from their mouths and not from their writing  \"   is only coming to exclude a mute who is not able to speak  ,   but someone who is able to speak does not need to speak  .  \n",
      "\n",
      "\n",
      "rambam concludes it is not allowed  ,   but in monetary law the chachomim enacted that it would be accepted in order to not prohibit the ability of people to secure loans   (  hilchos edus    :     )  \n",
      "\n",
      "\n",
      "  דין תורה שאין מקבלין עדות  ,   לא בדיני ממונות ולא בדיני נפשות  ,   אלא מפי העדים  :    שנאמר   \"  על פי שניים עדים  \"     (  דברים יז  ,  ו  )    -    -  מפיהם  ,   ולא מכתב ידן  .    אבל מדברי סופרים שחותכין דיני ממונות בעדות שבשטר  ,   אף על פי שאין העדים קיימין  ,   כדי שלא תנעול דלת בפני לווין  .\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "check out image trace in adobe illustrator  .   \n",
      "\n",
      "i like using python and pil  ,   however  .  \n",
      "\n",
      "from pil import image\n",
      "image  _  file   =   image  .  open  (    \"  myimage  .  bmp  \"    )   \n",
      "image  _  file   =   image  _  file  .  convert  (    '     '    )     #   convert\n",
      "image  _  file  .  save  (   aresult  .  bmp  '    )\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "to show group headers see this example   (  build the   &  lt  ;  thead  &  gt  ;   element using colspan and rowspan  )    .  \n",
      "\n",
      "to show borders around you group build css classes with border  -  left and border  -  right and use the columns  .  classname option for the first and last column in your group  .\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "ulimit is a shell builtin  ,   and thus only affects the current shell  ,   and processes started by that shell  :  \n",
      "\n",
      "  $   type ulimit\n",
      "ulimit is a shell builtin\n",
      "\n",
      "\n",
      "from man ulimit  :  \n",
      "\n",
      "the  ulimit  utility  shall  set  or report the file  -  size writing limit\n",
      "imposed on files written by the shell and its child processes   (  files of\n",
      "any  size  may be read  )    .   only a process with appropriate privileges can\n",
      "increase the limit  .  \n",
      "\n",
      "\n",
      "so  ,   yes  ,   child processes are affected  .  \n",
      "\n",
      "to set limits permanently or for all processes  ,   edit   /  etc  /  security  /  limits  .  conf and reboot  .   the examples in the manpage are fairly good  .   you just need to add something like  :  \n",
      "\n",
      "username   -   core unlimited\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "you could try http  :    /    /  transgaming  .  com  /     (  cedega  )    .    i did this in the past and it worked fine  ,   but you have to pay for it   -     :    \\\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "well to better assist with your question  :  \n",
      "\n",
      "  -  which vmware product   &  amp  ;   version\n",
      "  -  what is the windows   r  vm running  ?  \n",
      "  -  where are you seeing the high memory usage  ?   windows or your vmware product  ?\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "you can also switch gesture based grapple escapes off in the kinect settings of dead rising     (  along with all the rest of the kinect features  )   so that you have to do qte type button presses instead  .\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "tokenize = !!!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize = !!!\n",
      "tokenize = !!!\n",
      "tokenize = !!!\n",
      "t_max = \n",
      "[[1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.66666667 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.        ]]\n",
      "t_min = \n",
      "[[0.33333333 0.33333333 0.         0.         0.         0.\n",
      "  0.33333333 0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33333333 0.33333333 0.33333333 0.33333333\n",
      "  0.33333333 0.2        0.         0.         0.         0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(itertools.chain(*train_texts, *test_texts), max_features)\n",
    "#max_features = 60000\n",
    "embedding_matrix = load_embedding(EMBEDDING_PATH, vocab['token2id'])\n",
    "#此时embedding_matrix = (47462,300)的对应的矩阵\n",
    "#!!!这里面先获得使用fasttext.pkl训练好的词向量矩阵作为初始化矩阵\n",
    "embedding_matrix = w2v_fine_tune(all_texts, vocab, embedding_matrix)\n",
    "#使用w2v_fine_tune 用word2vec对fasttext训练得到的结果进行操作\n",
    "\n",
    "print('train_texts = ')\n",
    "for  i  in  range(10):\n",
    "    print(train_texts[i][0])\n",
    "    print('##################################################################')\n",
    "#print(train_texts[0:10,0])\n",
    "print('train_texts1 = ')\n",
    "for  i  in  range(10):\n",
    "    print(train_texts[i][1])\n",
    "    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "train_q = tokenize(train_texts[:, 0], vocab['token2id'], max_q_len)\n",
    "#这里[0]位置为question_title+question_body的内容,[1]位置为question_answer内容\n",
    "#train_q = (6079,)\n",
    "train_a = tokenize(train_texts[:, 1], vocab['token2id'], max_a_len)\n",
    "#这里[1]位置为answer的对应的内容\n",
    "#train_a = (6079,)\n",
    "train_x = np.array([train_q, train_a]).T\n",
    "#train_x = (6079,2)，一个问句以及一个答句\n",
    "train_y = train_df[target_names].values\n",
    "#train_y = (6079,30)\n",
    "train_group = train_df['question_body'].values\n",
    "#train_group之中存储'question_body'的对应内容\n",
    "#train_group = (6079,)\n",
    "\n",
    "test_q = tokenize(test_texts[:, 0], vocab['token2id'], max_q_len)\n",
    "#test_q = (476,)\n",
    "test_a = tokenize(test_texts[:, 1], vocab['token2id'], max_a_len)\n",
    "#test_a = (476,)\n",
    "test_x = np.array([test_q, test_a]).T\n",
    "#test_x = (476,2)\n",
    "#tokenize为前面定义过的函数，如果发现前面没有引用相关的库及包的\n",
    "#时候，就需要考虑是不是这个函数已经被定义过了\n",
    "\n",
    "#比如a=[1,2],b=[3,4],result = np.array([a,b]).T\n",
    "#= [[1,2]   转置之后为 [[1,3]\n",
    "#   [3,4]],             [2,4]]\n",
    "#np.array为括号里面的数组按行依次放置\n",
    "\n",
    "# target scaling\n",
    "t_max = train_y.max(axis=0)[np.newaxis, :]\n",
    "t_min = train_y.min(axis=0)[np.newaxis, :]\n",
    "#train_y = train_df[target_names].values\n",
    "#为对应属性的各项值\n",
    "#axis = 1时，数组变化时横向的，体现出列的增加或减少\n",
    "#axis = 0时，数组变化是纵向的，体现出行的增加或减少\n",
    "\n",
    "#np.newaxis功能:插入新维度\n",
    "#a = np.array([1,2,3,4,5])  shape = (5)\n",
    "#aa = a[:,np.newaxis]  这里操作在后面插入一个新维度\n",
    "#a = [[1],[2],[3],[4],[5]]  shape = (1,5)\n",
    "#aa = a[np.newaxis,:]\n",
    "#a = [[1 2 3 4 5]]   shape = (5,1)\n",
    "\n",
    "#比如arr1 = np.array([[1,5,3],[4,2,6]])\n",
    "#t_max = arr1.max(axis=0)[np.newaxis,:]操作之后\n",
    "#t_max = [[4 5 6]]\n",
    "#t_min = arr1.min(axis=0)[np.newaxis,:]操作之后\n",
    "#t_min = [[1 2 3]]\n",
    "print('t_max = ')\n",
    "print(t_max)\n",
    "print('t_min = ')\n",
    "print(t_min)\n",
    "train_y = (train_y - t_min) / (t_max - t_min)\n",
    "#!!!训练完之后进行归一化操作很重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面先得到一个w2v训练出来的embedding_matrix的权重矩阵，然后下面又使用bert模型的文件得到另外一个相应的权重矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniversalSentenceEncoder(object):\n",
    "    \n",
    "    def __init__(self, model_dir: str, batch_size: int = 128):\n",
    "    #model_dir之中放置'universal-sentence-encoder-qa'的对应的目录\n",
    "    #里面的内容含有saved_model.pb,以及variables的相应文件夹，variables\n",
    "    #之中含有variables.data-00000-of-00001以及variables.index\n",
    "    #这两个对应的相应的文件\n",
    "        self.module = hub.load(str(model_dir))\n",
    "    #这里面的load函数将对应的variables.data-00000-of-00001\n",
    "    #以及variables.index两个对应文件同时加载出来\n",
    "    #hub.load这里是载入模型，所以载入的是一个目录\n",
    "    \n",
    "    #import tensorflow_hub as hub\n",
    "    #迁移学习是把已训练好的模型(预训练模型)参数迁移到新的模型\n",
    "    #来帮助新模型训练。\n",
    "    \n",
    "    #实现迁移学习的手段\n",
    "    #1.Transfer Learning:冻结预训练模型的全部卷积层，只训练自己定制\n",
    "    #的全连接层\n",
    "    #2.Extract Feature Vector:先计算出预训练模型的卷积层对所有训练\n",
    "    #和测试数据的特征向量，然后抛开预训练模型，只训练自己定制的简配版全连接网络。\n",
    "    #3.Fine-Tuning:冻结预训练模型的部分卷积层（通常是靠近输入的多数卷积层，因为这\n",
    "    #些层保留了大量底层信息）甚至不冻结任何网络层，训练剩下的卷积层（通常是靠近输\n",
    "    #出的部分卷积层）和全连接层。\n",
    "        self.batch_size = batch_size\n",
    "    #二进制文件中包含所有的weights,biases,gradients和其他的variables的值\n",
    "    #这个文件使用.ckpt后缀，有两个对应的文件\n",
    "    #variables.data-00000-of-00001以及variables.index,.data文件就是保存\n",
    "    #训练的variables\n",
    "    \n",
    "    #所有预训练之后的文件都放在相应的文件夹之中，比如saved_model.pd\n",
    "    #variables之中的variables.data-00000-of-00001,variables.index的文件\n",
    "        \n",
    "    def __call__(self, texts: List[str], mode: str) -> torch.FloatTensor:\n",
    "        assert mode in ['question', 'answer']\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            text = texts[i:(i + self.batch_size)]\n",
    "            #提取texts[i:(i+self.batch_size)]中的文本信息\n",
    "            if mode == 'question':\n",
    "        #TensorFlow Hub是一个用于促进机器学习模型中可复用部分再次进行\n",
    "        #探索与发布的库，主要是预训练过的Tensorflow模型片段再次利用到\n",
    "        #新的任务上\n",
    "                h_embedding = self.module.signatures['question_encoder'](\n",
    "                    tf.constant(text))['outputs']\n",
    "            #self.module = hub.load(str(model_dir))\n",
    "            else:\n",
    "                h_embedding = self.module.signatures['response_encoder'](\n",
    "                    input=tf.constant(text),\n",
    "                    context=tf.constant(text))['outputs']\n",
    "                \n",
    "            h_embedding = torch.FloatTensor(h_embedding.numpy())\n",
    "            embeddings.append(h_embedding)\n",
    "        return torch.cat(embeddings, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['question'][0])\n",
    "print(test_df['question'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1219 01:06:51.981954 140155548452672 registry.py:53] resolver HttpCompressedFileResolver does not support the provided handle.\n",
      "I1219 01:06:51.983100 140155548452672 registry.py:53] resolver GcsCompressedFileResolver does not support the provided handle.\n",
      "I1219 01:06:51.983887 140155548452672 registry.py:53] resolver HttpUncompressedFileResolver does not support the provided handle.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What am I losing when using extension tubes instead of a macro lens? After playing around with macro photography on-the-cheap (read: reversed lens, rev. lens mounted on a straight lens, passive extension tubes), I would like to get further with this. The problems with the techniques I used is that focus is manual and aperture control is problematic at best. This limited my setup to still subjects (read: dead insects) Now, as spring is approaching, I want to be able to shoot live insects. I believe that for this, autofocus and settable aperture will be of great help.\n",
      "\n",
      "So, one obvious but expensive option is a macro lens (say, EF 100mm Macro) However, I am not really interested in yet another prime lens. An alternative is the electrical extension tubes.\n",
      "\n",
      "Except for maximum focusing distance, what am I losing when using tubes (coupled with a fine lens, say EF70-200/2.8) instead of a macro lens?\n",
      "\n",
      "I just got extension tubes, so here's the skinny.\n",
      "\n",
      "\n",
      "  ...what am I losing when using tubes...?\n",
      "\n",
      "\n",
      "A very considerable amount of light!  Increasing that distance from the end of the lens to the sensor can cut your light several stops.  Combined with the fact that you'll usually shoot stopped down - expect to need to increase your ISO considerably.\n",
      "\n",
      "The fact the macro's are usually considered very very sharp, although I believe that 70-200mm 2.8 is supposed to be quite sharp.\n",
      "\n",
      "The ultra low distortion typical of many macros.\n",
      "\n",
      "I wouldn't worry too much about the bokeh since the DOF will still be quite limited.\n",
      "\n",
      "Coupled on my 50mm, a full 60mm'ish extension tube results in a DOF of about a couple inches in front of the lens.  On my 70-300, its probably around 2-3 feet in front of the lens to about a foot in front of the lens.\n",
      "\n",
      "len(train_df) = 6079\n",
      "(6079, 512)\n",
      "(476, 512)\n",
      "(6079, 512)\n",
      "(476, 512)\n"
     ]
    }
   ],
   "source": [
    "use = UniversalSentenceEncoder(USE_DIR, batch_size=1)\n",
    "q_texts = list(itertools.chain(train_df['question'].values, test_df['question'].values))\n",
    "print(q_texts[0])\n",
    "a_texts = list(itertools.chain(train_df['answer'].values, test_df['answer'].values))\n",
    "print(a_texts[0])\n",
    "#这里面q_texts以及a_texts都是文本，\n",
    "q_emb = use(q_texts, mode='question')\n",
    "#使用use函数将问题对应的权重矩阵提取出来\n",
    "#q_emb = (6555,512),本身len(train_df) = 6079,len(a_emb) = 476,所以len(q_emb) = 6555\n",
    "a_emb = use(a_texts, mode='answer')\n",
    "#a_emb = (6555,512),里面隐藏着self.batch_size = 1\n",
    "print('len(train_df) = %d'%len(train_df))\n",
    "train_q_emb, test_q_emb = q_emb[:len(train_df)], q_emb[len(train_df):]\n",
    "train_a_emb, test_a_emb = a_emb[:len(train_df)], a_emb[len(train_df):]\n",
    "#从itertool.chain中截取出train_q_emb以及test_q_emb中相应内容\n",
    "print(np.array(train_q_emb).shape)\n",
    "#train_q_emb = (6079,512)\n",
    "print(np.array(test_q_emb).shape)\n",
    "#test_q_emb = (476,512)\n",
    "print(np.array(train_a_emb).shape)\n",
    "#train_a_emb = (6079,512)\n",
    "print(np.array(test_a_emb).shape)\n",
    "#test_q_emb = (476,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_emb.shape = \n",
      "(6555, 512)\n",
      "a_emb.shape = \n",
      "(6555, 512)\n",
      "train_df.shape = \n",
      "(6079, 42)\n",
      "q_emb = \n",
      "tensor([[ 0.0474, -0.0153,  0.0554,  ...,  0.0042,  0.0046,  0.0418],\n",
      "        [ 0.0633, -0.0249,  0.0011,  ..., -0.0287, -0.0181,  0.0133],\n",
      "        [-0.0070,  0.0484, -0.0083,  ...,  0.0430,  0.0517,  0.0322],\n",
      "        ...,\n",
      "        [ 0.0613,  0.0327, -0.0353,  ...,  0.0827, -0.0024, -0.0913],\n",
      "        [ 0.0152,  0.0706, -0.0084,  ...,  0.0548,  0.0321,  0.0410],\n",
      "        [-0.0577, -0.0303, -0.0383,  ...,  0.0382, -0.0041, -0.0004]])\n",
      "a_emb = \n",
      "tensor([[ 0.0363, -0.0201,  0.0728,  ..., -0.0221,  0.0135, -0.0279],\n",
      "        [ 0.0477, -0.0742, -0.0025,  ..., -0.0641,  0.0107, -0.0709],\n",
      "        [-0.0482,  0.0220,  0.0407,  ..., -0.0528,  0.0384,  0.0372],\n",
      "        ...,\n",
      "        [ 0.0518, -0.0172, -0.0412,  ...,  0.0423, -0.0334, -0.1007],\n",
      "        [-0.0169,  0.0823, -0.0198,  ...,  0.0356,  0.0324, -0.0193],\n",
      "        [-0.0567, -0.0106, -0.1018,  ..., -0.0576, -0.0535,  0.0446]])\n"
     ]
    }
   ],
   "source": [
    "print('q_emb.shape = ')\n",
    "print(np.array(q_emb).shape)\n",
    "#q_emb = (6555,512)\n",
    "print('a_emb.shape = ')\n",
    "print(np.array(a_emb).shape)\n",
    "#a_emb = (6555,512)\n",
    "print('train_df.shape = ')\n",
    "print(np.array(train_df).shape)\n",
    "#train_df = (6079,42)\n",
    "print('q_emb = ')\n",
    "print(q_emb)\n",
    "#q_emb为一个6555*512的权重矩阵\n",
    "print('a_emb = ')\n",
    "print(a_emb)\n",
    "#a_emb为一个6555\n",
    "\n",
    "#这里训练出来的对应内容后面才会用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_title</th>\n",
       "      <th>question_body</th>\n",
       "      <th>question_user_name</th>\n",
       "      <th>question_user_page</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_user_name</th>\n",
       "      <th>answer_user_page</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>...</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "      <td>After playing around with macro photography on...</td>\n",
       "      <td>ysap</td>\n",
       "      <td>https://photo.stackexchange.com/users/1024</td>\n",
       "      <td>I just got extension tubes, so here's the skin...</td>\n",
       "      <td>rfusca</td>\n",
       "      <td>https://photo.stackexchange.com/users/1917</td>\n",
       "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>What am I losing when using extension tubes in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "      <td>I am trying to understand what kinds of places...</td>\n",
       "      <td>russellpierce</td>\n",
       "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
       "      <td>It might be helpful to look into the definitio...</td>\n",
       "      <td>Erik Schmidt</td>\n",
       "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
       "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>What is the distinction between a city and a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "      <td>I'm working on a PCB that has through-hole com...</td>\n",
       "      <td>Joe Baker</td>\n",
       "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
       "      <td>Do you even need grooves?  We make several pro...</td>\n",
       "      <td>Dwayne Reid</td>\n",
       "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
       "      <td>http://electronics.stackexchange.com/questions...</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>Maximum protusion length for through-hole comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Can an affidavit be used in Beit Din?</td>\n",
       "      <td>An affidavit, from what i understand, is basic...</td>\n",
       "      <td>Scimonster</td>\n",
       "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
       "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
       "      <td>Y     e     z</td>\n",
       "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
       "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Can an affidavit be used in Beit Din? An affid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How do you make a binary image in Photoshop?</td>\n",
       "      <td>I am trying to make a binary image. I want mor...</td>\n",
       "      <td>leigero</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
       "      <td>q2ra</td>\n",
       "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
       "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>How do you make a binary image in Photoshop? I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6074</th>\n",
       "      <td>9642</td>\n",
       "      <td>Using a ski helmet for winter biking</td>\n",
       "      <td>I am curious if anyone uses a skiing helmet fo...</td>\n",
       "      <td>sixtyfootersdude</td>\n",
       "      <td>https://bicycles.stackexchange.com/users/134</td>\n",
       "      <td>If you're thinking about wearing a ski helmet ...</td>\n",
       "      <td>Matt Leo</td>\n",
       "      <td>https://bicycles.stackexchange.com/users/3340</td>\n",
       "      <td>http://bicycles.stackexchange.com/questions/99...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>Using a ski helmet for winter biking I am curi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6075</th>\n",
       "      <td>9643</td>\n",
       "      <td>Adjustment to road bike brakes for high grade ...</td>\n",
       "      <td>I have a road bike with a front brake that wea...</td>\n",
       "      <td>ash</td>\n",
       "      <td>https://bicycles.stackexchange.com/users/14519</td>\n",
       "      <td>\\nYou can replace the pads (as stated elsewher...</td>\n",
       "      <td>Daniel R Hicks</td>\n",
       "      <td>https://bicycles.stackexchange.com/users/1584</td>\n",
       "      <td>http://bicycles.stackexchange.com/questions/25...</td>\n",
       "      <td>CULTURE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>Adjustment to road bike brakes for high grade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>9645</td>\n",
       "      <td>Suppress 'file truncated' messages when using ...</td>\n",
       "      <td>I'm tailing a log file using tail -f messages....</td>\n",
       "      <td>Maneating Koala</td>\n",
       "      <td>https://unix.stackexchange.com/users/60445</td>\n",
       "      <td>Maybe help if can be fixes origin of this erro...</td>\n",
       "      <td>BG Bruno</td>\n",
       "      <td>https://unix.stackexchange.com/users/68208</td>\n",
       "      <td>http://unix.stackexchange.com/questions/169054...</td>\n",
       "      <td>TECHNOLOGY</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>Suppress 'file truncated' messages when using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>9646</td>\n",
       "      <td>When should a supervisor be a co-author?</td>\n",
       "      <td>What are people's views on this?  To be specif...</td>\n",
       "      <td>MrB</td>\n",
       "      <td>https://mathoverflow.net/users/2189</td>\n",
       "      <td>As a non-mathematician, I am somewhat mystifie...</td>\n",
       "      <td>angela</td>\n",
       "      <td>https://mathoverflow.net/users/4267</td>\n",
       "      <td>http://mathoverflow.net/questions/57337</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>When should a supervisor be a co-author? What ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>9647</td>\n",
       "      <td>Why are there so many different types of screw...</td>\n",
       "      <td>Newbie question.\\n\\nWhy is it that there's a b...</td>\n",
       "      <td>Doug T.</td>\n",
       "      <td>https://diy.stackexchange.com/users/321</td>\n",
       "      <td>First, I really like Eric's answer for practic...</td>\n",
       "      <td>Scivitri</td>\n",
       "      <td>https://diy.stackexchange.com/users/113</td>\n",
       "      <td>http://diy.stackexchange.com/questions/2701/wh...</td>\n",
       "      <td>LIFE_ARTS</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Why are there so many different types of screw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6079 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      qa_id                                     question_title  \\\n",
       "0         0  What am I losing when using extension tubes in...   \n",
       "1         1  What is the distinction between a city and a s...   \n",
       "2         2  Maximum protusion length for through-hole comp...   \n",
       "3         3              Can an affidavit be used in Beit Din?   \n",
       "4         5       How do you make a binary image in Photoshop?   \n",
       "...     ...                                                ...   \n",
       "6074   9642               Using a ski helmet for winter biking   \n",
       "6075   9643  Adjustment to road bike brakes for high grade ...   \n",
       "6076   9645  Suppress 'file truncated' messages when using ...   \n",
       "6077   9646           When should a supervisor be a co-author?   \n",
       "6078   9647  Why are there so many different types of screw...   \n",
       "\n",
       "                                          question_body question_user_name  \\\n",
       "0     After playing around with macro photography on...               ysap   \n",
       "1     I am trying to understand what kinds of places...      russellpierce   \n",
       "2     I'm working on a PCB that has through-hole com...          Joe Baker   \n",
       "3     An affidavit, from what i understand, is basic...         Scimonster   \n",
       "4     I am trying to make a binary image. I want mor...            leigero   \n",
       "...                                                 ...                ...   \n",
       "6074  I am curious if anyone uses a skiing helmet fo...   sixtyfootersdude   \n",
       "6075  I have a road bike with a front brake that wea...                ash   \n",
       "6076  I'm tailing a log file using tail -f messages....    Maneating Koala   \n",
       "6077  What are people's views on this?  To be specif...                MrB   \n",
       "6078  Newbie question.\\n\\nWhy is it that there's a b...            Doug T.   \n",
       "\n",
       "                                     question_user_page  \\\n",
       "0            https://photo.stackexchange.com/users/1024   \n",
       "1              https://rpg.stackexchange.com/users/8774   \n",
       "2     https://electronics.stackexchange.com/users/10157   \n",
       "3          https://judaism.stackexchange.com/users/5151   \n",
       "4     https://graphicdesign.stackexchange.com/users/...   \n",
       "...                                                 ...   \n",
       "6074       https://bicycles.stackexchange.com/users/134   \n",
       "6075     https://bicycles.stackexchange.com/users/14519   \n",
       "6076         https://unix.stackexchange.com/users/60445   \n",
       "6077                https://mathoverflow.net/users/2189   \n",
       "6078            https://diy.stackexchange.com/users/321   \n",
       "\n",
       "                                                 answer answer_user_name  \\\n",
       "0     I just got extension tubes, so here's the skin...           rfusca   \n",
       "1     It might be helpful to look into the definitio...     Erik Schmidt   \n",
       "2     Do you even need grooves?  We make several pro...      Dwayne Reid   \n",
       "3     Sending an \"affidavit\" it is a dispute between...    Y     e     z   \n",
       "4     Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n",
       "...                                                 ...              ...   \n",
       "6074  If you're thinking about wearing a ski helmet ...         Matt Leo   \n",
       "6075  \\nYou can replace the pads (as stated elsewher...   Daniel R Hicks   \n",
       "6076  Maybe help if can be fixes origin of this erro...         BG Bruno   \n",
       "6077  As a non-mathematician, I am somewhat mystifie...           angela   \n",
       "6078  First, I really like Eric's answer for practic...         Scivitri   \n",
       "\n",
       "                                       answer_user_page  \\\n",
       "0            https://photo.stackexchange.com/users/1917   \n",
       "1              https://rpg.stackexchange.com/users/1871   \n",
       "2     https://electronics.stackexchange.com/users/64754   \n",
       "3          https://judaism.stackexchange.com/users/4794   \n",
       "4     https://graphicdesign.stackexchange.com/users/...   \n",
       "...                                                 ...   \n",
       "6074      https://bicycles.stackexchange.com/users/3340   \n",
       "6075      https://bicycles.stackexchange.com/users/1584   \n",
       "6076         https://unix.stackexchange.com/users/68208   \n",
       "6077                https://mathoverflow.net/users/4267   \n",
       "6078            https://diy.stackexchange.com/users/113   \n",
       "\n",
       "                                                    url    category  ...  \\\n",
       "0     http://photo.stackexchange.com/questions/9169/...   LIFE_ARTS  ...   \n",
       "1     http://rpg.stackexchange.com/questions/47820/w...     CULTURE  ...   \n",
       "2     http://electronics.stackexchange.com/questions...     SCIENCE  ...   \n",
       "3     http://judaism.stackexchange.com/questions/551...     CULTURE  ...   \n",
       "4     http://graphicdesign.stackexchange.com/questio...   LIFE_ARTS  ...   \n",
       "...                                                 ...         ...  ...   \n",
       "6074  http://bicycles.stackexchange.com/questions/99...     CULTURE  ...   \n",
       "6075  http://bicycles.stackexchange.com/questions/25...     CULTURE  ...   \n",
       "6076  http://unix.stackexchange.com/questions/169054...  TECHNOLOGY  ...   \n",
       "6077            http://mathoverflow.net/questions/57337     SCIENCE  ...   \n",
       "6078  http://diy.stackexchange.com/questions/2701/wh...   LIFE_ARTS  ...   \n",
       "\n",
       "     answer_helpful  answer_level_of_information  answer_plausible  \\\n",
       "0          1.000000                     0.666667          1.000000   \n",
       "1          0.888889                     0.555556          0.888889   \n",
       "2          0.777778                     0.555556          1.000000   \n",
       "3          0.833333                     0.333333          0.833333   \n",
       "4          1.000000                     0.666667          1.000000   \n",
       "...             ...                          ...               ...   \n",
       "6074       1.000000                     0.555556          1.000000   \n",
       "6075       0.888889                     0.555556          1.000000   \n",
       "6076       0.888889                     0.555556          0.888889   \n",
       "6077       0.888889                     0.555556          1.000000   \n",
       "6078       0.888889                     0.555556          1.000000   \n",
       "\n",
       "      answer_relevance  answer_satisfaction  answer_type_instructions  \\\n",
       "0             1.000000             0.800000                  1.000000   \n",
       "1             0.888889             0.666667                  0.000000   \n",
       "2             1.000000             0.666667                  0.000000   \n",
       "3             1.000000             0.800000                  0.000000   \n",
       "4             1.000000             0.800000                  1.000000   \n",
       "...                ...                  ...                       ...   \n",
       "6074          1.000000             0.866667                  0.000000   \n",
       "6075          1.000000             0.733333                  0.666667   \n",
       "6076          0.888889             0.800000                  1.000000   \n",
       "6077          1.000000             0.533333                  0.000000   \n",
       "6078          0.888889             0.733333                  0.000000   \n",
       "\n",
       "      answer_type_procedure  answer_type_reason_explanation  \\\n",
       "0                  0.000000                        0.000000   \n",
       "1                  0.000000                        0.666667   \n",
       "2                  0.333333                        1.000000   \n",
       "3                  0.000000                        1.000000   \n",
       "4                  0.000000                        1.000000   \n",
       "...                     ...                             ...   \n",
       "6074               0.000000                        0.000000   \n",
       "6075               0.333333                        0.000000   \n",
       "6076               0.000000                        0.333333   \n",
       "6077               0.333333                        0.666667   \n",
       "6078               0.000000                        1.000000   \n",
       "\n",
       "      answer_well_written                                           question  \n",
       "0                1.000000  What am I losing when using extension tubes in...  \n",
       "1                0.888889  What is the distinction between a city and a s...  \n",
       "2                0.888889  Maximum protusion length for through-hole comp...  \n",
       "3                1.000000  Can an affidavit be used in Beit Din? An affid...  \n",
       "4                1.000000  How do you make a binary image in Photoshop? I...  \n",
       "...                   ...                                                ...  \n",
       "6074             0.888889  Using a ski helmet for winter biking I am curi...  \n",
       "6075             0.888889  Adjustment to road bike brakes for high grade ...  \n",
       "6076             0.555556  Suppress 'file truncated' messages when using ...  \n",
       "6077             1.000000  When should a supervisor be a co-author? What ...  \n",
       "6078             1.000000  Why are there so many different types of screw...  \n",
       "\n",
       "[6079 rows x 42 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q_emb = \n",
    "tensor([[ 0.0474, -0.0153,  0.0554,  ...,  0.0042,  0.0046,  0.0418],\n",
    "        [ 0.0633, -0.0249,  0.0011,  ..., -0.0287, -0.0181,  0.0133],\n",
    "        [-0.0070,  0.0484, -0.0083,  ...,  0.0430,  0.0517,  0.0322],\n",
    "        ...,\n",
    "        [ 0.0613,  0.0327, -0.0353,  ...,  0.0827, -0.0024, -0.0913],\n",
    "        [ 0.0152,  0.0706, -0.0084,  ...,  0.0548,  0.0321,  0.0410],\n",
    "        [-0.0577, -0.0303, -0.0383,  ...,  0.0382, -0.0041, -0.0004]])\n",
    "a_emb = \n",
    "tensor([[ 0.0363, -0.0201,  0.0728,  ..., -0.0221,  0.0135, -0.0279],\n",
    "        [ 0.0477, -0.0742, -0.0025,  ..., -0.0641,  0.0107, -0.0709],\n",
    "        [-0.0482,  0.0220,  0.0407,  ..., -0.0528,  0.0384,  0.0372],\n",
    "        ...,\n",
    "        [ 0.0518, -0.0172, -0.0412,  ...,  0.0423, -0.0334, -0.1007],\n",
    "        [-0.0169,  0.0823, -0.0198,  ...,  0.0356,  0.0324, -0.0193],\n",
    "        [-0.0567, -0.0106, -0.1018,  ..., -0.0576, -0.0535,  0.0446]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1219 01:30:44.995497 140155548452672 driver.py:224] init\n",
      "I1219 01:30:44.999042 140155548452672 driver.py:603] reset context of device 0\n",
      "I1219 01:30:45.000501 140155548452672 driver.py:605] reset context of device 0\n"
     ]
    }
   ],
   "source": [
    "train_text_emb = torch.cat((train_q_emb, train_a_emb), 1)\n",
    "#原先train_q_emb = [6079,512],train_a_emb = [6079,512],\n",
    "#拼接完之后train_text_emb = [6079,1024]\n",
    "test_text_emb = torch.cat((test_q_emb, test_a_emb), 1)\n",
    "#torch.cat(,)后面值为0的时候表示按维数0(行进行拼接)\n",
    "#比如A = tensor([[1,1,1],[1,1,1]]),B = tensor([[2,2,2],[2,2,2]])\n",
    "#C = torch.cat((A,B),0)之后\n",
    "#C = tensor([[1,1,1],\n",
    "#             [1,1,1],\n",
    "#             [2,2,2],\n",
    "#             [2,2,2]])\n",
    "#如果A = tensor([[1,1,1],[1,1,1]]),B = tensor([[2,2,2,2],[2,2,2,2]])\n",
    "#    C = tensor([[1,1,1,2,2,2,2],\n",
    "#                [1,1,1,2,2,2,2]])\n",
    "#test_text_emb = \n",
    "\n",
    "#train_q_emb+train_a_emb\n",
    "#test_q_emb+test_a_emb\n",
    "del use\n",
    "cuda.select_device(0)\n",
    "#假设有4块显卡，编号分别为0,1,2,3\n",
    "#如果我们想使用第0到n块卡，只需要进行相应的设置即可\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_text_emb.shape = \n",
      "torch.Size([6079, 1024])\n",
      "test_text_emb.shape = \n",
      "torch.Size([476, 1024])\n"
     ]
    }
   ],
   "source": [
    "print('train_text_emb.shape = ')\n",
    "print(train_text_emb.shape)\n",
    "print('test_text_emb.shape = ')\n",
    "print(test_text_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, seqs, targets=None):\n",
    "    #第一次传入的是一问一答，seqs是问句,targets是答句\n",
    "    #第二次传入的时候只传入问句，注意看__getitem__之中的\n",
    "    #内容，取出很有艺术感，一个类将两种取出方式都搞定了\n",
    "        print('TextDataset __init__')\n",
    "        self.seqs = seqs\n",
    "    #使用self.seqs = seqs,这里的seqs对应着\n",
    "    #相应的test_x([476,2])的问答构成的矩阵\n",
    "        self.targets = targets\n",
    "    #这里面self.targets构成矩阵的特点\n",
    "        \n",
    "    def __len__(self):\n",
    "        print('TextDataset __len__')\n",
    "    #使用__len__限定循环的最大长度\n",
    "        return len(self.seqs)\n",
    "        \n",
    "    def get_keys(self):\n",
    "        print('TextDataset get_keys')\n",
    "        #print('before self.seqs')\n",
    "        #self.seqs.shape = (476,2)\n",
    "        #for  i  in  range(len(self.seqs)):\n",
    "        #    print('i = %d:%d,%d'%(i,len(self.seqs[i][0]),len(self.seqs[i][1])))\n",
    "        #self.seqs[i][0] = 225,self.seqs[i][1] = 28,此时\n",
    "        #get_keys(self)输出的第一个维度为225+28=253\n",
    "        return np.vectorize(len)(self.seqs).sum(axis=1)\n",
    "    #axis = 1就是将矩阵的从左到右的向量取出最大值，这里对应的向量里面的内容为self.seqs\n",
    "    #注意千万不要用行和列的思想去想axis，因为行和列是没有方向的，这样想会在遇到不同的\n",
    "    #例子时感到困惑\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #print('TextDataset __getitem__')\n",
    "        if self.targets is None:\n",
    "            return index, self.seqs[index]\n",
    "        return index, self.seqs[index], self.targets[index]\n",
    "    #很有艺术感的取出方式\n",
    "\n",
    "def collate_fn(data):\n",
    "    #感觉这里的collate_fn是对相应的数据进行处理\n",
    "    print('function collate_fn')\n",
    "    def _pad_sequences(seqs):\n",
    "        print('collate_fn _pad_sequences')\n",
    "        lens = [len(seq) for seq in seqs]\n",
    "        max_len = max(lens)\n",
    "\n",
    "        padded_seqs = torch.zeros(len(seqs), max_len).long()\n",
    "        for i, seq in enumerate(seqs):\n",
    "            start = max_len - lens[i]\n",
    "            padded_seqs[i, start:] = torch.LongTensor(seq)\n",
    "        return padded_seqs\n",
    "\n",
    "    transposed = list(zip(*data))\n",
    "    index = transposed[0]\n",
    "    q_seqs, a_seqs = zip(*transposed[1])\n",
    "    q_seqs = _pad_sequences(q_seqs)\n",
    "    a_seqs = _pad_sequences(a_seqs)\n",
    "    seqs = [q_seqs, a_seqs]\n",
    "    if len(transposed) == 2:  # targets == None\n",
    "        return index, seqs\n",
    "    return index, seqs, torch.FloatTensor(transposed[2])\n",
    "\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "#这里面相当于自定义了Sampler\n",
    "    def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=1048, shuffle_data=True):\n",
    "        super().__init__(data_source)\n",
    "    #test_sampler = BucketSampler(test_dataset,test_dataset.get_keys(),batch_size = batch_size,\n",
    "    #shuffle = False),这里面定义的batch_size = 128\n",
    "    \n",
    "    #后面的循环之中的定义\n",
    "    #第一次循环的时候\n",
    "    #train_sampler = BucketSampler(train_dataset, train_dataset.get_keys(),\n",
    "    #                              bucket_size=batch_size * 20, batch_size=batch_size)\n",
    "    #bucket_size = 128*20 = 2560,batch_size = 128\n",
    "    #第二次循环的时候\n",
    "    #valid_sampler = BucketSampler(valid_dataset, valid_dataset.get_keys(),\n",
    "    #                              batch_size=batch_size, shuffle_data=False)\n",
    "    #这里面定义的data_source = test_dataset,sort_keys = test_dataset.get_keys(),batch_size = \n",
    "    #shuffle = False，这里面的操作中batch_size = 128\n",
    "        print('BucketSampler __init__')\n",
    "        self.shuffle = shuffle_data\n",
    "        #传入待检验的对应的数据\n",
    "        \n",
    "        #传入False\n",
    "        self.batch_size = batch_size\n",
    "        #batch_size = 128\n",
    "        self.sort_keys = sort_keys\n",
    "        #调入的参数为test_dataset.get_keys(),这之后sort_keys = (476,)\n",
    "        print('after get_keys')\n",
    "        print(sort_keys)\n",
    "        print('sort_keys.shape = ')\n",
    "        print(np.array(sort_keys).shape)\n",
    "        self.bucket_size = bucket_size if bucket_size is not None else len(sort_keys)\n",
    "        print('self.bucket_size = ')\n",
    "        print(self.bucket_size)\n",
    "        #self.bucket_size = 128*20 = 2560\n",
    "        #这里面传入的bucket_size = None,所以这里bucket_size变为476\n",
    "        self.weights = None\n",
    "\n",
    "        if not shuffle_data:\n",
    "        #这里shuffle_data = False,正好调用的prepare_buckets()\n",
    "            self.index = self.prepare_buckets()\n",
    "            print('!!!!!!!!!!!!!!!!!!!!!self.index = !!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            print(self.index)\n",
    "        else:\n",
    "            self.index = None\n",
    "        #这里面返回的self.index = [132,274,146,170,...,408,74,82]\n",
    "        #一顿操作猛如虎，一看战绩0-5，这里面的self.index的值与self.sort_keys\n",
    "        #的对应值一样\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        assert weights >= 0\n",
    "        print('BucketSampler set_weights')\n",
    "        total = np.sum(weights)\n",
    "        if total != 1:\n",
    "            weights = weights / total\n",
    "        self.weights = weights\n",
    "\n",
    "    def __iter__(self):\n",
    "        print('BucketSampler __iter__')\n",
    "        indices = None\n",
    "        if self.weights is not None:\n",
    "            total = len(self.sort_keys)\n",
    "            indices = np.random.choice(total, (total,), p=self.weights)\n",
    "        if self.shuffle:\n",
    "            self.index = self.prepare_buckets(indices)\n",
    "        return iter(self.index)\n",
    "\n",
    "    def get_reverse_indexes(self):\n",
    "        print('BucketSampler get_reverse_indexes')\n",
    "        indexes = np.zeros((len(self.index),), dtype=np.int32)\n",
    "        for i, j in enumerate(self.index):\n",
    "            indexes[j] = i\n",
    "        return indexes\n",
    "\n",
    "    def __len__(self):\n",
    "        print('BucketSampler __len__')\n",
    "        return len(self.sort_keys)\n",
    "        \n",
    "    def prepare_buckets(self, indices=None):\n",
    "        print('BucketSampler prepare_buckets')\n",
    "        lens = - self.sort_keys\n",
    "        #self.sort_keys为之前的(476,)的相应矩阵，用来判断len(self.sort_keys[0])\n",
    "        #以及len(self.sort_keys[1])的相应的长度的，\n",
    "        assert self.bucket_size % self.batch_size == 0 or self.bucket_size == len(lens)\n",
    "        #self.bucket_size = 476,self.batch_size = 128\n",
    "        \n",
    "        if indices is None:\n",
    "        #传入indices = None\n",
    "            if self.shuffle:\n",
    "            #传入self.shuffle = False，\n",
    "                indices = shuffle(np.arange(len(lens), dtype=np.int32))\n",
    "            #这里shuffle为from sklearn.util import shuffle\n",
    "            #lens = (476,)\n",
    "                lens = lens[indices]\n",
    "            else:\n",
    "            #这里传入的self.shuffle = False,由于初始化的时候传入的self.shuffle\n",
    "            #= False的时候才调用prepare_buckets,所以只能调用这一个else之中的内容\n",
    "                indices = np.arange(len(lens), dtype=np.int32)\n",
    "        print('indices = ')\n",
    "        print(indices)\n",
    "        #indices为[0,1,2,...475]构成的相应数组\n",
    "        \n",
    "        #  bucket iterator\n",
    "        def divide_chunks(l, n):\n",
    "            print('BucketSampler divide_chunks')\n",
    "            if n == len(l):\n",
    "                yield np.arange(len(l), dtype=np.int32), l\n",
    "            else:\n",
    "                # looping till length l\n",
    "                for i in range(0, len(l), n):\n",
    "                    data = l[i:i + n]\n",
    "                    yield np.arange(i, i + len(data), dtype=np.int32), data\n",
    "                #这里需要明白yield函数的原理，每次运行的时候接着上次的进行运行\n",
    "        new_indices = []\n",
    "        extra_batch = None\n",
    "        print('lens = ')\n",
    "        print(lens)\n",
    "        print('self.bucket_size = ')\n",
    "        print(self.bucket_size)\n",
    "        #lens = [-253 -226 -203 -876...\n",
    "        #       .....................]\n",
    "        #lens对应的数值为原先sort_keys()中的数组取对应的负值\n",
    "        for chunk_index, chunk in divide_chunks(lens, self.bucket_size):\n",
    "            # sort indices in bucket by descending order of length\n",
    "            #lens = [-253 -226 -203 -876...],bucket_size = 476\n",
    "            \n",
    "            #chunk_index = [0 1 2 3 4 ... 475],chunk = [-253 -226 -203...-277]\n",
    "            #这里的chunk为之前sort_keys相加之后取的负数\n",
    "            #因为len(lens) == self.bucket_size,所以这里总共就一波\n",
    "            print('chunk_index = ')\n",
    "            print(chunk_index)\n",
    "            print('chunk = ')\n",
    "            print(chunk)\n",
    "            indices_sorted = chunk_index[np.argsort(chunk, axis=-1)]\n",
    "            #np.argsort返回排序后的索引值的数组，\n",
    "            print('indices_sorted = ')\n",
    "            print(indices_sorted)\n",
    "            #indices_sorted = [132,274,146,170...144,240,172]\n",
    "            print('chunk 132 = ')\n",
    "            print(chunk[132])\n",
    "            batches = []\n",
    "            for u, batch in divide_chunks(indices_sorted, self.batch_size):\n",
    "                print('u = ')\n",
    "                print(u)\n",
    "                #第一次的时候u = [0,1,2,3,4...127]\n",
    "                #第二次的时候u = [128,129,...255]\n",
    "                #第三次的时候u = [256,257,258...383]\n",
    "                #第四次的时候u = [384,385,...474,475]\n",
    "                print('batch = ')\n",
    "                print(batch)\n",
    "                #第一次的时候batch = [132,274,146,170...144,240,172]\n",
    "                #batch = 128\n",
    "                #这里chunk[132] = -1024\n",
    "                #第二次的时候batch = []\n",
    "                if len(batch) == self.batch_size:\n",
    "                    batches.append(batch.tolist())\n",
    "                else:\n",
    "                    assert extra_batch is None\n",
    "                    assert batch is not None\n",
    "                    extra_batch = batch\n",
    "            #记录最后一组对应的batch内容\n",
    "            # shuffling batches within buckets\n",
    "            #这里对应的self.shuffle = False\n",
    "            if self.shuffle:\n",
    "                batches = shuffle(batches)\n",
    "            #它这个传入的参数之中self.shuffle = False,一直不准备打乱\n",
    "            #相应的数值\n",
    "            \n",
    "            #它这个每组取出来的操作感觉是将相应的数组内容打乱\n",
    "            #但是这里面它进行排序了，所以就没有想进行这个操作\n",
    "            print('***************************batches = ********************************')\n",
    "            print(batches)\n",
    "            print('*********************************************************************')\n",
    "            for batch in batches:\n",
    "                new_indices.extend(batch)\n",
    "            print('new_indices = ')\n",
    "            print(new_indices)\n",
    "            #new_indices为对应的上面和的长度最大的放在前面，最小的放在后面\n",
    "            #(实际上是和的负值的最小放在前面，最大放在后面)\n",
    "    \n",
    "        if extra_batch is not None:\n",
    "            new_indices.extend(extra_batch)\n",
    "        #new_indices当中先放入相应的batch的内容，再放入相应的extra_batch\n",
    "        #的内容，注意是往里面不断填充的\n",
    "        \n",
    "        #!!!这里面的操作是傻逼操作，一顿操作猛如虎，回头一看0-5，操作了半天\n",
    "        #之后new_indices的内容为[132, 274, 146, 170, 351, 72, 210, 457, 57, 100,\n",
    "        #...303,408,74,82]\n",
    "        print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!new_indices = !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print(len(new_indices))\n",
    "        print('#############################indices = ##################################')\n",
    "        print(indices)\n",
    "        print('###############')\n",
    "        #indices = [0,1,2,3,4,....471,472,473,474,475]\n",
    "        #new_indices = [132,274,146]\n",
    "        return indices[new_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类与类定义在一起，运行过程与运行过程定义在一起的好处就是运行的时候发现类中有需要添加或者删除的打印信息的时候，\n",
    "可以只修改相应的类模块，再通过运行看对应的类模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextDataset __init__\n",
      "TextDataset get_keys\n",
      "BucketSampler __init__\n",
      "after get_keys\n",
      "[ 253  226  203  876  211  117  689  320  466  111  724  101  593  175\n",
      "  257  684  560  190  365  219  111  541  120  131   92  368  709  347\n",
      "  349  433  292  817  333  149  299  547  174  714  266  206  305  723\n",
      "  579  650  817  300  241  305  613  162  236  240  765  495  452  151\n",
      "  103  983  285  424  187  201   95  183  256  262  171  622  148  216\n",
      "  265  310 1011  273  167  121  339  232  179  196  261  429  167  294\n",
      "   70  383  615  435  211  225  302  138   70  310   70  725  148  628\n",
      "  139  441  965  491  197  169  233   72  213  134  400  294  559  182\n",
      "  514  145  183  738  553  416  389  171  347  286  732  372   82  712\n",
      "   87  366  110  425  126  502 1024  386  224  308  567  253  608  405\n",
      "  582  578  365  345   68  310 1024  344  112  938  362  297  588  125\n",
      "  225  147  145  517  135  211  570  202  379  293  701  251  344  206\n",
      "  286  246 1024  157   52  155  281  393  180  204  430  296  331  148\n",
      "  354  115  134  827  294  192  270  400  596  271  272  718  185  307\n",
      "  372  270  252  151  361  259  261   93  817  792  453  162  171  661\n",
      " 1011  174  196  297  207  154  144  182  541  643  159  212  260  578\n",
      "  151  665  343  308  390  602  551  310  219  279  147  289  350  886\n",
      "  638  287   56  369  304  377  204  399  365  619  539  450  309  141\n",
      "  698  130  721  144  613  584  288  378  424  328  677  655  193  949\n",
      "  126  131  268  463  414  479  134   97 1024  195  414  155   88  312\n",
      "  676  310  115  405  778  580  586  417  236  639  505  408  186  615\n",
      "  405  498  425  519  150  378   97  616  114  168  324  449  148  550\n",
      "  201  231  531  240  409  286  574  497  239  225  254  558  695  161\n",
      "  149  191  381  174  571  617  162  591  504  121  576  375  591  428\n",
      "  170   96  260  602  143  590  718  268   77  534  190  265  100  269\n",
      "  121 1024  392  442  225  233  277  125  243  358  234  615  156  391\n",
      "  415  241  282  179  416  567  530  402  833  140  192  556  726  175\n",
      "  550  211  213  299  130  362  127  522  627  613  114  140  232  123\n",
      "  120  371  174  246  300  386  274  192  818  344  263  220   82  160\n",
      "  111  173  167  401  823  111  221  479  434  534  795  257  559  231\n",
      "  647  366  324  699  232  137  623  235  134  570  358  286   84   91\n",
      "  495  355  269  408  321  583  170  172  200  400  564  135  169  181\n",
      "  327  588  772  545  216   96  489  261  327 1001  386  282  316  787\n",
      "  204  291  374  414  561  269  110  196  176  587  229  270  277  107]\n",
      "sort_keys.shape = \n",
      "(476,)\n",
      "self.bucket_size = \n",
      "476\n",
      "BucketSampler prepare_buckets\n",
      "indices = \n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475]\n",
      "lens = \n",
      "[ -253  -226  -203  -876  -211  -117  -689  -320  -466  -111  -724  -101\n",
      "  -593  -175  -257  -684  -560  -190  -365  -219  -111  -541  -120  -131\n",
      "   -92  -368  -709  -347  -349  -433  -292  -817  -333  -149  -299  -547\n",
      "  -174  -714  -266  -206  -305  -723  -579  -650  -817  -300  -241  -305\n",
      "  -613  -162  -236  -240  -765  -495  -452  -151  -103  -983  -285  -424\n",
      "  -187  -201   -95  -183  -256  -262  -171  -622  -148  -216  -265  -310\n",
      " -1011  -273  -167  -121  -339  -232  -179  -196  -261  -429  -167  -294\n",
      "   -70  -383  -615  -435  -211  -225  -302  -138   -70  -310   -70  -725\n",
      "  -148  -628  -139  -441  -965  -491  -197  -169  -233   -72  -213  -134\n",
      "  -400  -294  -559  -182  -514  -145  -183  -738  -553  -416  -389  -171\n",
      "  -347  -286  -732  -372   -82  -712   -87  -366  -110  -425  -126  -502\n",
      " -1024  -386  -224  -308  -567  -253  -608  -405  -582  -578  -365  -345\n",
      "   -68  -310 -1024  -344  -112  -938  -362  -297  -588  -125  -225  -147\n",
      "  -145  -517  -135  -211  -570  -202  -379  -293  -701  -251  -344  -206\n",
      "  -286  -246 -1024  -157   -52  -155  -281  -393  -180  -204  -430  -296\n",
      "  -331  -148  -354  -115  -134  -827  -294  -192  -270  -400  -596  -271\n",
      "  -272  -718  -185  -307  -372  -270  -252  -151  -361  -259  -261   -93\n",
      "  -817  -792  -453  -162  -171  -661 -1011  -174  -196  -297  -207  -154\n",
      "  -144  -182  -541  -643  -159  -212  -260  -578  -151  -665  -343  -308\n",
      "  -390  -602  -551  -310  -219  -279  -147  -289  -350  -886  -638  -287\n",
      "   -56  -369  -304  -377  -204  -399  -365  -619  -539  -450  -309  -141\n",
      "  -698  -130  -721  -144  -613  -584  -288  -378  -424  -328  -677  -655\n",
      "  -193  -949  -126  -131  -268  -463  -414  -479  -134   -97 -1024  -195\n",
      "  -414  -155   -88  -312  -676  -310  -115  -405  -778  -580  -586  -417\n",
      "  -236  -639  -505  -408  -186  -615  -405  -498  -425  -519  -150  -378\n",
      "   -97  -616  -114  -168  -324  -449  -148  -550  -201  -231  -531  -240\n",
      "  -409  -286  -574  -497  -239  -225  -254  -558  -695  -161  -149  -191\n",
      "  -381  -174  -571  -617  -162  -591  -504  -121  -576  -375  -591  -428\n",
      "  -170   -96  -260  -602  -143  -590  -718  -268   -77  -534  -190  -265\n",
      "  -100  -269  -121 -1024  -392  -442  -225  -233  -277  -125  -243  -358\n",
      "  -234  -615  -156  -391  -415  -241  -282  -179  -416  -567  -530  -402\n",
      "  -833  -140  -192  -556  -726  -175  -550  -211  -213  -299  -130  -362\n",
      "  -127  -522  -627  -613  -114  -140  -232  -123  -120  -371  -174  -246\n",
      "  -300  -386  -274  -192  -818  -344  -263  -220   -82  -160  -111  -173\n",
      "  -167  -401  -823  -111  -221  -479  -434  -534  -795  -257  -559  -231\n",
      "  -647  -366  -324  -699  -232  -137  -623  -235  -134  -570  -358  -286\n",
      "   -84   -91  -495  -355  -269  -408  -321  -583  -170  -172  -200  -400\n",
      "  -564  -135  -169  -181  -327  -588  -772  -545  -216   -96  -489  -261\n",
      "  -327 -1001  -386  -282  -316  -787  -204  -291  -374  -414  -561  -269\n",
      "  -110  -196  -176  -587  -229  -270  -277  -107]\n",
      "self.bucket_size = \n",
      "476\n",
      "BucketSampler divide_chunks\n",
      "chunk_index = \n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475]\n",
      "chunk = \n",
      "[ -253  -226  -203  -876  -211  -117  -689  -320  -466  -111  -724  -101\n",
      "  -593  -175  -257  -684  -560  -190  -365  -219  -111  -541  -120  -131\n",
      "   -92  -368  -709  -347  -349  -433  -292  -817  -333  -149  -299  -547\n",
      "  -174  -714  -266  -206  -305  -723  -579  -650  -817  -300  -241  -305\n",
      "  -613  -162  -236  -240  -765  -495  -452  -151  -103  -983  -285  -424\n",
      "  -187  -201   -95  -183  -256  -262  -171  -622  -148  -216  -265  -310\n",
      " -1011  -273  -167  -121  -339  -232  -179  -196  -261  -429  -167  -294\n",
      "   -70  -383  -615  -435  -211  -225  -302  -138   -70  -310   -70  -725\n",
      "  -148  -628  -139  -441  -965  -491  -197  -169  -233   -72  -213  -134\n",
      "  -400  -294  -559  -182  -514  -145  -183  -738  -553  -416  -389  -171\n",
      "  -347  -286  -732  -372   -82  -712   -87  -366  -110  -425  -126  -502\n",
      " -1024  -386  -224  -308  -567  -253  -608  -405  -582  -578  -365  -345\n",
      "   -68  -310 -1024  -344  -112  -938  -362  -297  -588  -125  -225  -147\n",
      "  -145  -517  -135  -211  -570  -202  -379  -293  -701  -251  -344  -206\n",
      "  -286  -246 -1024  -157   -52  -155  -281  -393  -180  -204  -430  -296\n",
      "  -331  -148  -354  -115  -134  -827  -294  -192  -270  -400  -596  -271\n",
      "  -272  -718  -185  -307  -372  -270  -252  -151  -361  -259  -261   -93\n",
      "  -817  -792  -453  -162  -171  -661 -1011  -174  -196  -297  -207  -154\n",
      "  -144  -182  -541  -643  -159  -212  -260  -578  -151  -665  -343  -308\n",
      "  -390  -602  -551  -310  -219  -279  -147  -289  -350  -886  -638  -287\n",
      "   -56  -369  -304  -377  -204  -399  -365  -619  -539  -450  -309  -141\n",
      "  -698  -130  -721  -144  -613  -584  -288  -378  -424  -328  -677  -655\n",
      "  -193  -949  -126  -131  -268  -463  -414  -479  -134   -97 -1024  -195\n",
      "  -414  -155   -88  -312  -676  -310  -115  -405  -778  -580  -586  -417\n",
      "  -236  -639  -505  -408  -186  -615  -405  -498  -425  -519  -150  -378\n",
      "   -97  -616  -114  -168  -324  -449  -148  -550  -201  -231  -531  -240\n",
      "  -409  -286  -574  -497  -239  -225  -254  -558  -695  -161  -149  -191\n",
      "  -381  -174  -571  -617  -162  -591  -504  -121  -576  -375  -591  -428\n",
      "  -170   -96  -260  -602  -143  -590  -718  -268   -77  -534  -190  -265\n",
      "  -100  -269  -121 -1024  -392  -442  -225  -233  -277  -125  -243  -358\n",
      "  -234  -615  -156  -391  -415  -241  -282  -179  -416  -567  -530  -402\n",
      "  -833  -140  -192  -556  -726  -175  -550  -211  -213  -299  -130  -362\n",
      "  -127  -522  -627  -613  -114  -140  -232  -123  -120  -371  -174  -246\n",
      "  -300  -386  -274  -192  -818  -344  -263  -220   -82  -160  -111  -173\n",
      "  -167  -401  -823  -111  -221  -479  -434  -534  -795  -257  -559  -231\n",
      "  -647  -366  -324  -699  -232  -137  -623  -235  -134  -570  -358  -286\n",
      "   -84   -91  -495  -355  -269  -408  -321  -583  -170  -172  -200  -400\n",
      "  -564  -135  -169  -181  -327  -588  -772  -545  -216   -96  -489  -261\n",
      "  -327 -1001  -386  -282  -316  -787  -204  -291  -374  -414  -561  -269\n",
      "  -110  -196  -176  -587  -229  -270  -277  -107]\n",
      "indices_sorted = \n",
      "[132 274 146 170 351  72 210 457  57 100 265 149 237   3 372 185 410 400\n",
      " 204  44  31 416 205 461 284 450  52 115 122 376  95  10  41 254 193 342\n",
      "  37 125  26 164 423 252 320   6  15 262 280 225 209 263  43 420 219 289\n",
      " 238  97 386 426  67 247 327 301 361  86 293 256  48 387 138 229 339 190\n",
      "  12 329 334 341 449 152 471 286 257 439 140 285  42 223 141 332 314 326\n",
      " 429 160 369 136 444 466  16 110 418 319 375 116 230 378 307  35 451 218\n",
      "  21 248 415 345 310 370 385 297 157 112 290 330 131 295 315 434  53 101\n",
      " 454 413 271   8 269 206  54 249 305 353  99  87 414  29 178  81 335 129\n",
      " 296 260  59 287 117 368 364 276 270 465 312 291 437 283 139 294 371 409\n",
      " 108 189 443 245 175 352 363 228 118 458 133 397  85 324 162 299 259 243\n",
      " 333 464 196 123 393 241  25 127 421  18 246 142 383 150 200 430 359 435\n",
      " 182 236  28 120  27 143 147 166 401 226  76  32 180 261 456 448 304 422\n",
      " 438   7 460 279  71  93 145 231 281 250 227 135 195  40  47 242  90  45\n",
      " 396 381  34 151 213 179  83 109 186 163  30 463 235 258 239 431 121 313\n",
      " 168  58 459 366 174 233 356 474 398  73 192 191 473 197 188 349 467 436\n",
      " 268 343  38  70 347 402  65 202 455  80 338 222 201  14 417  64 318   0\n",
      " 137 198 165 395 169 358  46 365  51 311 316  50 288 427 360 104 355 390\n",
      " 424  77 419 309 472   1  89 154 354 317 134 412 403  19 232 452  69 106\n",
      " 380 221   4  88 159 379 214  39 167 177 244 462   2 161  61 308 442 102\n",
      " 469  79 212 275 264 374 399 187 323  17 346  60 292 194 114  63 217 111\n",
      " 447 176  78 367 470  13 377 211 325 394  36 407 441 208  66 119 440 336\n",
      " 103 446 303 408  74  82 207 328  49 321 405 220 171 362 173 277 215 199\n",
      "  55 224 298  33 322  96 181 306  68 234 155 156 113 255 216 340 251 373\n",
      " 389  98  91 425 445 158 107 272 184 428  23 267 253 382 384 130 266 357\n",
      " 153 391 331  75 350  22 392   5 282 183 302 388 148 411 406   9  20 468\n",
      " 128 475  56  11 348 273 300 453 337  62 203  24 433 278 126 432 404 124\n",
      " 344 105  94  92  84 144 240 172]\n",
      "chunk 132 = \n",
      "-1024\n",
      "BucketSampler divide_chunks\n",
      "u = \n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127]\n",
      "batch = \n",
      "[132 274 146 170 351  72 210 457  57 100 265 149 237   3 372 185 410 400\n",
      " 204  44  31 416 205 461 284 450  52 115 122 376  95  10  41 254 193 342\n",
      "  37 125  26 164 423 252 320   6  15 262 280 225 209 263  43 420 219 289\n",
      " 238  97 386 426  67 247 327 301 361  86 293 256  48 387 138 229 339 190\n",
      "  12 329 334 341 449 152 471 286 257 439 140 285  42 223 141 332 314 326\n",
      " 429 160 369 136 444 466  16 110 418 319 375 116 230 378 307  35 451 218\n",
      "  21 248 415 345 310 370 385 297 157 112 290 330 131 295 315 434  53 101\n",
      " 454 413]\n",
      "u = \n",
      "[128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
      " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
      " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
      " 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
      " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235\n",
      " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
      " 254 255]\n",
      "batch = \n",
      "[271   8 269 206  54 249 305 353  99  87 414  29 178  81 335 129 296 260\n",
      "  59 287 117 368 364 276 270 465 312 291 437 283 139 294 371 409 108 189\n",
      " 443 245 175 352 363 228 118 458 133 397  85 324 162 299 259 243 333 464\n",
      " 196 123 393 241  25 127 421  18 246 142 383 150 200 430 359 435 182 236\n",
      "  28 120  27 143 147 166 401 226  76  32 180 261 456 448 304 422 438   7\n",
      " 460 279  71  93 145 231 281 250 227 135 195  40  47 242  90  45 396 381\n",
      "  34 151 213 179  83 109 186 163  30 463 235 258 239 431 121 313 168  58\n",
      " 459 366]\n",
      "u = \n",
      "[256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273\n",
      " 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291\n",
      " 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309\n",
      " 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327\n",
      " 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345\n",
      " 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363\n",
      " 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381\n",
      " 382 383]\n",
      "batch = \n",
      "[174 233 356 474 398  73 192 191 473 197 188 349 467 436 268 343  38  70\n",
      " 347 402  65 202 455  80 338 222 201  14 417  64 318   0 137 198 165 395\n",
      " 169 358  46 365  51 311 316  50 288 427 360 104 355 390 424  77 419 309\n",
      " 472   1  89 154 354 317 134 412 403  19 232 452  69 106 380 221   4  88\n",
      " 159 379 214  39 167 177 244 462   2 161  61 308 442 102 469  79 212 275\n",
      " 264 374 399 187 323  17 346  60 292 194 114  63 217 111 447 176  78 367\n",
      " 470  13 377 211 325 394  36 407 441 208  66 119 440 336 103 446 303 408\n",
      "  74  82]\n",
      "u = \n",
      "[384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
      " 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437\n",
      " 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455\n",
      " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473\n",
      " 474 475]\n",
      "batch = \n",
      "[207 328  49 321 405 220 171 362 173 277 215 199  55 224 298  33 322  96\n",
      " 181 306  68 234 155 156 113 255 216 340 251 373 389  98  91 425 445 158\n",
      " 107 272 184 428  23 267 253 382 384 130 266 357 153 391 331  75 350  22\n",
      " 392   5 282 183 302 388 148 411 406   9  20 468 128 475  56  11 348 273\n",
      " 300 453 337  62 203  24 433 278 126 432 404 124 344 105  94  92  84 144\n",
      " 240 172]\n",
      "***************************batches = ********************************\n",
      "[[132, 274, 146, 170, 351, 72, 210, 457, 57, 100, 265, 149, 237, 3, 372, 185, 410, 400, 204, 44, 31, 416, 205, 461, 284, 450, 52, 115, 122, 376, 95, 10, 41, 254, 193, 342, 37, 125, 26, 164, 423, 252, 320, 6, 15, 262, 280, 225, 209, 263, 43, 420, 219, 289, 238, 97, 386, 426, 67, 247, 327, 301, 361, 86, 293, 256, 48, 387, 138, 229, 339, 190, 12, 329, 334, 341, 449, 152, 471, 286, 257, 439, 140, 285, 42, 223, 141, 332, 314, 326, 429, 160, 369, 136, 444, 466, 16, 110, 418, 319, 375, 116, 230, 378, 307, 35, 451, 218, 21, 248, 415, 345, 310, 370, 385, 297, 157, 112, 290, 330, 131, 295, 315, 434, 53, 101, 454, 413], [271, 8, 269, 206, 54, 249, 305, 353, 99, 87, 414, 29, 178, 81, 335, 129, 296, 260, 59, 287, 117, 368, 364, 276, 270, 465, 312, 291, 437, 283, 139, 294, 371, 409, 108, 189, 443, 245, 175, 352, 363, 228, 118, 458, 133, 397, 85, 324, 162, 299, 259, 243, 333, 464, 196, 123, 393, 241, 25, 127, 421, 18, 246, 142, 383, 150, 200, 430, 359, 435, 182, 236, 28, 120, 27, 143, 147, 166, 401, 226, 76, 32, 180, 261, 456, 448, 304, 422, 438, 7, 460, 279, 71, 93, 145, 231, 281, 250, 227, 135, 195, 40, 47, 242, 90, 45, 396, 381, 34, 151, 213, 179, 83, 109, 186, 163, 30, 463, 235, 258, 239, 431, 121, 313, 168, 58, 459, 366], [174, 233, 356, 474, 398, 73, 192, 191, 473, 197, 188, 349, 467, 436, 268, 343, 38, 70, 347, 402, 65, 202, 455, 80, 338, 222, 201, 14, 417, 64, 318, 0, 137, 198, 165, 395, 169, 358, 46, 365, 51, 311, 316, 50, 288, 427, 360, 104, 355, 390, 424, 77, 419, 309, 472, 1, 89, 154, 354, 317, 134, 412, 403, 19, 232, 452, 69, 106, 380, 221, 4, 88, 159, 379, 214, 39, 167, 177, 244, 462, 2, 161, 61, 308, 442, 102, 469, 79, 212, 275, 264, 374, 399, 187, 323, 17, 346, 60, 292, 194, 114, 63, 217, 111, 447, 176, 78, 367, 470, 13, 377, 211, 325, 394, 36, 407, 441, 208, 66, 119, 440, 336, 103, 446, 303, 408, 74, 82]]\n",
      "*********************************************************************\n",
      "new_indices = \n",
      "[132, 274, 146, 170, 351, 72, 210, 457, 57, 100, 265, 149, 237, 3, 372, 185, 410, 400, 204, 44, 31, 416, 205, 461, 284, 450, 52, 115, 122, 376, 95, 10, 41, 254, 193, 342, 37, 125, 26, 164, 423, 252, 320, 6, 15, 262, 280, 225, 209, 263, 43, 420, 219, 289, 238, 97, 386, 426, 67, 247, 327, 301, 361, 86, 293, 256, 48, 387, 138, 229, 339, 190, 12, 329, 334, 341, 449, 152, 471, 286, 257, 439, 140, 285, 42, 223, 141, 332, 314, 326, 429, 160, 369, 136, 444, 466, 16, 110, 418, 319, 375, 116, 230, 378, 307, 35, 451, 218, 21, 248, 415, 345, 310, 370, 385, 297, 157, 112, 290, 330, 131, 295, 315, 434, 53, 101, 454, 413, 271, 8, 269, 206, 54, 249, 305, 353, 99, 87, 414, 29, 178, 81, 335, 129, 296, 260, 59, 287, 117, 368, 364, 276, 270, 465, 312, 291, 437, 283, 139, 294, 371, 409, 108, 189, 443, 245, 175, 352, 363, 228, 118, 458, 133, 397, 85, 324, 162, 299, 259, 243, 333, 464, 196, 123, 393, 241, 25, 127, 421, 18, 246, 142, 383, 150, 200, 430, 359, 435, 182, 236, 28, 120, 27, 143, 147, 166, 401, 226, 76, 32, 180, 261, 456, 448, 304, 422, 438, 7, 460, 279, 71, 93, 145, 231, 281, 250, 227, 135, 195, 40, 47, 242, 90, 45, 396, 381, 34, 151, 213, 179, 83, 109, 186, 163, 30, 463, 235, 258, 239, 431, 121, 313, 168, 58, 459, 366, 174, 233, 356, 474, 398, 73, 192, 191, 473, 197, 188, 349, 467, 436, 268, 343, 38, 70, 347, 402, 65, 202, 455, 80, 338, 222, 201, 14, 417, 64, 318, 0, 137, 198, 165, 395, 169, 358, 46, 365, 51, 311, 316, 50, 288, 427, 360, 104, 355, 390, 424, 77, 419, 309, 472, 1, 89, 154, 354, 317, 134, 412, 403, 19, 232, 452, 69, 106, 380, 221, 4, 88, 159, 379, 214, 39, 167, 177, 244, 462, 2, 161, 61, 308, 442, 102, 469, 79, 212, 275, 264, 374, 399, 187, 323, 17, 346, 60, 292, 194, 114, 63, 217, 111, 447, 176, 78, 367, 470, 13, 377, 211, 325, 394, 36, 407, 441, 208, 66, 119, 440, 336, 103, 446, 303, 408, 74, 82]\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!new_indices = !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "476\n",
      "#############################indices = ##################################\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475]\n",
      "###############\n",
      "!!!!!!!!!!!!!!!!!!!!!self.index = !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[132 274 146 170 351  72 210 457  57 100 265 149 237   3 372 185 410 400\n",
      " 204  44  31 416 205 461 284 450  52 115 122 376  95  10  41 254 193 342\n",
      "  37 125  26 164 423 252 320   6  15 262 280 225 209 263  43 420 219 289\n",
      " 238  97 386 426  67 247 327 301 361  86 293 256  48 387 138 229 339 190\n",
      "  12 329 334 341 449 152 471 286 257 439 140 285  42 223 141 332 314 326\n",
      " 429 160 369 136 444 466  16 110 418 319 375 116 230 378 307  35 451 218\n",
      "  21 248 415 345 310 370 385 297 157 112 290 330 131 295 315 434  53 101\n",
      " 454 413 271   8 269 206  54 249 305 353  99  87 414  29 178  81 335 129\n",
      " 296 260  59 287 117 368 364 276 270 465 312 291 437 283 139 294 371 409\n",
      " 108 189 443 245 175 352 363 228 118 458 133 397  85 324 162 299 259 243\n",
      " 333 464 196 123 393 241  25 127 421  18 246 142 383 150 200 430 359 435\n",
      " 182 236  28 120  27 143 147 166 401 226  76  32 180 261 456 448 304 422\n",
      " 438   7 460 279  71  93 145 231 281 250 227 135 195  40  47 242  90  45\n",
      " 396 381  34 151 213 179  83 109 186 163  30 463 235 258 239 431 121 313\n",
      " 168  58 459 366 174 233 356 474 398  73 192 191 473 197 188 349 467 436\n",
      " 268 343  38  70 347 402  65 202 455  80 338 222 201  14 417  64 318   0\n",
      " 137 198 165 395 169 358  46 365  51 311 316  50 288 427 360 104 355 390\n",
      " 424  77 419 309 472   1  89 154 354 317 134 412 403  19 232 452  69 106\n",
      " 380 221   4  88 159 379 214  39 167 177 244 462   2 161  61 308 442 102\n",
      " 469  79 212 275 264 374 399 187 323  17 346  60 292 194 114  63 217 111\n",
      " 447 176  78 367 470  13 377 211 325 394  36 407 441 208  66 119 440 336\n",
      " 103 446 303 408  74  82 207 328  49 321 405 220 171 362 173 277 215 199\n",
      "  55 224 298  33 322  96 181 306  68 234 155 156 113 255 216 340 251 373\n",
      " 389  98  91 425 445 158 107 272 184 428  23 267 253 382 384 130 266 357\n",
      " 153 391 331  75 350  22 392   5 282 183 302 388 148 411 406   9  20 468\n",
      " 128 475  56  11 348 273 300 453 337  62 203  24 433 278 126 432 404 124\n",
      " 344 105  94  92  84 144 240 172]\n",
      "collate_fn = \n",
      "<function collate_fn at 0x7f76cc7537b8>\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TextDataset(test_x)\n",
    "#将test_x放入TextDataset之中，test_x = (476,2),里面的2中的第一个list\n",
    "#内容为第一个list对应值([53, 1860, 13032, 4097, 328, 7199, 49, 18435, 32, 12, 131,\n",
    "#363, 13, 293, 817, 102, 63, 4, 72, 2276, 16, 13032, 45, 64, 58, 31, \n",
    "#250, 59, 2, 4646, 4, 332, 117, 4097, 328, 40, 1, 12, 122, 25, 12, 42, \n",
    "#67, 8, 30598, 4, 495, 117, 15, 25, 76, 53, 1869, 37, 1087, 732, 19, 8, 45595, 1...])\n",
    "#(里面的内容为问句的单词相应的编号)\n",
    "#第二个list对应值 list([57, 11, 95, 6243, 30, 1860, 13032, 1719, 1, 71, 23, 79, 38, \n",
    "#1098, 8, 1394, 16, 1845, 19, 55, 342, 16, 508, 8, 1511, 3170, 7076, 1])]\n",
    "#(里面的内容为答句的单词相应的编号)\n",
    "test_sampler = BucketSampler(test_dataset, test_dataset.get_keys(),\n",
    "                             batch_size=batch_size, shuffle_data=False)\n",
    "#这里面定义完test_sampler之后就初始化完了\n",
    "\n",
    "#get_keys()输入的是两个数组长度相加，比如self.seqs[0][0] = 225,self.seqs[0][1] = 28\n",
    "#此时得到的长度self.seqs[0] = 225+28 = 253\n",
    "print('collate_fn = ')\n",
    "print(collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler,\n",
    "                         shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "#DataLoader本质上是一个可迭代的对象，使用iter()访问，不能使用next()访问\n",
    "#collate_fn应当是一个可调用对象，DataLoader不设置collate_fn参数时每个\n",
    "#mini-batch输出的样本序列，所以可以理解为一个后处理的函数\n",
    "\n",
    "#class BucketSampler(Sampler)这里定义BucketSampler的时候直接继承了Sampler的内容\n",
    "#collate_fn为上面定义了的相应的collate_fn的函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.BucketSampler object at 0x7f76ca3b9400>\n"
     ]
    }
   ],
   "source": [
    "print(test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmUnit(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_matrix, lstm_hidden_size=120, gru_hidden_size=60):\n",
    "        super(LstmUnit, self).__init__()\n",
    "        print('LstmUnit __init__')\n",
    "    #第一次初始化的时候self.q_lstm = LstmUnit(embedding_matrix, q_lstm_size(120), q_lstm2_size(120))\n",
    "    #第二次初始化的时候self.a_lstm = LstmUnit(embedding_matrix, a_lstm_size(120), a_lstm2_size(120))\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "    #*args与**kwargs的区别：两者都是python中的可变参数\n",
    "    #这里面的embedding_matrix.shape正好为一个矩阵的形状数组，所以使用*\n",
    "    #标志进行num_embeddings以及embedding_dim的数值放置\n",
    "    \n",
    "    #*args表示任何多个无名参数，它本质上是一个tuple\n",
    "    #**kwargs表示关键字参数，它本质上是一个dict\n",
    "    \n",
    "    #torch.nn.Embedding(num_embeddings,embedding_dim,padding_idx = None,\n",
    "    #max_norm = None,norm_type = 2,scale_grad_by_freq = False,sparse = False)\n",
    "\n",
    "    #如果同时使用*args和**kwargs时，必须*args参数列要在**kwargs之前\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "    #在pytorch里面使用word2vec训练好的词向量,embedding_matrix = (47462,300)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    #在反向传播的时候，不要对这些词向量进行求导更新\n",
    "        self.embedding_dropout = nn.Dropout2d(0.2)\n",
    "\n",
    "        print('embedding_matrix.shape1 = ')\n",
    "        print(embedding_matrix.shape[1])\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], lstm_hidden_size, bidirectional=True, batch_first=True)\n",
    "        #nn.LSTM输入的参数列表表示:input_size输入数据的特征维数，通常就是\n",
    "        #embedding_dim(词向量的维度) = 300,lstm_hidden_size = 120\n",
    "        \n",
    "        #hidden_size = lstm_hidden_size为LSTM中隐层的维度\n",
    "        #num_layers:循环神经网络的层数，bias:用不用偏置，default = True\n",
    "        \n",
    "        #batch_first 这个要注意，通常我们输入的数据shape=(batch_size,seq_length,embedding_dim),而batch_first默认\n",
    "        #是False,所以我们的输入数据最好送进LSTM之前将batch_size与seq_length这两个维度调换\n",
    "        #如果batch_first = True的时候，此时第一个放入batch_size(embedding_matrix.shape[1])即可\n",
    "        #因为此时batch_first意味着batch_size放在第一位的位置\n",
    "        \n",
    "        #dropout:默认是0，代表不用dropout\n",
    "        #bidirectional默认是false，代表不用双向LSTM\n",
    "        self.lstm2 = nn.LSTM(lstm_hidden_size * 2, gru_hidden_size, bidirectional=True, batch_first=True)\n",
    "        #lstm_hidden_size*2 = 240,gru_hidden_size = 60\n",
    "        \n",
    "    def apply_spatial_dropout(self, h_embedding):\n",
    "        print('LstmUnit apply_spatial_dropout')\n",
    "        h_embedding = h_embedding.transpose(1, 2).unsqueeze(2)\n",
    "        print('shape1 = ')\n",
    "        print(h_embedding.transpose(1,2).shape)\n",
    "        print('shape2 = ')\n",
    "        print(h_embedding.transpose(1,2).unsqueeze(2).shape)\n",
    "        h_embedding = self.embedding_dropout(h_embedding).squeeze(2).transpose(1, 2)\n",
    "        return h_embedding\n",
    "    \n",
    "    def flatten_parameters(self):\n",
    "        print('LstmUnit flatten_parameters')\n",
    "        self.lstm.flatten_parameters()\n",
    "        print('self.lstm.shape = ')\n",
    "        print(self.lstm.shape)\n",
    "        self.lstm2.flatten_parameters()\n",
    "        print('self.lstm2.shape = ')\n",
    "        print(self.lstm2.shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('LstmUnit forward')\n",
    "        batch_size = x.size(0)\n",
    "        h_embedding = self.embedding(x)\n",
    "        #self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        print('h_embedding1 = ')\n",
    "        print(h_embedding.shape)\n",
    "        h_embedding = self.apply_spatial_dropout(h_embedding)\n",
    "        print('h_embedding2 = ')\n",
    "        print(h_embedding.shape)\n",
    "        h_lstm, data1 = self.lstm(h_embedding)\n",
    "        print('h_lstm = ')\n",
    "        print(h_lstm)\n",
    "        print('data1 = ')\n",
    "        print(data1)\n",
    "        #self.lstm = nn.LSTM(embedding_matrix.shape[1], lstm_hidden_size, bidirectional=True, batch_first=True)\n",
    "        h_lstm, data2 = self.lstm2(h_lstm)\n",
    "        print('h_lstm = ')\n",
    "        print(h_lstm)\n",
    "        print('data2 = ')\n",
    "        print(data2)\n",
    "        #self.lstm2 = nn.LSTM(lstm_hidden_size * 2, gru_hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "\n",
    "        out = torch.cat((avg_pool, max_pool), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LstmModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(LstmModel, self).__init__()\n",
    "        print('LstmModel __init__')\n",
    "        #embedding_matrix为上面fasttext+word2vec训练出来的内容\n",
    "        #\n",
    "        \n",
    "        q_lstm_size = 120\n",
    "        q_lstm2_size = 120\n",
    "        \n",
    "        a_lstm_size = 120\n",
    "        a_lstm2_size = 120\n",
    "        \n",
    "        self.q_lstm = LstmUnit(embedding_matrix, q_lstm_size, q_lstm2_size)\n",
    "        self.a_lstm = LstmUnit(embedding_matrix, a_lstm_size, a_lstm2_size)\n",
    "\n",
    "        self.linear = nn.Linear((q_lstm2_size + a_lstm2_size) * 4 + 512 * 2, 200)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(200, num_targets)\n",
    "        \n",
    "    def flatten_parameters(self):\n",
    "        print('LstmModel flatten_parameters')\n",
    "        self.q_lstm.flatten_parameters()\n",
    "        self.a_lstm.flatten_parameters()\n",
    "        \n",
    "    def forward(self, q_seqs, a_seqs, text_emb):\n",
    "        print('LstmModel forward')\n",
    "        h_q = self.q_lstm(q_seqs)\n",
    "        h_a = self.a_lstm(a_seqs)\n",
    "\n",
    "        conc = torch.cat((h_q, h_a, text_emb), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA(object):\n",
    "\n",
    "    def __init__(self, model, mu, level='batch', n=1):\n",
    "        # self.ema_model = copy.deepcopy(model)\n",
    "        print('EMA __init__')\n",
    "        self.mu = mu\n",
    "        self.level = level\n",
    "        self.n = n\n",
    "        self.cnt = self.n\n",
    "        self.shadow = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data\n",
    "\n",
    "    def _update(self, model):\n",
    "        print('EMA _update')\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                new_average = (1 - self.mu) * param.data + self.mu * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def set_weights(self, ema_model):\n",
    "        print('EMA set_weights')\n",
    "        for name, param in ema_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def on_batch_end(self, model):\n",
    "        print('EMA on_batch_end')\n",
    "        if self.level is 'batch':\n",
    "            self.cnt -= 1\n",
    "            if self.cnt == 0:\n",
    "                self._update(model)\n",
    "                self.cnt = self.n\n",
    "                \n",
    "    def on_epoch_end(self, model):\n",
    "        print('EMA on_epoch_end')\n",
    "        if self.level is 'epoch':\n",
    "            self._update(model)\n",
    "def get_scores(y_true, y_pred) -> Dict[str, float]:\n",
    "    print('get_scores')\n",
    "    # y_true, y_pred: np.ndarray with shape (sample_size, num_targets)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    assert y_true.shape[1] == num_targets\n",
    "    scores = {}\n",
    "    for target_name, i in zip(target_names, range(y_true.shape[1])):\n",
    "        scores[target_name] = scipy.stats.spearmanr(y_true[:, i], y_pred[:, i])[0]\n",
    "    return scores\n",
    "\n",
    "\n",
    "def predict(model: nn.Module,\n",
    "            data_loader: DataLoader,\n",
    "            text_emb: torch.Tensor,\n",
    "            device: torch.device = torch.device('cuda')) -> np.ndarray:\n",
    "    print('predict')\n",
    "    model.eval()\n",
    "    preds_fold = np.zeros((len(data_loader.dataset), num_targets))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, x_batch in data_loader:\n",
    "            x_batch = (x.to(device) for x in x_batch)\n",
    "            emb_batch = text_emb[list(index)].to(device)\n",
    "            y_pred = model(*x_batch, emb_batch).detach()\n",
    "            preds_fold[list(index)] = torch.sigmoid(y_pred.cpu()).numpy()\n",
    "    return preds_fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "train_fold_emb = \n",
      "torch.Size([4863, 1024])\n",
      "valid_fold_emb = \n",
      "torch.Size([1216, 1024])\n",
      "TextDataset __init__\n",
      "TextDataset __init__\n",
      "sampler = \n",
      "TextDataset get_keys\n",
      "BucketSampler __init__\n",
      "after get_keys\n",
      "[282 387 216 ... 178 470 313]\n",
      "sort_keys.shape = \n",
      "(4863,)\n",
      "self.bucket_size = \n",
      "2560\n",
      "TextDataset get_keys\n",
      "BucketSampler __init__\n",
      "after get_keys\n",
      "[370 359 165 ... 245 351 382]\n",
      "sort_keys.shape = \n",
      "(1216,)\n",
      "self.bucket_size = \n",
      "1216\n",
      "BucketSampler prepare_buckets\n",
      "indices = \n",
      "[   0    1    2 ... 1213 1214 1215]\n",
      "lens = \n",
      "[-370 -359 -165 ... -245 -351 -382]\n",
      "self.bucket_size = \n",
      "1216\n",
      "BucketSampler divide_chunks\n",
      "chunk_index = \n",
      "[   0    1    2 ... 1213 1214 1215]\n",
      "chunk = \n",
      "[-370 -359 -165 ... -245 -351 -382]\n",
      "indices_sorted = \n",
      "[540 941 282 ... 565 327 493]\n",
      "chunk 132 = \n",
      "-300\n",
      "BucketSampler divide_chunks\n",
      "u = \n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127]\n",
      "batch = \n",
      "[ 540  941  282  751  151  487  300  438  399  930  573  193  595 1043\n",
      "  891   99  888  360 1067  987  495  406  160  984   77 1111  986  761\n",
      "  777    7  921   69 1072  768 1028  592  855  275  785 1050  465  437\n",
      "  343  261  366  695   92  627  660  683  276  382  971  965  421  140\n",
      "   48  633  577  127  937 1095  229  818   18  235  368  581 1076  203\n",
      "  545  690  586  894  126  943  625 1137   88  719  948   25  962  629\n",
      " 1056  729  236  810  981  176  530  455  583  207  144 1016  152  787\n",
      "  135  103  521  277  306  310  108  479  319 1078  389 1117  391  264\n",
      "   21   42 1061  907  302  863  221 1030   91  414  707 1093 1101  473\n",
      "  555  908]\n",
      "u = \n",
      "[128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
      " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
      " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
      " 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
      " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235\n",
      " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
      " 254 255]\n",
      "batch = \n",
      "[ 500 1185  842 1142  507  432  461  844  214  439 1180  944  641 1083\n",
      "  383  245  435  875  587 1200   71  192  917  485   19  488  717  607\n",
      "  115  667 1020  445  680  492  433 1211  263  551  122  211  852  174\n",
      " 1199  513  504   89  869  464  426  926  225  733  422 1122  884  288\n",
      "  895  619 1201  602  546 1099  385  293  860  100  284  423  827  121\n",
      "  357  744  448 1136  404 1178  390  854   11 1073  828  552  899 1139\n",
      "  750 1151  280  647  881  369  570 1005  916  205  512   80  901   15\n",
      "   59  770  482 1175  137  514  260   47  637  952  608  402    3  320\n",
      "  331  834  753  114  796  778 1008  255 1176  523 1001  919  893  200\n",
      "  332  462]\n",
      "u = \n",
      "[256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273\n",
      " 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291\n",
      " 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309\n",
      " 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327\n",
      " 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345\n",
      " 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363\n",
      " 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381\n",
      " 382 383]\n",
      "batch = \n",
      "[ 697  281 1160  361 1068  252  757  424 1129  232  995  758  703  661\n",
      "  885  849  599  662  960   78  292  669   97  183  202   45  731   43\n",
      "  124  459   51  924  949 1186 1060  867  631  428   22   16  238  743\n",
      "  273  655  670  328  582  557  489 1052  234 1084  301  247 1102  883\n",
      "  483  666 1103  172  145   83 1089 1086  912  783  179  835  528 1209\n",
      "  837  453  580  515  556  832  398  443   75 1108  329  181  740  913\n",
      " 1123 1110 1114  560   70  111 1074  896  294  472  561  889  598  953\n",
      " 1154  699  569  682  659  701  628  600 1177  129  915 1192 1148 1167\n",
      "  411 1011  157  872  377  272  440   60  291   64 1146 1100  676 1140\n",
      "  658  572]\n",
      "u = \n",
      "[384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419\n",
      " 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437\n",
      " 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455\n",
      " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473\n",
      " 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491\n",
      " 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509\n",
      " 510 511]\n",
      "batch = \n",
      "[  67  605  325  506  710  376  415  330  536  314 1071  307  969  938\n",
      "  477  447   58  450  847  508  527 1162  716  189  588 1094  117  309\n",
      "  471   57  533  897  359  673   68 1125  767  808  845 1097   96  524\n",
      " 1193 1215  265  585   17  671  345 1075 1092  194   44  610   34  789\n",
      "  809   31  978  227 1189  618 1173  499  209  749    0  643  634  955\n",
      " 1057  626  798  914  566  983  243  616  841  589   76  392   61  244\n",
      "  772  648 1147  251  256  456   93    1  198  931   52  452  653 1047\n",
      "  711  125   50   14  640  148  864  871  375  412 1179  349 1214 1014\n",
      "  858   98  715 1040  741  584 1115 1055 1202  321  876  454  597  723\n",
      "  830  700]\n",
      "u = \n",
      "[512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529\n",
      " 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547\n",
      " 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565\n",
      " 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583\n",
      " 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601\n",
      " 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619\n",
      " 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637\n",
      " 638 639]\n",
      "batch = \n",
      "[ 910  308  620 1183   53  708  105  141  509  373  804  153  905 1197\n",
      "  964  324  136  381   66  824  642    6  230  178  769  780  578  297\n",
      "  334  370  996  217  550   73 1081  737  840  833  727 1168 1135  416\n",
      " 1190  502 1053  522  215 1049   72  857 1041  621 1037  959 1091  173\n",
      "  553  725  425  384  497  982  269  718  248    5 1195  197  326 1054\n",
      " 1034  548  346  668  305  481  942  372  298  754  210  694  474 1166\n",
      "  132 1128  466 1210 1069  945  739 1204  814  394 1161  344  684 1010\n",
      "  420  882  242 1062 1121  856 1051 1155  206   62  564  665  692  886\n",
      "  350 1035 1174   40 1106 1188  233  990  102  110  902 1132  374  649\n",
      "  106   54]\n",
      "u = \n",
      "[640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657\n",
      " 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675\n",
      " 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693\n",
      " 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711\n",
      " 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729\n",
      " 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747\n",
      " 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765\n",
      " 766 767]\n",
      "batch = \n",
      "[1004  228  534 1032  218   55  241  636 1120  651  476 1096  418  797\n",
      "  299  612  304  630  792 1171  335  175   32 1141  874  223 1133 1196\n",
      "  958   79  478  201 1198  788  803 1212  817  405  396 1164  988  313\n",
      "  806  936  400  839  449 1126  567  258  417 1153  735   63  686  395\n",
      "  635  362  596   85 1156  191  815  480  285 1170 1207  999  614  568\n",
      "  622  877 1187  491   81 1182  823  347  196  822 1000  430  807 1015\n",
      "  836  259  946  838  219  811  401 1003  851  341 1143  611 1213  363\n",
      " 1169  604  859  892 1159  922 1105  742 1181  776 1203  906  278  755\n",
      "  781  469  434  726  516  691  188  409   82  571  351  147  979  764\n",
      "  591   27]\n",
      "u = \n",
      "[768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785\n",
      " 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803\n",
      " 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821\n",
      " 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839\n",
      " 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857\n",
      " 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875\n",
      " 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893\n",
      " 894 895]\n",
      "batch = \n",
      "[ 155  463  870  249   94  968  113  705  222 1206  918  163 1063  821\n",
      "  246  289 1002  574  156 1113   46    4  829  903  290  355  652  486\n",
      "  993  801  107 1025  190  624  142 1119  738  563  134  748  928   28\n",
      "  734  239 1042  654  672 1048  998  168  799  623  387  848  813  371\n",
      "  650   41   65  929 1158  559 1065  615  689 1131  775  143  118  674\n",
      "  112 1036  212  747  696 1184  139  934  774  724  765  518  520  138\n",
      "  543 1138  128 1039  939  601   38  843 1079 1087  182  165  547  593\n",
      "  271  295  722 1208  154  169  457  923   86 1038  240  268  358 1026\n",
      "  323    9  270 1118  721  645  403  606  186  444  353 1082  171  973\n",
      "  558  441]\n",
      "u = \n",
      "[ 896  897  898  899  900  901  902  903  904  905  906  907  908  909\n",
      "  910  911  912  913  914  915  916  917  918  919  920  921  922  923\n",
      "  924  925  926  927  928  929  930  931  932  933  934  935  936  937\n",
      "  938  939  940  941  942  943  944  945  946  947  948  949  950  951\n",
      "  952  953  954  955  956  957  958  959  960  961  962  963  964  965\n",
      "  966  967  968  969  970  971  972  973  974  975  976  977  978  979\n",
      "  980  981  982  983  984  985  986  987  988  989  990  991  992  993\n",
      "  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007\n",
      " 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021\n",
      " 1022 1023]\n",
      "batch = \n",
      "[1007   20   95 1024  846 1064  475  177  166  101  644   39 1022  388\n",
      "  490 1149  123  554  678  687  356  541  935 1112  266 1165  904 1059\n",
      "  184  613  887  231  352  149 1085  410  617 1134 1019  706  213  336\n",
      "  940  575  237  505  873  793  104  603 1046 1116  119  544  287  354\n",
      " 1023  704  274 1006 1017 1172  333  879  656  525 1157  677  162  820\n",
      "  976  386 1098   23  253  451  989 1045    2  997  950  763  994  532\n",
      "  609 1029  484  853  519  675  170  378 1044  966  408  130  816  954\n",
      "  819  427   87  698  794  590  109  632  380  909  866 1066  951  379\n",
      "   74  526 1130  766 1124  120  436  364  339  393  150  517 1090  709\n",
      " 1058   12]\n",
      "u = \n",
      "[1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037\n",
      " 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051\n",
      " 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065\n",
      " 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079\n",
      " 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093\n",
      " 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107\n",
      " 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121\n",
      " 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135\n",
      " 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149\n",
      " 1150 1151]\n",
      "batch = \n",
      "[ 539  800  784  315  890 1012 1080  657  419   26  226  782  878  825\n",
      "  992  529  985  367 1088 1194  927 1018  752  562  537  664 1205  646\n",
      "  167  220  185  756  947  732   49  861  131  972  925   84  826 1191\n",
      " 1127  638  685  868  880  759 1013  158  779  311 1033  216  501  224\n",
      " 1027  318   13  802  342 1077  980  503  164   10   90   56  679 1150\n",
      "  693  283  267  957  579  963  576  468  195   36  712  535  831  791\n",
      "  496  911  312  257  510  720  340  446   33  728  187  933 1009  322\n",
      "  970  850  961 1021  773 1109  736  790  771  208 1144 1031  116  991\n",
      "  542  470  688  407  746  467  511  967  681  974  338  538  549  316\n",
      "  180  956]\n",
      "u = \n",
      "[1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165\n",
      " 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179\n",
      " 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193\n",
      " 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207\n",
      " 1208 1209 1210 1211 1212 1213 1214 1215]\n",
      "batch = \n",
      "[ 760 1163    8  199  865  745  296  431  498  204 1070   24   30 1107\n",
      "  977  714  663  920  303  397  494  365  262  337  413  900  250  159\n",
      "  975  279 1152  862  702  161  348  594  932   29   35  254  730  795\n",
      "  713  805  133 1104  460  639  429  762  317  442  812  531  458  898\n",
      " 1145  786  146   37  286  565  327  493]\n",
      "***************************batches = ********************************\n",
      "[[540, 941, 282, 751, 151, 487, 300, 438, 399, 930, 573, 193, 595, 1043, 891, 99, 888, 360, 1067, 987, 495, 406, 160, 984, 77, 1111, 986, 761, 777, 7, 921, 69, 1072, 768, 1028, 592, 855, 275, 785, 1050, 465, 437, 343, 261, 366, 695, 92, 627, 660, 683, 276, 382, 971, 965, 421, 140, 48, 633, 577, 127, 937, 1095, 229, 818, 18, 235, 368, 581, 1076, 203, 545, 690, 586, 894, 126, 943, 625, 1137, 88, 719, 948, 25, 962, 629, 1056, 729, 236, 810, 981, 176, 530, 455, 583, 207, 144, 1016, 152, 787, 135, 103, 521, 277, 306, 310, 108, 479, 319, 1078, 389, 1117, 391, 264, 21, 42, 1061, 907, 302, 863, 221, 1030, 91, 414, 707, 1093, 1101, 473, 555, 908], [500, 1185, 842, 1142, 507, 432, 461, 844, 214, 439, 1180, 944, 641, 1083, 383, 245, 435, 875, 587, 1200, 71, 192, 917, 485, 19, 488, 717, 607, 115, 667, 1020, 445, 680, 492, 433, 1211, 263, 551, 122, 211, 852, 174, 1199, 513, 504, 89, 869, 464, 426, 926, 225, 733, 422, 1122, 884, 288, 895, 619, 1201, 602, 546, 1099, 385, 293, 860, 100, 284, 423, 827, 121, 357, 744, 448, 1136, 404, 1178, 390, 854, 11, 1073, 828, 552, 899, 1139, 750, 1151, 280, 647, 881, 369, 570, 1005, 916, 205, 512, 80, 901, 15, 59, 770, 482, 1175, 137, 514, 260, 47, 637, 952, 608, 402, 3, 320, 331, 834, 753, 114, 796, 778, 1008, 255, 1176, 523, 1001, 919, 893, 200, 332, 462], [697, 281, 1160, 361, 1068, 252, 757, 424, 1129, 232, 995, 758, 703, 661, 885, 849, 599, 662, 960, 78, 292, 669, 97, 183, 202, 45, 731, 43, 124, 459, 51, 924, 949, 1186, 1060, 867, 631, 428, 22, 16, 238, 743, 273, 655, 670, 328, 582, 557, 489, 1052, 234, 1084, 301, 247, 1102, 883, 483, 666, 1103, 172, 145, 83, 1089, 1086, 912, 783, 179, 835, 528, 1209, 837, 453, 580, 515, 556, 832, 398, 443, 75, 1108, 329, 181, 740, 913, 1123, 1110, 1114, 560, 70, 111, 1074, 896, 294, 472, 561, 889, 598, 953, 1154, 699, 569, 682, 659, 701, 628, 600, 1177, 129, 915, 1192, 1148, 1167, 411, 1011, 157, 872, 377, 272, 440, 60, 291, 64, 1146, 1100, 676, 1140, 658, 572], [67, 605, 325, 506, 710, 376, 415, 330, 536, 314, 1071, 307, 969, 938, 477, 447, 58, 450, 847, 508, 527, 1162, 716, 189, 588, 1094, 117, 309, 471, 57, 533, 897, 359, 673, 68, 1125, 767, 808, 845, 1097, 96, 524, 1193, 1215, 265, 585, 17, 671, 345, 1075, 1092, 194, 44, 610, 34, 789, 809, 31, 978, 227, 1189, 618, 1173, 499, 209, 749, 0, 643, 634, 955, 1057, 626, 798, 914, 566, 983, 243, 616, 841, 589, 76, 392, 61, 244, 772, 648, 1147, 251, 256, 456, 93, 1, 198, 931, 52, 452, 653, 1047, 711, 125, 50, 14, 640, 148, 864, 871, 375, 412, 1179, 349, 1214, 1014, 858, 98, 715, 1040, 741, 584, 1115, 1055, 1202, 321, 876, 454, 597, 723, 830, 700], [910, 308, 620, 1183, 53, 708, 105, 141, 509, 373, 804, 153, 905, 1197, 964, 324, 136, 381, 66, 824, 642, 6, 230, 178, 769, 780, 578, 297, 334, 370, 996, 217, 550, 73, 1081, 737, 840, 833, 727, 1168, 1135, 416, 1190, 502, 1053, 522, 215, 1049, 72, 857, 1041, 621, 1037, 959, 1091, 173, 553, 725, 425, 384, 497, 982, 269, 718, 248, 5, 1195, 197, 326, 1054, 1034, 548, 346, 668, 305, 481, 942, 372, 298, 754, 210, 694, 474, 1166, 132, 1128, 466, 1210, 1069, 945, 739, 1204, 814, 394, 1161, 344, 684, 1010, 420, 882, 242, 1062, 1121, 856, 1051, 1155, 206, 62, 564, 665, 692, 886, 350, 1035, 1174, 40, 1106, 1188, 233, 990, 102, 110, 902, 1132, 374, 649, 106, 54], [1004, 228, 534, 1032, 218, 55, 241, 636, 1120, 651, 476, 1096, 418, 797, 299, 612, 304, 630, 792, 1171, 335, 175, 32, 1141, 874, 223, 1133, 1196, 958, 79, 478, 201, 1198, 788, 803, 1212, 817, 405, 396, 1164, 988, 313, 806, 936, 400, 839, 449, 1126, 567, 258, 417, 1153, 735, 63, 686, 395, 635, 362, 596, 85, 1156, 191, 815, 480, 285, 1170, 1207, 999, 614, 568, 622, 877, 1187, 491, 81, 1182, 823, 347, 196, 822, 1000, 430, 807, 1015, 836, 259, 946, 838, 219, 811, 401, 1003, 851, 341, 1143, 611, 1213, 363, 1169, 604, 859, 892, 1159, 922, 1105, 742, 1181, 776, 1203, 906, 278, 755, 781, 469, 434, 726, 516, 691, 188, 409, 82, 571, 351, 147, 979, 764, 591, 27], [155, 463, 870, 249, 94, 968, 113, 705, 222, 1206, 918, 163, 1063, 821, 246, 289, 1002, 574, 156, 1113, 46, 4, 829, 903, 290, 355, 652, 486, 993, 801, 107, 1025, 190, 624, 142, 1119, 738, 563, 134, 748, 928, 28, 734, 239, 1042, 654, 672, 1048, 998, 168, 799, 623, 387, 848, 813, 371, 650, 41, 65, 929, 1158, 559, 1065, 615, 689, 1131, 775, 143, 118, 674, 112, 1036, 212, 747, 696, 1184, 139, 934, 774, 724, 765, 518, 520, 138, 543, 1138, 128, 1039, 939, 601, 38, 843, 1079, 1087, 182, 165, 547, 593, 271, 295, 722, 1208, 154, 169, 457, 923, 86, 1038, 240, 268, 358, 1026, 323, 9, 270, 1118, 721, 645, 403, 606, 186, 444, 353, 1082, 171, 973, 558, 441], [1007, 20, 95, 1024, 846, 1064, 475, 177, 166, 101, 644, 39, 1022, 388, 490, 1149, 123, 554, 678, 687, 356, 541, 935, 1112, 266, 1165, 904, 1059, 184, 613, 887, 231, 352, 149, 1085, 410, 617, 1134, 1019, 706, 213, 336, 940, 575, 237, 505, 873, 793, 104, 603, 1046, 1116, 119, 544, 287, 354, 1023, 704, 274, 1006, 1017, 1172, 333, 879, 656, 525, 1157, 677, 162, 820, 976, 386, 1098, 23, 253, 451, 989, 1045, 2, 997, 950, 763, 994, 532, 609, 1029, 484, 853, 519, 675, 170, 378, 1044, 966, 408, 130, 816, 954, 819, 427, 87, 698, 794, 590, 109, 632, 380, 909, 866, 1066, 951, 379, 74, 526, 1130, 766, 1124, 120, 436, 364, 339, 393, 150, 517, 1090, 709, 1058, 12], [539, 800, 784, 315, 890, 1012, 1080, 657, 419, 26, 226, 782, 878, 825, 992, 529, 985, 367, 1088, 1194, 927, 1018, 752, 562, 537, 664, 1205, 646, 167, 220, 185, 756, 947, 732, 49, 861, 131, 972, 925, 84, 826, 1191, 1127, 638, 685, 868, 880, 759, 1013, 158, 779, 311, 1033, 216, 501, 224, 1027, 318, 13, 802, 342, 1077, 980, 503, 164, 10, 90, 56, 679, 1150, 693, 283, 267, 957, 579, 963, 576, 468, 195, 36, 712, 535, 831, 791, 496, 911, 312, 257, 510, 720, 340, 446, 33, 728, 187, 933, 1009, 322, 970, 850, 961, 1021, 773, 1109, 736, 790, 771, 208, 1144, 1031, 116, 991, 542, 470, 688, 407, 746, 467, 511, 967, 681, 974, 338, 538, 549, 316, 180, 956]]\n",
      "*********************************************************************\n",
      "new_indices = \n",
      "[540, 941, 282, 751, 151, 487, 300, 438, 399, 930, 573, 193, 595, 1043, 891, 99, 888, 360, 1067, 987, 495, 406, 160, 984, 77, 1111, 986, 761, 777, 7, 921, 69, 1072, 768, 1028, 592, 855, 275, 785, 1050, 465, 437, 343, 261, 366, 695, 92, 627, 660, 683, 276, 382, 971, 965, 421, 140, 48, 633, 577, 127, 937, 1095, 229, 818, 18, 235, 368, 581, 1076, 203, 545, 690, 586, 894, 126, 943, 625, 1137, 88, 719, 948, 25, 962, 629, 1056, 729, 236, 810, 981, 176, 530, 455, 583, 207, 144, 1016, 152, 787, 135, 103, 521, 277, 306, 310, 108, 479, 319, 1078, 389, 1117, 391, 264, 21, 42, 1061, 907, 302, 863, 221, 1030, 91, 414, 707, 1093, 1101, 473, 555, 908, 500, 1185, 842, 1142, 507, 432, 461, 844, 214, 439, 1180, 944, 641, 1083, 383, 245, 435, 875, 587, 1200, 71, 192, 917, 485, 19, 488, 717, 607, 115, 667, 1020, 445, 680, 492, 433, 1211, 263, 551, 122, 211, 852, 174, 1199, 513, 504, 89, 869, 464, 426, 926, 225, 733, 422, 1122, 884, 288, 895, 619, 1201, 602, 546, 1099, 385, 293, 860, 100, 284, 423, 827, 121, 357, 744, 448, 1136, 404, 1178, 390, 854, 11, 1073, 828, 552, 899, 1139, 750, 1151, 280, 647, 881, 369, 570, 1005, 916, 205, 512, 80, 901, 15, 59, 770, 482, 1175, 137, 514, 260, 47, 637, 952, 608, 402, 3, 320, 331, 834, 753, 114, 796, 778, 1008, 255, 1176, 523, 1001, 919, 893, 200, 332, 462, 697, 281, 1160, 361, 1068, 252, 757, 424, 1129, 232, 995, 758, 703, 661, 885, 849, 599, 662, 960, 78, 292, 669, 97, 183, 202, 45, 731, 43, 124, 459, 51, 924, 949, 1186, 1060, 867, 631, 428, 22, 16, 238, 743, 273, 655, 670, 328, 582, 557, 489, 1052, 234, 1084, 301, 247, 1102, 883, 483, 666, 1103, 172, 145, 83, 1089, 1086, 912, 783, 179, 835, 528, 1209, 837, 453, 580, 515, 556, 832, 398, 443, 75, 1108, 329, 181, 740, 913, 1123, 1110, 1114, 560, 70, 111, 1074, 896, 294, 472, 561, 889, 598, 953, 1154, 699, 569, 682, 659, 701, 628, 600, 1177, 129, 915, 1192, 1148, 1167, 411, 1011, 157, 872, 377, 272, 440, 60, 291, 64, 1146, 1100, 676, 1140, 658, 572, 67, 605, 325, 506, 710, 376, 415, 330, 536, 314, 1071, 307, 969, 938, 477, 447, 58, 450, 847, 508, 527, 1162, 716, 189, 588, 1094, 117, 309, 471, 57, 533, 897, 359, 673, 68, 1125, 767, 808, 845, 1097, 96, 524, 1193, 1215, 265, 585, 17, 671, 345, 1075, 1092, 194, 44, 610, 34, 789, 809, 31, 978, 227, 1189, 618, 1173, 499, 209, 749, 0, 643, 634, 955, 1057, 626, 798, 914, 566, 983, 243, 616, 841, 589, 76, 392, 61, 244, 772, 648, 1147, 251, 256, 456, 93, 1, 198, 931, 52, 452, 653, 1047, 711, 125, 50, 14, 640, 148, 864, 871, 375, 412, 1179, 349, 1214, 1014, 858, 98, 715, 1040, 741, 584, 1115, 1055, 1202, 321, 876, 454, 597, 723, 830, 700, 910, 308, 620, 1183, 53, 708, 105, 141, 509, 373, 804, 153, 905, 1197, 964, 324, 136, 381, 66, 824, 642, 6, 230, 178, 769, 780, 578, 297, 334, 370, 996, 217, 550, 73, 1081, 737, 840, 833, 727, 1168, 1135, 416, 1190, 502, 1053, 522, 215, 1049, 72, 857, 1041, 621, 1037, 959, 1091, 173, 553, 725, 425, 384, 497, 982, 269, 718, 248, 5, 1195, 197, 326, 1054, 1034, 548, 346, 668, 305, 481, 942, 372, 298, 754, 210, 694, 474, 1166, 132, 1128, 466, 1210, 1069, 945, 739, 1204, 814, 394, 1161, 344, 684, 1010, 420, 882, 242, 1062, 1121, 856, 1051, 1155, 206, 62, 564, 665, 692, 886, 350, 1035, 1174, 40, 1106, 1188, 233, 990, 102, 110, 902, 1132, 374, 649, 106, 54, 1004, 228, 534, 1032, 218, 55, 241, 636, 1120, 651, 476, 1096, 418, 797, 299, 612, 304, 630, 792, 1171, 335, 175, 32, 1141, 874, 223, 1133, 1196, 958, 79, 478, 201, 1198, 788, 803, 1212, 817, 405, 396, 1164, 988, 313, 806, 936, 400, 839, 449, 1126, 567, 258, 417, 1153, 735, 63, 686, 395, 635, 362, 596, 85, 1156, 191, 815, 480, 285, 1170, 1207, 999, 614, 568, 622, 877, 1187, 491, 81, 1182, 823, 347, 196, 822, 1000, 430, 807, 1015, 836, 259, 946, 838, 219, 811, 401, 1003, 851, 341, 1143, 611, 1213, 363, 1169, 604, 859, 892, 1159, 922, 1105, 742, 1181, 776, 1203, 906, 278, 755, 781, 469, 434, 726, 516, 691, 188, 409, 82, 571, 351, 147, 979, 764, 591, 27, 155, 463, 870, 249, 94, 968, 113, 705, 222, 1206, 918, 163, 1063, 821, 246, 289, 1002, 574, 156, 1113, 46, 4, 829, 903, 290, 355, 652, 486, 993, 801, 107, 1025, 190, 624, 142, 1119, 738, 563, 134, 748, 928, 28, 734, 239, 1042, 654, 672, 1048, 998, 168, 799, 623, 387, 848, 813, 371, 650, 41, 65, 929, 1158, 559, 1065, 615, 689, 1131, 775, 143, 118, 674, 112, 1036, 212, 747, 696, 1184, 139, 934, 774, 724, 765, 518, 520, 138, 543, 1138, 128, 1039, 939, 601, 38, 843, 1079, 1087, 182, 165, 547, 593, 271, 295, 722, 1208, 154, 169, 457, 923, 86, 1038, 240, 268, 358, 1026, 323, 9, 270, 1118, 721, 645, 403, 606, 186, 444, 353, 1082, 171, 973, 558, 441, 1007, 20, 95, 1024, 846, 1064, 475, 177, 166, 101, 644, 39, 1022, 388, 490, 1149, 123, 554, 678, 687, 356, 541, 935, 1112, 266, 1165, 904, 1059, 184, 613, 887, 231, 352, 149, 1085, 410, 617, 1134, 1019, 706, 213, 336, 940, 575, 237, 505, 873, 793, 104, 603, 1046, 1116, 119, 544, 287, 354, 1023, 704, 274, 1006, 1017, 1172, 333, 879, 656, 525, 1157, 677, 162, 820, 976, 386, 1098, 23, 253, 451, 989, 1045, 2, 997, 950, 763, 994, 532, 609, 1029, 484, 853, 519, 675, 170, 378, 1044, 966, 408, 130, 816, 954, 819, 427, 87, 698, 794, 590, 109, 632, 380, 909, 866, 1066, 951, 379, 74, 526, 1130, 766, 1124, 120, 436, 364, 339, 393, 150, 517, 1090, 709, 1058, 12, 539, 800, 784, 315, 890, 1012, 1080, 657, 419, 26, 226, 782, 878, 825, 992, 529, 985, 367, 1088, 1194, 927, 1018, 752, 562, 537, 664, 1205, 646, 167, 220, 185, 756, 947, 732, 49, 861, 131, 972, 925, 84, 826, 1191, 1127, 638, 685, 868, 880, 759, 1013, 158, 779, 311, 1033, 216, 501, 224, 1027, 318, 13, 802, 342, 1077, 980, 503, 164, 10, 90, 56, 679, 1150, 693, 283, 267, 957, 579, 963, 576, 468, 195, 36, 712, 535, 831, 791, 496, 911, 312, 257, 510, 720, 340, 446, 33, 728, 187, 933, 1009, 322, 970, 850, 961, 1021, 773, 1109, 736, 790, 771, 208, 1144, 1031, 116, 991, 542, 470, 688, 407, 746, 467, 511, 967, 681, 974, 338, 538, 549, 316, 180, 956]\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!new_indices = !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "1216\n",
      "#############################indices = ##################################\n",
      "[   0    1    2 ... 1213 1214 1215]\n",
      "###############\n",
      "!!!!!!!!!!!!!!!!!!!!!self.index = !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[540 941 282 ... 565 327 493]\n",
      "data loader\n",
      "embedding_matrix = \n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.50497133  0.31065521 -0.14664398 ...  0.41370535  1.15226185\n",
      "  -0.40046465]\n",
      " [ 0.0231      0.017       0.0157     ...  0.0744     -0.1118\n",
      "   0.0963    ]\n",
      " ...\n",
      " [ 0.15880001  0.0234      0.1594     ...  0.1435     -0.46110001\n",
      "   0.22759999]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "LstmModel __init__\n",
      "LstmUnit __init__\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_matrix.shape1 = \n",
      "300\n",
      "LstmUnit __init__\n",
      "embedding_matrix.shape1 = \n",
      "300\n",
      "eval = \n",
      "TextDataset __len__\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EMA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2950bd1129d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mema_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mupdates_per_epoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mema_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EMA' is not defined"
     ]
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "cv_scores = []\n",
    "ema_cv_scores = []\n",
    "fold_scores = []\n",
    "train_preds = np.zeros((len(train_x), num_targets))\n",
    "#train_preds = (6079,30)\n",
    "test_preds = np.zeros((len(test_x), num_targets))\n",
    "#test_preds = (476,30)\n",
    "ema_train_preds = np.zeros((len(train_x), num_targets))\n",
    "#ema_train_preds = (6079,30)\n",
    "ema_test_preds = np.zeros((len(test_x), num_targets))\n",
    "#ema_test_preds = (476,30)\n",
    "\n",
    "#输入输出的内容太多的时候，将对应的打印内容提取到一个相应的文件上\n",
    "#然后一句一句分析其中相应的内容\n",
    "for i, (train_idx, valid_idx) in enumerate(list(gkf.split(train_x, train_y, train_group))):\n",
    "    #train_x = (6079,2):对应内容为问句+答句，train_y = (6079,30):30个内容的标签\n",
    "    #gkf.split(train_x,train_y,train_group):仅将训练集进行划分\n",
    "    #train_group:Always ignored,exists for compatibility\n",
    "    print(f'fold {i + 1}')\n",
    "    #(train_idx,valid_idx) = (train_x,train_y)\n",
    "    #打印出fold {i+1} = fold {0+1} = fold {1}\n",
    "    train_fold_x, train_fold_y = train_x[train_idx], train_y[train_idx]\n",
    "    valid_fold_x, valid_fold_y = train_x[valid_idx], train_y[valid_idx]\n",
    "    #train_x = np.array([train_q,train_a]).T,train_x = (6079,2),对应一个问句和一个答句\n",
    "    #这里切出来的train_idx为训练集，valid_idx为验证集，train_y = (6079,30)\n",
    "    \n",
    "    train_fold_emb = train_text_emb[train_idx]\n",
    "    valid_fold_emb = train_text_emb[valid_idx]\n",
    "    #train_fold_emb = train_text_emb[train_idx]\n",
    "    #valid_fold_emb = train_text_emb[valid_idx]\n",
    "    print('train_fold_emb = ')\n",
    "    print(train_fold_emb.shape)\n",
    "    #train_fold_emb = [4863,1024]\n",
    "    print('valid_fold_emb = ')\n",
    "    print(valid_fold_emb.shape)\n",
    "    #train_fold_emb = [4863,1024]\n",
    "    #valid_fold_emb = [1216,1024]\n",
    "    #问句和答句融合在一起的对应的内容\n",
    "    train_dataset = TextDataset(train_fold_x, train_fold_y)\n",
    "    #这里面TextDataset初始化定义为def __init__(self, seqs, targets=None):\n",
    "    #如果同时出现问句和答句的话，使用TextDataset将问句和答句的长度加在一起\n",
    "    #构成相应的数组\n",
    "    valid_dataset = TextDataset(valid_fold_x)\n",
    "    #如果只出现一个句子的话，使用TextDataset算出相应的valid_fold_x的内容\n",
    "    #选出的对应的验证集\n",
    "    \n",
    "    #TextDataset为上面定义的一个继承的\n",
    "    #这里面train_dataset.get_keys()的对应值为\n",
    "    #valid_dataset.get_keys()的对应值为\n",
    "    #[ 253  226  203  876  211  117  689  320  466  111  724  101  593  175\n",
    "    #257  684  560  190  365  219  111  541  120  131   92  368  709  347\n",
    "    # 349  433  292  817  333  149  299  547  174  714  266  206  305  723\n",
    "    #579  650  817  300  241  305  613  162  236  240  765  495  452  151\n",
    "    #103  983  285  424  187  201   95  183  256  262  171  622  148  216\n",
    "    #265  310 1011  273  167  121  339  232  179  196  261  429  167  294\n",
    "    #70  383  615  435  211  225  302  138   70  310   70  725  148  628\n",
    "    #139  441  965  491  197  169  233   72  213  134  400  294  559  182\n",
    "    #514  145  183  738  553  416  389  171  347  286  732  372   82  712\n",
    "    #87  366  110  425  126  502 1024  386  224  308  567  253  608  405\n",
    "    #582  578  365  345   68  310 1024  344  112  938  362  297  588  125\n",
    "    #它这个操作是先从中抽取出句子，然后使用get_keys()方法获得问句和答句的加和矩阵\n",
    "    \n",
    "    #针对valid_dataset数据操作的时候valid_dataset = TextDataset(valid_fold_x)\n",
    "    #此时TextDataset之中只有对应的一个数值，所以内容就是valid_fold_x的长度\n",
    "    print('sampler = ')\n",
    "    train_sampler = BucketSampler(train_dataset, train_dataset.get_keys(),\n",
    "                                  bucket_size=batch_size * 20, batch_size=batch_size)\n",
    "    #bucket_size = 128*20 = 2560,batch_size = 128\n",
    "    valid_sampler = BucketSampler(valid_dataset, valid_dataset.get_keys(),\n",
    "                                  batch_size=batch_size, shuffle_data=False)\n",
    "    #batch_size = 128,\n",
    "    print('data loader')\n",
    "    #BucketSampler对应的初始化类型为\n",
    "    #def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=1048, shuffle_data=True):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              sampler=train_sampler, num_workers=0, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              sampler=valid_sampler, collate_fn=collate_fn)\n",
    "    #这里DataLoader没有调用任何内部函数，只是将相应的数值取出来，\n",
    "    print('embedding_matrix = ')\n",
    "    print(embedding_matrix)\n",
    "    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "    #embedding_matrix = (47462,300),这里的embedding_matrix为之前使用\n",
    "    #fasttext.pkl+w2vec训练得到的最终结果\n",
    "    model = LstmModel(embedding_matrix).to(device)\n",
    "    #初始化在这里调用LstmModel(embedding_matrix)的初始化函数\n",
    "    model.zero_grad()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    ema_model = copy.deepcopy(model)\n",
    "    #copy浅拷贝，没有拷贝子对象，所以原始数据改变，子对象会改变\n",
    "    #deepcopy深拷贝，包含对象里面的子对象的拷贝，所以原始对象的改变\n",
    "    #不会造成深拷贝里面任何子元素的改变\n",
    "    print('eval = ')\n",
    "    ema_model.eval()\n",
    "    \n",
    "    ema_n = int(len(train_loader.dataset) / (updates_per_epoch * batch_size))\n",
    "    ema = EMA(model, mu, n=ema_n)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        for index, x_batch, y_batch in train_loader:\n",
    "            x_batch = (x.to(device) for x in x_batch)\n",
    "            y_batch = y_batch.to(device)\n",
    "            emb_batch = train_fold_emb[list(index)].to(device)\n",
    "            y_preds = model(*x_batch, emb_batch)\n",
    "            \n",
    "            loss = nn.BCEWithLogitsLoss()(y_preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            ema.on_batch_end(model)\n",
    "        \n",
    "        valid_preds = predict(model, valid_loader, valid_fold_emb, device=device)\n",
    "        scores = get_scores(valid_fold_y, valid_preds)\n",
    "        score = np.mean(list(scores.values()))\n",
    "        fold_scores.append({\n",
    "            'fold': i + 1,\n",
    "            'epoch': epoch + 1,\n",
    "            'score': score\n",
    "        })\n",
    "        elapsed_time = time.time() - epoch_start_time\n",
    "        print('Epoch {}/{} \\t score: {:.4f} \\t time: {:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, score, elapsed_time))\n",
    "        ema.on_epoch_end(model)\n",
    "        \n",
    "    ema.set_weights(ema_model)\n",
    "    ema_model.flatten_parameters()\n",
    "    \n",
    "    train_preds[valid_idx] = valid_preds\n",
    "    ema_valid_preds = predict(ema_model, valid_loader, valid_fold_emb, device=device)\n",
    "    ema_train_preds[valid_idx] = ema_valid_preds\n",
    "\n",
    "    cv_scores.append(score)\n",
    "    ema_scores = get_scores(valid_fold_y, ema_valid_preds)\n",
    "    ema_score = np.mean(list(ema_scores.values()))\n",
    "    print(f'EMA score: {ema_score:.4f}')\n",
    "    ema_cv_scores.append(ema_scores)\n",
    "    \n",
    "    test_preds += predict(model, test_loader, test_text_emb, device=device) / n_splits\n",
    "    ema_test_preds += predict(ema_model, test_loader, test_text_emb, device=device) / n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(pd.DataFrame(fold_scores), x='epoch', y='score', color='fold')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_cv_scores = pd.DataFrame(ema_cv_scores).mean().reset_index()\n",
    "ema_cv_scores.columns = ['target_name', 'score']\n",
    "ema_cv_scores.sort_values('score', inplace=True)\n",
    "fig = px.bar(ema_cv_scores, x='score', y='target_name', orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
    "submission[target_names] = ema_test_preds\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = np.mean(cv_scores)\n",
    "ema_score = ema_cv_scores['score'].mean()\n",
    "print(f'CV score: {cv_score:.4f}')\n",
    "print(f'EMA score: {ema_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.threshold = [0., 1.]\n",
    "        self.ab_start = [(0., 0.2), (0.8, 1.)]\n",
    "    \n",
    "    def fit(self, train_labels, train_preds):\n",
    "        assert train_labels.shape == train_preds.shape\n",
    "        assert train_labels.ndim == 1\n",
    "        \n",
    "        self.best_score = self.score(train_labels, train_preds)\n",
    "        self._golden_section_search(train_labels, train_preds, 0)  # lower threshold\n",
    "        score = self.score(train_labels, train_preds)\n",
    "        if score > self.best_score + 1e-3:\n",
    "            self.best_score = score\n",
    "        else:\n",
    "            self.threshold[0] = 0.\n",
    "        \n",
    "        self._golden_section_search(train_labels, train_preds, 1)  # higher threshold\n",
    "        score = self.score(train_labels, train_preds)\n",
    "        if score > self.best_score + 1e-3:\n",
    "            self.best_score = score\n",
    "        else:\n",
    "            self.threshold[1] = 1.\n",
    "\n",
    "    def _golden_section_search(self, train_labels, train_preds, idx):\n",
    "        # idx == 0 -> lower threshold search\n",
    "        # idx == 1 -> higher threshold search\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        for _ in range(10):\n",
    "            a, b = self.ab_start[idx]\n",
    "            # calc losses\n",
    "            self.threshold[idx] = a\n",
    "            la = -self.score(train_labels, train_preds)\n",
    "            self.threshold[idx] = b\n",
    "            lb = -self.score(train_labels, train_preds)\n",
    "            for _ in range(20):\n",
    "                # choose value\n",
    "                if la > lb:\n",
    "                    a = b - (b - a) * golden1\n",
    "                    self.threshold[idx] = a\n",
    "                    la = -self.score(train_labels, train_preds)\n",
    "                else:\n",
    "                    b = b - (b - a) * golden2\n",
    "                    self.threshold[idx] = b\n",
    "                    lb = -self.score(train_labels, train_preds)\n",
    "\n",
    "    def transform(self, preds):\n",
    "        transformed = np.clip(preds, *self.threshold)\n",
    "        if np.unique(transformed).size == 1:\n",
    "            return preds\n",
    "        return transformed\n",
    "        \n",
    "    def score(self, labels, preds):\n",
    "        p = self.transform(preds)\n",
    "        score = scipy.stats.spearmanr(labels, p)[0]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=1029)\n",
    "train_scores = []\n",
    "valid_scores = []\n",
    "train_optimized_scores = defaultdict(list)\n",
    "valid_optimized_scores = defaultdict(list)\n",
    "thresholds = defaultdict(list)\n",
    "\n",
    "for train_idx, valid_idx in kf.split(train_y):\n",
    "    train_fold_preds, train_fold_y = ema_train_preds[train_idx], train_y[train_idx]\n",
    "    valid_fold_preds, valid_fold_y = ema_train_preds[valid_idx], train_y[valid_idx]\n",
    "    train_scores.append(get_scores(train_fold_y, train_fold_preds))\n",
    "    valid_scores.append(get_scores(valid_fold_y, valid_fold_preds))\n",
    "    \n",
    "    for i, target_name in enumerate(target_names):\n",
    "        optimizer = OptimizedRounder()\n",
    "        optimizer.fit(train_y[train_idx, i], ema_train_preds[train_idx, i])\n",
    "        train_score = optimizer.score(train_y[train_idx, i], ema_train_preds[train_idx, i])\n",
    "        valid_score = optimizer.score(train_y[valid_idx, i], ema_train_preds[valid_idx, i])\n",
    "        train_optimized_scores[target_name].append(train_score)\n",
    "        valid_optimized_scores[target_name].append(valid_score)\n",
    "        thresholds[target_name].append(optimizer.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(valid_optimized_scores).mean().loc[ema_cv_scores['target_name']].reset_index()\n",
    "scores.columns = ['target_name', 'score']\n",
    "fig = px.bar(scores, x='score', y='target_name', orientation='h')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = pd.DataFrame(train_scores).values.mean()\n",
    "valid_score = pd.DataFrame(valid_scores).values.mean()\n",
    "train_optimized_score = np.mean(list(train_optimized_scores.values()))\n",
    "valid_optimized_score = np.mean(list(valid_optimized_scores.values()))\n",
    "print(f'train score: {train_score:.4f} -> {train_optimized_score:.4f}')\n",
    "print(f'valid score: {valid_score:.4f} -> {valid_optimized_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, target_name in enumerate(target_names):\n",
    "    optimizer = OptimizedRounder()\n",
    "    optimizer.threshold = np.mean(thresholds[target_name], axis=0)\n",
    "    ema_test_preds[:, i] = optimizer.transform(ema_test_preds[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".submission = pd.read_csv(SAMPLE_SUBMISSION_PATH)\n",
    "submission[target_names] = ema_test_preds\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(f'all processes done in {(time.time() - start_time) / 60:.2f} min.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
