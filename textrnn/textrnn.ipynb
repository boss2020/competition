{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自然语言处理中更常用的是RNN:Recurrent Neural Network,能够更好地表达上下文信息\n",
    "RNN是在自然语言处理中非常标配的一个网络，在序列标注/命名体识别/Seq2seq灯场景中都有应用\n",
    "RNN的对应公式:P(w1,w2...wm)=(连乘(i=1,m))P(wi|w1,...wi-1)\n",
    "这里的每个词对应的概率依赖于前面所有的词对应的概率，与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx = ***\n",
      "{'dog': 0, 'hate': 1, 'love': 2, 'milk': 3, 'like': 4, 'coffee': 5, 'i': 6}\n",
      "idx2word = ***\n",
      "{0: 'dog', 1: 'hate', 2: 'love', 3: 'milk', 4: 'like', 5: 'coffee', 6: 'i'}\n",
      "make_data\n",
      "word = ***\n",
      "['i', 'like', 'dog']\n",
      "input_batch = ***\n",
      "[array([[0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 0., 0.]])]\n",
      "word = ***\n",
      "['i', 'love', 'coffee']\n",
      "input_batch = ***\n",
      "[array([[0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 0., 0.]]), array([[0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0., 0., 0.]])]\n",
      "word = ***\n",
      "['i', 'hate', 'milk']\n",
      "input_batch = ***\n",
      "[array([[0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 0., 0.]]), array([[0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0., 0., 0.]]), array([[0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 0.]])]\n",
      "----------------------------------------------------------------------------\n",
      "input_batch = !!!\n",
      "[array([[0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 0., 0.]]), array([[0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0., 0., 0.]]), array([[0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 1., 0., 0., 0., 0., 0.]])]\n",
      "target_batch = !!!\n",
      "[0, 5, 3]\n",
      "############################################################################\n",
      "input_batch.shape = ***\n",
      "torch.Size([3, 2, 7])\n",
      "target_batch.shape = ***\n",
      "torch.Size([3])\n",
      "dataset 0 = ***\n",
      "(tensor([[0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0.]]), tensor(0))\n",
      "----------------loader = --------------------------\n",
      "img.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "label.shape = ***\n",
      "torch.Size([2])\n",
      "img.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "label.shape = ***\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  code by Tae Hwan Jung(Jeff Jung) @graykode, modify by wmathor\n",
    "'''\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "sentences = [ \"i like dog\", \"i love coffee\", \"i hate milk\"]\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "#word_list=['i','like','dog','i','love','coffee','i','hate','milk']\n",
    "vocab = list(set(word_list))\n",
    "#vocab=['hate', 'love', 'milk', 'like', 'dog', 'coffee', 'i']\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "#word2idx = {'hate': 0, 'love': 1, 'milk': 2, 'like': 3, 'dog': 4, 'coffee': 5, 'i': 6}\n",
    "#!!!注意!!!每次word2idx与对应的单词编码的结果可能会不一样\n",
    "print('word2idx = ***')\n",
    "print(word2idx)\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "#idx2word = {0: 'hate', 1: 'love', 2: 'milk', 3: 'like', 4: 'dog', 5: 'coffee', 6: 'i'}\n",
    "print('idx2word = ***')\n",
    "print(idx2word)\n",
    "n_class = len(vocab)\n",
    "#n_class = 7,因为总共有对应的7个单词\n",
    "# TextRNN Parameter\n",
    "batch_size = 2\n",
    "n_step = 2 # number of cells(= number of Step)\n",
    "n_hidden = 5 # number of hidden units in one cell\n",
    "\n",
    "def make_data(sentences):\n",
    "    print('make_data')\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        #print('sen = ***')\n",
    "        #print(sen)\n",
    "        word = sen.split()\n",
    "        print('word = ***')\n",
    "        print(word)\n",
    "        input = [word2idx[n] for n in word[:-1]]\n",
    "        #print('input = ***')\n",
    "        #print(input)\n",
    "        target = word2idx[word[-1]]\n",
    "        #将前两个单词作为对应的input，最后一个单词\n",
    "        #作为目标对应的target的值，比如'i','like','dog'\n",
    "        #input=[6,3]为'i','like'对应的值，'dog'=[4]为\n",
    "        #target对应的值\n",
    "        #print('target = ***')\n",
    "        #print(target)\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        #这里面n_class的对应值为7，numpy.eye()为生成对角矩阵\n",
    "        #对应的n_class值为7的时候为长度为7的对角矩阵，这里的\n",
    "        #input选择哪个的时候对应的值就为哪个，所以本质上是\n",
    "        #将对应的input的值进行one-hot的编码\n",
    "        print('input_batch = ***')\n",
    "        print(input_batch)\n",
    "        target_batch.append(target)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "input_batch, target_batch = make_data(sentences)\n",
    "print('----------------------------------------------------------------------------')\n",
    "print('input_batch = !!!')\n",
    "print(input_batch)\n",
    "print('target_batch = !!!')\n",
    "print(target_batch)\n",
    "input_batch, target_batch = torch.Tensor(input_batch), torch.LongTensor(target_batch)\n",
    "print('############################################################################')\n",
    "print('input_batch.shape = ***')\n",
    "print(input_batch.shape)\n",
    "print('target_batch.shape = ***')\n",
    "print(target_batch.shape)\n",
    "'''\n",
    "input_batch为one-hot对应的向量组成的相应的数组\n",
    "target_batch为对应target的值\n",
    "'''\n",
    "dataset = Data.TensorDataset(input_batch, target_batch)\n",
    "#TensorDataset:对给定的tensor数据(样本和标签),将它们包装成dataset\n",
    "#注意如果是numpy的array，或者Pandas的DataFrame需要先转换成Tensor\n",
    "\n",
    "#将对应的tensor组装成一个input_batch和target_batch构成的一个整体\n",
    "print('dataset 0 = ***')\n",
    "print(dataset[0])\n",
    "#比如dataset[0]组成的对应的值为\n",
    "#(tensor([[0., 0., 0., 0., 0., 1., 0.],\n",
    "#        [0., 1., 0., 0., 0., 0., 0.]]), tensor(6))\n",
    "loader = Data.DataLoader(dataset, batch_size, True)\n",
    "print('----------------loader = --------------------------')\n",
    "for  img,label  in  loader:\n",
    "    print('img.shape = ***')\n",
    "    print(img.shape)\n",
    "    print('label.shape = ***')\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size的对应值为2，\n",
    "dataset为刚才写的input组成的one-hot编码以及target_batch的tensor组成的\n",
    "对应的结果值\n",
    "DataLoader本质上是一个iterable,与python内置的list类似，并利用多进程\n",
    "来加速batch data，使用yield使用有限的内存\n",
    "\n",
    "DataLoader是一个高效、简洁、直观的网络输入数据结果，\n",
    "使用pytorch将数据加载到模型一般的操作顺序如下：\n",
    "1.创建一个Dataset对象\n",
    "2.创建一个DataLoader对象\n",
    "3.循环这个DataLoader对象，将img，label加载到模型中进行训练\n",
    "\n",
    "比如本例子之中对应的word2idx的对应值如下:\n",
    "{'hate':0,'like':1,'coffee':2,'milk':3,'love':4,'i':5,'dog':6}\n",
    "那么此时对应的dataset的内容为\n",
    "[([5,1],[6]),([5,4],[2]),([5,0],[3])]\n",
    "经过DataLoader之后，对应的loader的内容为\n",
    "[([5,4],[5,0]),([2],[3]),([5,1]),([6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__ TextRnn\n",
      "n_class = 7\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 0\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 2, 5])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 2, 5])\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.2114, -0.2030,  0.6489,  0.7838, -0.4902],\n",
      "        [ 0.2114, -0.2030,  0.6489,  0.7838, -0.4902]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[ 0.2253, -0.3544, -1.0087, -0.7038, -0.2774,  0.3763,  0.4961],\n",
      "        [ 0.2253, -0.3544, -1.0087, -0.7038, -0.2774,  0.3763,  0.4961]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([3, 0])\n",
      "!!!before loss!!!\n",
      "loss = 2.136951\n",
      "loss = ***\n",
      "tensor(2.1370, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 1, 5])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 1, 5])\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 1, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.2085, -0.2059,  0.6471,  0.7826, -0.4879]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[ 0.2297, -0.3560, -1.0114, -0.6983, -0.2794,  0.3736,  0.4913]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([5])\n",
      "!!!before loss!!!\n",
      "loss = 1.523009\n",
      "loss = ***\n",
      "tensor(1.5230, grad_fn=<NllLossBackward>)\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 1\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 2, 5])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 2, 5])\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.2062, -0.2085,  0.6455,  0.7823, -0.4891],\n",
      "        [ 0.2062, -0.2085,  0.6455,  0.7823, -0.4891]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[ 0.2303, -0.3594, -1.0150, -0.6958, -0.2822,  0.3770,  0.4862],\n",
      "        [ 0.2303, -0.3594, -1.0150, -0.6958, -0.2822,  0.3770,  0.4862]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([3, 5])\n",
      "!!!before loss!!!\n",
      "loss = 2.054939\n",
      "loss = ***\n",
      "tensor(2.0549, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 1, 5])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 1, 5])\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 1, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.2037, -0.2108,  0.6437,  0.7817, -0.4904]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[ 0.2296, -0.3627, -1.0183, -0.6924, -0.2849,  0.3807,  0.4808]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([0])\n",
      "!!!before loss!!!\n",
      "loss = 1.664781\n",
      "loss = ***\n",
      "tensor(1.6648, grad_fn=<NllLossBackward>)\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 2\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 2, 5])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 2, 5])\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.2029, -0.2134,  0.6421,  0.7811, -0.4910],\n",
      "        [ 0.2029, -0.2134,  0.6421,  0.7811, -0.4910]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[ 0.2315, -0.3660, -1.0219, -0.6901, -0.2882,  0.3828,  0.4762],\n",
      "        [ 0.2315, -0.3660, -1.0219, -0.6901, -0.2882,  0.3828,  0.4762]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([5, 0])\n",
      "!!!before loss!!!\n",
      "loss = 1.586175\n",
      "loss = ***\n",
      "tensor(1.5862, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 1, 5])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 1, 5])\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 1, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.2022, -0.2160,  0.6405,  0.7805, -0.4919]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[ 0.2337, -0.3694, -1.0256, -0.6884, -0.2916,  0.3853,  0.4715]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([3])\n",
      "!!!before loss!!!\n",
      "loss = 2.580755\n",
      "loss = ***\n",
      "tensor(2.5808, grad_fn=<NllLossBackward>)\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 3\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 2, 5])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 2, 5])\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.2010, -0.2179,  0.6388,  0.7797, -0.4924],\n",
      "        [ 0.2010, -0.2179,  0.6388,  0.7797, -0.4924]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[ 0.2351, -0.3725, -1.0288, -0.6855, -0.2947,  0.3870,  0.4668],\n",
      "        [ 0.2351, -0.3725, -1.0288, -0.6855, -0.2947,  0.3870,  0.4668]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([0, 3])\n",
      "!!!before loss!!!\n",
      "loss = 2.116451\n",
      "loss = ***\n",
      "tensor(2.1165, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 1, 5])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 1, 5])\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 1, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.1998, -0.2197,  0.6371,  0.7788, -0.4925]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[ 0.2370, -0.3754, -1.0319, -0.6823, -0.2978,  0.3880,  0.4622]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([5])\n",
      "!!!before loss!!!\n",
      "loss = 1.502113\n",
      "loss = ***\n",
      "tensor(1.5021, grad_fn=<NllLossBackward>)\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "epoch = 4\n",
      "x = ***\n",
      "torch.Size([2, 2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 2, 5])\n",
      "x.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 2, 5])\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 2, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 2, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 2, 5])\n",
      "out = ###\n",
      "tensor([[ 0.1983, -0.2218,  0.6355,  0.7782, -0.4932],\n",
      "        [ 0.1983, -0.2218,  0.6355,  0.7782, -0.4932]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([2, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([2, 7])\n",
      "pred = ***\n",
      "torch.Size([2, 7])\n",
      "y = ***\n",
      "torch.Size([2])\n",
      "pred = ###\n",
      "tensor([[ 0.2381, -0.3786, -1.0352, -0.6797, -0.3008,  0.3904,  0.4573],\n",
      "        [ 0.2381, -0.3786, -1.0352, -0.6797, -0.3008,  0.3904,  0.4573]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([3, 5])\n",
      "!!!before loss!!!\n",
      "loss = 2.033726\n",
      "loss = ***\n",
      "tensor(2.0337, grad_fn=<NllLossBackward>)\n",
      "x = ***\n",
      "torch.Size([1, 2, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "n_hidden = ***\n",
      "5\n",
      "hidden = ***\n",
      "torch.Size([1, 1, 5])\n",
      "x.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 1, 5])\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X.shape = ***\n",
      "torch.Size([1, 2, 7])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X.shape = ***\n",
      "torch.Size([2, 1, 7])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 1, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 1, 5])\n",
      "out = ###\n",
      "tensor([[ 0.1965, -0.2238,  0.6338,  0.7775, -0.4941]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([1, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([1, 7])\n",
      "pred = ***\n",
      "torch.Size([1, 7])\n",
      "y = ***\n",
      "torch.Size([1])\n",
      "pred = ###\n",
      "tensor([[ 0.2387, -0.3818, -1.0384, -0.6767, -0.3037,  0.3930,  0.4524]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "y = ###\n",
      "tensor([0])\n",
      "!!!before loss!!!\n",
      "loss = 1.649310\n",
      "loss = ***\n",
      "tensor(1.6493, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN, self).__init__()\n",
    "        print('__init__ TextRnn')\n",
    "        print('n_class = %d'%n_class)\n",
    "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden)\n",
    "        #input_size:输入特征的维度，hidden_size:隐藏层神经元的个数\n",
    "        #这里体现了使用RNN对文本进行操作分类\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "        #乘上对应的n_hidden*n_class的对应的矩阵\n",
    "\n",
    "    def forward(self, hidden, X):\n",
    "        print('hidden.shape = ***')\n",
    "        print(hidden.shape)\n",
    "        print('X.shape = ***')\n",
    "        print(X.shape)\n",
    "        print('forward = ***')\n",
    "        # X: [batch_size, n_step, n_class]\n",
    "        print('before transpose')\n",
    "        print('X.shape = ***')\n",
    "        print(X.shape)\n",
    "        print('type X = ***')\n",
    "        print(type(X))\n",
    "        #变换之前X的对应的矩阵为\n",
    "        #[[[0,0,0,0,1,0,0],\n",
    "        #  [0,0,0,0,0,0,1]],\n",
    "        #  [0,0,0,0,1,0,0],\n",
    "        #  [0,0,0,1,0,0,0]]]\n",
    "        X = X.transpose(0, 1) # X : [n_step, batch_size, n_class]\n",
    "        #pytorch中的transpose方法的作用是交换矩阵的两个维度\n",
    "        \n",
    "        #transpose中只有两个参数，torch.transpose中有三个参数\n",
    "        print('after transpose')\n",
    "        print('X.shape = ***')\n",
    "        print(X.shape)\n",
    "        #变换之后X对应的矩阵为\n",
    "        #[[[0,0,0,0,1,0,0],\n",
    "        #  [0,0,0,0,1,0,0],\n",
    "        #  [0,0,0,0,0,0,1],\n",
    "        #  [0,0,0,1,0,0,0]]]\n",
    "        print('hidden = ***')\n",
    "        print(hidden)\n",
    "        print('self.rnn = ***')\n",
    "        out, hidden = self.rnn(X, hidden)\n",
    "        #对应的self.rnn,这里使用nn.rnn(input_size,hidden_size)\n",
    "        #其中input_size为输入特征的维度，hidden_size为隐藏层神经元的\n",
    "        #对应的个数，或者也叫输出的维度\n",
    "        \n",
    "        #比如这里定义了rnn_layer = nn.RNN(input_size=1027,hidden_size=256)\n",
    "        #然后这里面使用self.runn(X,5)(隐藏层对应的个数为5)\n",
    "        #输入的X为对应的([35,2,1027])的对应的矩阵，输出的Y为对应的\n",
    "        #[35,2,256]的对应的矩阵，\n",
    "        \n",
    "        # out : [n_step, batch_size, num_directions(=1) * n_hidden]\n",
    "        # hidden : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        print('!!!out.shape = !!!')\n",
    "        print(out.shape)\n",
    "        print('!!!hidden.shape = !!!')\n",
    "        print(hidden.shape)\n",
    "        #原先的out对应的矩阵内容为\n",
    "        #out = tensor([[[-0.3484, -0.1260, -0.2226, -0.0077, -0.4533],\n",
    "        #[-0.3484, -0.1260, -0.2226, -0.0077, -0.4533]],\n",
    "        #[[-0.5938,  0.3629, -0.2776,  0.2037, -0.1790],\n",
    "        #[-0.7574, -0.4174, -0.5710,  0.1534,  0.0740]]],grad_fn=<SelectBackward>)\n",
    "        \n",
    "        out = out[0] # [batch_size, num_directions(=1) * n_hidden] ⭐\n",
    "        #经过out[-1]的结果之后，对应的内容为\n",
    "        #out = tensor([[[-0.5938,  0.3629, -0.2776,  0.2037, -0.1790],\n",
    "        #[-0.7574, -0.4174, -0.5710,  0.1534,  0.0740]]],grad_fn=<SelectBackward>)\n",
    "        #因为总共数组里存在两个对应的内容，所以[-1]选取的是最后面的那个\n",
    "        print('out = ###')\n",
    "        print(out)\n",
    "        print('out.shape = ###')\n",
    "        print(out.shape)\n",
    "        model = self.fc(out)\n",
    "        #这里的定义为self.fc = nn.Linear(n_hidden,n_class)\n",
    "        print('n_hidden = %d,n_class = %d'%(n_hidden,n_class))\n",
    "        print('model = ###')\n",
    "        print(model.shape)\n",
    "        #这里面的内容为n_hidden = 5,n_class = 7,本身输入的内容为\n",
    "        #2*5的对应的内容，所以输出的内容为2*7的对应的内容，因为总共有\n",
    "        #7个类别，需要对应到7个类别\n",
    "        return model\n",
    "\n",
    "model = TextRNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#分类问题的处理一般都使用交叉熵损失函数\n",
    "#对应的公式为H(x)=-(i=1~n的和)P(xi)log(P(xi))\n",
    "#相对熵(KL散度)概念:DKL(p||q) = (i=1~n)p(xi)log(p(xi)/q(xi)),\n",
    "#其中这里的q(xi)代表的含义为原先预测的对应的概率\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "for epoch in range(5):\n",
    "    print('--------------------------------------------------------------------------------------------------------')\n",
    "    print('epoch = %d'%epoch)\n",
    "    for x, y in loader:\n",
    "        print('x = ***')\n",
    "        print(x.shape)\n",
    "        print('y = ***')\n",
    "        print(y.shape)\n",
    "        # hidden : [num_layers * num_directions, batch, hidden_size]\n",
    "        print('n_hidden = ***')\n",
    "        print(n_hidden)\n",
    "        hidden = torch.zeros(1, x.shape[0], n_hidden)\n",
    "        # x : [batch_size, n_step, n_class]\n",
    "        #torch.zeros(*size,*,out=None,dtype=None,layout=torch.strided,\n",
    "        #device=None,requires_grad=False)\n",
    "        #这里的out(Tensor)代表着对应的the  output tensor,\n",
    "        #x.shape[0] = 2,n_hidden = 5,对应的矩阵为1*2*5\n",
    "        #!!!rnn当中的隐藏层的对应输入必须为零矩阵!!!\n",
    "        \n",
    "        #实际形成的是一个1*2*5对应的相应的矩阵\n",
    "        print('hidden = ***')\n",
    "        print(hidden.shape)\n",
    "        #hidden = 5,为上面的对应的隐藏单元\n",
    "        \n",
    "        #这里面调用model里面对应的forward()的相应的函数\n",
    "        print('x.shape = ***')\n",
    "        print(x.shape)\n",
    "        pred = model(hidden, x)\n",
    "        #之前的model = TextRNN()只是定义了相应的参数，\n",
    "        #使用pred = model(hidden,x)直接进入相应的forward函数之中进行运行\n",
    "        print('pred = ***')\n",
    "        print(pred.shape)\n",
    "        print('y = ***')\n",
    "        print(y.shape)\n",
    "        # pred : [batch_size, n_class], y : [batch_size] (LongTensor, not one-hot)\n",
    "        # pred为输出的对应的2*7的矩阵\n",
    "        print('pred = ###')\n",
    "        print(pred)\n",
    "        print('y = ###')\n",
    "        print(y)\n",
    "        print('!!!before loss!!!')\n",
    "        loss = criterion(pred, y)\n",
    "        print('loss = %.6f'%loss)\n",
    "        #这里在求criterion的时候，pred=(2,7),y=(2)\n",
    "        #前面定义了criterion = nn.CrossEntropyLoss()\n",
    "        #维度前面的维度可以不同，后面的维度必须相同\n",
    "        \n",
    "        #所以这里为pred与y的交叉熵损失函数\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "        optimizer.zero_grad()\n",
    "        print('loss = ***')\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = ***\n",
      "[['i', 'like'], ['i', 'love'], ['i', 'hate']]\n",
      "hidden = ***\n",
      "torch.Size([1, 3, 5])\n",
      "input_batch = ###\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0., 0., 0.]]])\n",
      "hidden.shape = ***\n",
      "torch.Size([1, 3, 5])\n",
      "X.shape = ***\n",
      "torch.Size([3, 2, 7])\n",
      "forward = ***\n",
      "before transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0., 0., 0.]]])\n",
      "type X = ***\n",
      "<class 'torch.Tensor'>\n",
      "after transpose\n",
      "X = ***\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0.]]])\n",
      "hidden = ***\n",
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "self.rnn = ***\n",
      "!!!out.shape = !!!\n",
      "torch.Size([2, 3, 5])\n",
      "!!!hidden.shape = !!!\n",
      "torch.Size([1, 3, 5])\n",
      "out = ###\n",
      "tensor([[ 0.7352, -0.4512, -0.4275,  0.4072, -0.5029],\n",
      "        [ 0.7352, -0.4512, -0.4275,  0.4072, -0.5029],\n",
      "        [ 0.7352, -0.4512, -0.4275,  0.4072, -0.5029]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "out.shape = ###\n",
      "torch.Size([3, 5])\n",
      "n_hidden = 5,n_class = 7\n",
      "model = ###\n",
      "torch.Size([3, 7])\n",
      "predict1 = ***\n",
      "tensor([[ 0.1705, -0.3680,  0.5401, -0.0027,  0.1605, -0.4754,  0.2869],\n",
      "        [ 0.1705, -0.3680,  0.5401, -0.0027,  0.1605, -0.4754,  0.2869],\n",
      "        [ 0.1705, -0.3680,  0.5401, -0.0027,  0.1605, -0.4754,  0.2869]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "predict2 = ***\n",
      "torch.return_types.max(\n",
      "values=tensor([[0.5401],\n",
      "        [0.5401],\n",
      "        [0.5401]]),\n",
      "indices=tensor([[2],\n",
      "        [2],\n",
      "        [2]]))\n",
      "predict = ***\n",
      "tensor([[2],\n",
      "        [2],\n",
      "        [2]])\n",
      "[['i', 'like'], ['i', 'love'], ['i', 'hate']] -> ['love', 'love', 'love']\n"
     ]
    }
   ],
   "source": [
    "input = [sen.split()[:2] for sen in sentences]\n",
    "#input为截取的前面的两个单词作为相应的输入的内容\n",
    "# Predict,!!!\n",
    "print('input = ***')\n",
    "print(input)\n",
    "hidden = torch.zeros(1, len(input), n_hidden)\n",
    "print('hidden = ***')\n",
    "print(hidden.shape)\n",
    "#hidden对应的torch的大小为([1,3,5])01\n",
    "print('input_batch = ###')\n",
    "print(input_batch)\n",
    "#这里面的model相当于前面训练过的对应的model的内容\n",
    "predict1 = model(hidden, input_batch)\n",
    "print('predict1 = ***')\n",
    "print(predict1)\n",
    "#得到对应的不同类别的预测的概率值\n",
    "predict2 = predict1.data.max(1,keepdim=True)\n",
    "#提取出着所有单词当中出现最大的概率，提取出来的\n",
    "#tensor有两个部分，第一个部分为value是对应的概率\n",
    "#第二个部分为indices为对应的概率相应的具体的数值\n",
    "print('predict2 = ***')\n",
    "print(predict2)\n",
    "predict = predict2[1]\n",
    "print('predict = ***')\n",
    "print(predict)\n",
    "#predict = model(hidden, input_batch).data.max(1, keepdim=True)[1]\n",
    "#对应的预测种类的结果\n",
    "print([sen.split()[:2] for sen in sentences], '->', [idx2word[n.item()] for n in predict.squeeze()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "程序运行的过程解析：首先\n",
    "x = tensor([[[0., 1., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 1., 0., 0.]],\n",
    "\n",
    "        [[0., 1., 0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0., 1., 0.]]])\n",
    "(x为起始的两个对应的句子)\n",
    "y = tensor([2,0])\n",
    "(y为对应的两个答案内容的标签)\n",
    "\n",
    "接着对应的hidden矩阵的内容为\n",
    "hidden = tensor([[[0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer的step为什么不能放在min-batch的循环之外，optimizer.step和loss.backward的区别\n",
    "1.明确optimizer优化器的作用:根据网络反向传播的梯度信息更新网络的参数，以起到降低loss函数计算值的作用\n",
    "    1.优化器需要知道当前网络或者别的模型的参数空间\n",
    "    2.优化器需要知道反向传播的梯度信息\n",
    "    optimizer更新参数空间需要基于反向梯度，因此当调用optimizer.step()的时候应当是loss.backward()的时候，所以经常是loss.backward()后面跟上optimizer.step()函数\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rnn前向神经模型对应的网站https://blog.csdn.net/weixin_42792500/article/details/81254313\n",
    "可以看出隐藏层输入必须为零矩阵，所以"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
