{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F  \n",
    "import torch.utils.data as tud  \n",
    "from torch.nn.parameter import Parameter  #参数更新和优化函数\n",
    "from collections import Counter \n",
    "import numpy as np \n",
    "import random\n",
    "import math \n",
    "import pandas as pd\n",
    "import scipy  #\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity #余弦相似度函数\n",
    "\n",
    "# 负例采样就是Skip-Gram模型的输出不是周围词的概率了，是正例和负例的概率\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(53113)\n",
    "np.random.seed(53113)\n",
    "torch.manual_seed(53113)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(53113)\n",
    " \n",
    "\n",
    "K = 3   # 负样本随机采样数量\n",
    "C = 1    # 周围单词的数量\n",
    "NUM_EPOCHS = 2 \n",
    "VOCAB_SIZE = 20 \n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.2 \n",
    "EMBEDDING_SIZE = 5\n",
    "#对应的维度\n",
    "    \n",
    "LOG_FILE = \"word-embedding.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text = ***\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of']\n",
      "vocab = ***\n",
      "{'of': 2, 'anarchism': 1, 'originated': 1, 'as': 1, 'a': 1, 'term': 1, 'abuse': 1, 'first': 1, 'used': 1, 'against': 1, 'early': 1, 'working': 1, 'class': 1, 'radicals': 1, 'including': 1, 'the': 1, 'diggers': 1, '<unk>': 0}\n",
      "idx_to_word = ***\n",
      "['of', 'anarchism', 'originated', 'as', 'a', 'term', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', '<unk>']\n",
      "word_to_idx = ***\n",
      "{'of': 0, 'anarchism': 1, 'originated': 2, 'as': 3, 'a': 4, 'term': 5, 'abuse': 6, 'first': 7, 'used': 8, 'against': 9, 'early': 10, 'working': 11, 'class': 12, 'radicals': 13, 'including': 14, 'the': 15, 'diggers': 16, '<unk>': 17}\n",
      "word_counts = ***\n",
      "[2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      "18\n",
      "word_freqs1 = ###\n",
      "[0.11111111 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
      " 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
      " 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.        ]\n",
      "word_freqs = ***\n",
      "[0.09511438 0.05655535 0.05655535 0.05655535 0.05655535 0.05655535\n",
      " 0.05655535 0.05655535 0.05655535 0.05655535 0.05655535 0.05655535\n",
      " 0.05655535 0.05655535 0.05655535 0.05655535 0.05655535 0.        ]\n"
     ]
    }
   ],
   "source": [
    "def word_tokenize(text): \n",
    "    return text.split()\n",
    "\n",
    "\n",
    "with open(\"text8.train.txt\", \"r\") as file: \n",
    "    text = file.read() # 一次性读入文件所有内容为一个字符串\n",
    "    \n",
    "text = [w for w in word_tokenize(text.lower())] \n",
    "#词语对应的编号\n",
    "vocab = dict(Counter(text).most_common(VOCAB_SIZE-1))\n",
    "#词语出现的次数\n",
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
    "#目前这里面的[\"<unk>\"]对应的内容是必为0的内容\n",
    "print('text = ***')\n",
    "print(text)\n",
    "print('vocab = ***')\n",
    "print(vocab)\n",
    "idx_to_word = [word for word in vocab.keys()] \n",
    "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}\n",
    "#idx_to_word为所有切出来的单词构成的list\n",
    "print('idx_to_word = ***')\n",
    "print(idx_to_word)\n",
    "print('word_to_idx = ***')\n",
    "print(word_to_idx)\n",
    "#word_to_idx为将单词进行相应的编号\n",
    "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
    "print('word_counts = ***')\n",
    "print(word_counts)\n",
    "#word_counts为所有对应的单词构成的相应的矩阵\n",
    "#这里word_counts共有对应的483个单词\n",
    "print(len(word_counts))\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "print('word_freqs1 = ###')\n",
    "print(word_freqs)\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "#计算词频，按照原文转换为3/4次方\n",
    "word_freqs = word_freqs / np.sum(word_freqs)  # 用来做 negative sampling\n",
    "#词频为词频/词频的总和\n",
    "print('word_freqs = ***')\n",
    "print(word_freqs)\n",
    "#将所有的单词个数转化为一个483长度的词频矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset __init__\n"
     ]
    }
   ],
   "source": [
    "# 实现Dataloader\n",
    "class Dataset(tud.Dataset): # 继承tud.Dataset父类\n",
    "    \n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):    \n",
    "        super(Dataset, self).__init__() \n",
    "        print('Dataset __init__')\n",
    "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE-1) for t in text]\n",
    "        #依次对text当中的单词进行相应的查找\n",
    "        # get()返回指定键的值，没有则返回默认值\n",
    "        # 这里面的key -- 字典中要查找的键，default -- 如果指定键的值不存在时，返回该默认值。\n",
    "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
    "        #变成tensor类型，这里变成longtensor，也可以torch.LongTensor\n",
    "        self.word_to_idx = word_to_idx \n",
    "        self.idx_to_word = idx_to_word  \n",
    "        self.word_freqs = torch.Tensor(word_freqs) \n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.text_encoded) #所有单词的总数\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        ''' 这个function返回以下数据用于训练\n",
    "            - 中心词\n",
    "            - 这个单词附近的(positive)单词\n",
    "            - 随机采样的K个单词作为negative sample\n",
    "        '''\n",
    "        #print('Dataset__getitem__')\n",
    "        #self.text_encoded为原来英文语句的内容\n",
    "        center_word = self.text_encoded[idx] \n",
    "        #print('center_word = ***')\n",
    "        #print(center_word.shape)\n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
    "        pos_indices = [i%len(self.text_encoded) for i in pos_indices]\n",
    "        #取余，防止超出句子的范围(这里面把句子看成一个环形的句子)\n",
    "        #print('pos_indices = ###')\n",
    "        #print(pos_indices)\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        # replacement=True有放回的取\n",
    "        #print('pos_words.shape[0] = ###')\n",
    "        #print(pos_words.shape[0])\n",
    "        # 这里面的pos_words.shape[0] = 6,为左边取3个词组以及右边取3个词组\n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], replacement=True)\n",
    "        #print('neg_words = ###')\n",
    "        #print(neg_words)\n",
    "        # torch.multinomal(input,num_samples,replacement=False,out=None)        \n",
    "        #!!!replace=True为有放回的选取，这里K*pos_words.shape[0]为负采样的个数\n",
    "        \n",
    "        #这里使用multinomial的原因就在于随着样本频率的增加，它被抽样的概率越来越大\n",
    "        #负采样当中如果样本频率增加的情况下被采样的概率越来越大\n",
    "        \n",
    "        #!!!高频词在负采样中容易被选出来 所以即使左边3个词组和右边3个词组\n",
    "        \n",
    "        #!!!左右各取3个数值之后，负采样的个数为6*10个\n",
    "        #每次返回的时候center_word = [],pos_words = [6],neg_words = [60]\n",
    "        return center_word, pos_words, neg_words \n",
    "\n",
    "\n",
    "dataset = Dataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
    "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)  \n",
    "#BATCH_SIZE = 128,每一个批次随机取出128个对应的数值，这里打包成DataLoader的主要原因是\n",
    "#便于每次取出一个相应的批次操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义PyTorch模型\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "    #放入的vocab_size=30000,embed_size=100\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        print('EmbeddingModel __init__')\n",
    "        self.vocab_size = vocab_size  #30000\n",
    "        self.embed_size = embed_size  #100\n",
    "              \n",
    "        # 模型输入，输出是两个一样的矩阵参数nn.Embedding(30000, 100)\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        #torch.nn.Embedding(num_embeddings:int,embedding_dim:int,sparse:False)\n",
    "        #如果sparse=True的情况下出来的为稀疏矩阵\n",
    "        #!!!embedding为一个简单的存储固定大小的词典的嵌入向量的查找表\n",
    "        #给一个编号，返回这个编号对应的嵌入向量\n",
    "        #torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, \n",
    "        #max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None)\n",
    "        \n",
    "        #这里面的num_embeddings=30000,embedding_dim=100,\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "         # 权重初始化的一种方法\n",
    "        initrange = 0.5 / self.embed_size\n",
    "        #因为embed_size = 100,所以initrange = 0.005\n",
    "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
    "        print('origin = ***')\n",
    "        print('self.in_embed = ***')\n",
    "        print(self.in_embed)\n",
    "        print('self.out_embed = ***')\n",
    "        print(self.out_embed)\n",
    "        #因为输出的值为(1/2)/self.embed_size=0.5/self.embed_size,\n",
    "        #所以self.in_embed的范围为(-initrange,initrange)即(-0.005,0.005)\n",
    "        #self.out_embed的范围为(-initrange,initrange)\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: 中心词, [batch_size]\n",
    "        pos_labels: 中心词周围出现过的单词 [batch_size * (c * 2)],左边找出c个词组，右边找出c个词组\n",
    "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (c * 2 * K)]\n",
    "        return: loss, [batch_size]\n",
    "        '''\n",
    "        print('EmbeddingModel forward')\n",
    "        batch_size = input_labels.size(0) \n",
    "        print('input_labels = !!!')\n",
    "        print(input_labels)\n",
    "        print('pos_labels = !!!')\n",
    "        print(pos_labels)\n",
    "        print('neg_labels = !!!')\n",
    "        print(neg_labels)\n",
    "        print('batch_size = %d'%batch_size)\n",
    "        #原先这里input_labels = [128]\n",
    "        #pos_labels = [128,6],neg_labels = [128,60]\n",
    "        \n",
    "       \n",
    "        input_embedding = self.in_embed(input_labels) # B * embed_size\n",
    "        #这里的self.in_embed为定义过的nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
    "        #一个相应的函数，self.vocab_size=30000,self.embed_size=100\n",
    "        \n",
    "        #输入: (∗) , 包含提取的编号的任意形状的长整型张量。\n",
    "        #输出: (∗,H) , 其中 * 为输入的形状，H为embedding_dim\n",
    "        #embedding_dim的选择要注意，根据自己的符号数量，举个例子，如果你的词典尺寸是1024，那么极限压缩\n",
    "        #（用二进制表示）也需要10维，再考虑词性之间的相关性，怎么也要在15-20维左右，虽然embedding是用来降维的，\n",
    "        #但是也要注意这种极限维度，结合实际情况，合理定义\n",
    "        \n",
    "        #扩展维度之后生成的相当于一个随机矩阵的内容\n",
    "        \n",
    "        #经过self.in_embed(input_labels)处理之后input_embedding=[128,100]\n",
    "        #pos_labels = [128,6,100],neg_labels = [128,60,100]\n",
    "        pos_embedding = self.out_embed(pos_labels) # B * (2C) * embed_size \n",
    "        neg_embedding = self.out_embed(neg_labels) # B * (2*C*K) * embed_size\n",
    "        #!!!注意这里面使用到了对应的out_embed相应的参数\n",
    "        print('@@@input_embedding = @@@')\n",
    "        print(input_embedding)\n",
    "        print('@@@pos_embedding = @@@')\n",
    "        print(pos_embedding)\n",
    "        print('@@@neg_embedding = @@@')\n",
    "        print(neg_embedding)\n",
    "        #input_labels:[batch_size]\n",
    "        #pos_labels:[batch_size,(windows_size*2)]\n",
    "        #neg_labels:[batch_size,(windows_size*2*k)]\n",
    "        #torch.bmm()为batch间的矩阵相乘（b,n.m)*(b,m,p)=(b,n,p)\n",
    "        print('input_embedding.unsqueeze(2) = ***')\n",
    "        print(input_embedding.unsqueeze(2))\n",
    "        #input_embedding = [128,100],input_embedding.unsqueeze(2) = [128,100,1]\n",
    "        #pos_embedding = [128,6,100]\n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)) # B * (2*C)\n",
    "        #input_embedding.unsqueeze(2) = [batch_size,embedding_size,1]\n",
    "\n",
    "        print('@@@log_pos = @@@')\n",
    "        print(log_pos)\n",
    "        #[128,6,100]*[128,100,1] = [128,6,1]\n",
    "        #[batch_size,window_size*2,embedded]*[batch_size,embeded,1] = [batch_size,window_size*2,1]\n",
    "        log_pos = log_pos.squeeze()\n",
    "        print('@@@log_pos.squeeze() == ***')\n",
    "        #squeeze()将为1的维度缩出来，所以log_pos = [128,6]\n",
    "        print(log_pos)\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # B * (2*C*K)\n",
    "        #neg_embedding = [128,60,100],input_embedding.unsqueeze(2) = [128,100,1]\n",
    "        #相乘之后的结果为[128,60,1]\n",
    "        print('@@@log_neg = @@@')\n",
    "        print(log_neg)\n",
    "        print('@@@log_pos = @@@')\n",
    "        print(log_pos)\n",
    "        #下面loss计算就是论文里的公式\n",
    "        #log_neg.shape = [128,60]\n",
    "        #log_pos.shape = [128,6]\n",
    "        \n",
    "        log_pos = F.logsigmoid(log_pos).sum(1) # [batch_size]\n",
    "        log_neg = F.logsigmoid(log_neg).sum(1) # [batch_size]   \n",
    "        print('######################log_pos = #######################')\n",
    "        print(log_pos)\n",
    "        print('######################log_neg = #######################')\n",
    "        print(log_neg)\n",
    "        loss = log_pos + log_neg  # 正样本损失和负样本损失和尽量最大\n",
    "        #如果为负数的时候就是损失和尽量最小\n",
    "        #对应的大小为[batch_size]\n",
    "        #因为需要提取出来的这128个维度的单词集体操作\n",
    "        return -loss \n",
    "        #注意这里return的是-loss，最终的optimizer.step中还带有一个减号\n",
    "        #所以这里如果是当前选中的这个单词的周边单词的话的128个维度单词的梯度被减去，而周边单词的梯度被加上，\n",
    "        #而如果是这128个单词负采样的话这128个维度的单词\n",
    "    \n",
    "    # 模型训练有两个矩阵，self.in_embed和self.out_embed两个, 作者认为输入矩阵比较好，舍弃了输出矩阵\n",
    "    # 取出输入矩阵参数，self.in_embed的矩阵为正采样的相应的矩阵，self.out_embed为负采样的相应矩阵\n",
    "    def input_embeddings(self):   \n",
    "        return self.in_embed.weight.data.cpu().numpy() \n",
    "    def output_embeddings(self):\n",
    "        return self.out_embed.weight.data.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingModel __init__\n",
      "origin = ***\n",
      "self.in_embed = ***\n",
      "Embedding(20, 5)\n",
      "self.out_embed = ***\n",
      "Embedding(20, 5)\n"
     ]
    }
   ],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "#VOCAB_SIZE = 30000,EMBEDDING_SIZE = 100\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "#model = model.cuda(),这里面的LEARING_RATE=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 寻找nearest neighbors\n",
    "~~~\n",
    "def find_nearest(word):\n",
    "    '''embedding_weights是一个[vocab_size, embedding_size]的参数矩阵'''\n",
    "    index = word_to_idx[word] \n",
    "    embedding = embedding_weights[index] # 取出这个单词的embedding向量\n",
    "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    # 计算所有30000个embedding向量与传入单词embedding向量的相似度距离\n",
    "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]] # 返回前10个最相似的\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_labels = !!!\n",
      "tensor([11, 14,  3, 16])\n",
      "pos_labels = !!!\n",
      "tensor([[10, 12],\n",
      "        [13, 15],\n",
      "        [ 2,  4],\n",
      "        [15,  0]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 9, 16,  4,  8,  7, 14],\n",
      "        [ 6, 13,  8,  8, 15,  2],\n",
      "        [ 1,  2, 14,  5, 16, 11],\n",
      "        [ 4, 14,  0, 11,  1, 10]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.09634085 -0.05079702 -0.09764721 -0.05053148  0.02803528]\n",
      " [ 0.07910234 -0.00253747 -0.0092114   0.08016897  0.07994234]\n",
      " [-0.00411206  0.07705157  0.07703517 -0.07001527  0.01060919]\n",
      " [-0.04474556  0.09511333 -0.04521798 -0.04240331  0.0406464 ]\n",
      " [ 0.0812582  -0.05959148  0.09809767 -0.02271371 -0.00155157]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09734754 -0.09383321 -0.01098482 -0.05939215  0.0826202 ]\n",
      " [ 0.0777399   0.08806259 -0.07410274 -0.03753499  0.00243527]\n",
      " [-0.08257407  0.00867909 -0.01076714 -0.07757157 -0.09425669]\n",
      " [-0.03396392  0.02746927 -0.0773401  -0.08485603  0.03837057]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09822249  0.06757515 -0.04931423 -0.08419118  0.09988285]\n",
      " [ 0.01614852 -0.02888861 -0.04731026  0.08716858 -0.0112172 ]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.08869338  0.01102352  0.04814521 -0.00112363 -0.05590123]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.00512155  0.07467676 -0.04355403  0.07052828  0.06539901]\n",
      " [ 0.01089936 -0.01800267 -0.01550688  0.08277256 -0.08840557]\n",
      " [ 0.00083701 -0.03344972  0.05429759  0.06604428  0.03654701]\n",
      " [-0.01700632  0.04220336  0.00556344  0.07955574  0.05962846]\n",
      " [ 0.00151732  0.07319064 -0.00873277 -0.00376403 -0.06849217]\n",
      " [-0.03386819  0.01013829 -0.02733435 -0.02522949 -0.02591752]\n",
      " [ 0.00406118 -0.01747809  0.05532692 -0.08463414 -0.04119561]\n",
      " [ 0.08837286 -0.08350255 -0.07060494 -0.08184141  0.09026492]\n",
      " [ 0.09726392  0.01010766 -0.09975369 -0.04669186  0.08321423]\n",
      " [-0.0065546   0.09988397  0.01899377 -0.07461568 -0.08908959]\n",
      " [ 0.04158039 -0.06501577 -0.03525469 -0.02016342  0.01037975]\n",
      " [ 0.03592933 -0.04292066  0.04545163  0.05020288  0.06421535]\n",
      " [ 0.03265331 -0.04742007  0.01350822 -0.01186617 -0.04803563]\n",
      " [ 0.01942398 -0.0734751  -0.08958908  0.07566766  0.05688801]\n",
      " [-0.03696951 -0.04350419 -0.08427503 -0.01284425  0.05154333]\n",
      " [-0.03901384 -0.02431738  0.06870208  0.07070405  0.03974665]\n",
      " [-0.03101722 -0.04554204 -0.07931592  0.07828941  0.08329702]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([11, 14,  3, 16])\n",
      "pos_labels = !!!\n",
      "tensor([[10, 12],\n",
      "        [13, 15],\n",
      "        [ 2,  4],\n",
      "        [15,  0]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 9, 16,  4,  8,  7, 14],\n",
      "        [ 6, 13,  8,  8, 15,  2],\n",
      "        [ 1,  2, 14,  5, 16, 11],\n",
      "        [ 4, 14,  0, 11,  1, 10]])\n",
      "batch_size = 4\n",
      "@@@input_embedding = @@@\n",
      "tensor([[-0.0340,  0.0275, -0.0773, -0.0849,  0.0384],\n",
      "        [ 0.0161, -0.0289, -0.0473,  0.0872, -0.0112],\n",
      "        [-0.0447,  0.0951, -0.0452, -0.0424,  0.0406],\n",
      "        [-0.0887,  0.0110,  0.0481, -0.0011, -0.0559]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[ 0.0416, -0.0650, -0.0353, -0.0202,  0.0104],\n",
      "         [ 0.0327, -0.0474,  0.0135, -0.0119, -0.0480]],\n",
      "\n",
      "        [[ 0.0194, -0.0735, -0.0896,  0.0757,  0.0569],\n",
      "         [-0.0390, -0.0243,  0.0687,  0.0707,  0.0397]],\n",
      "\n",
      "        [[ 0.0008, -0.0334,  0.0543,  0.0660,  0.0365],\n",
      "         [ 0.0015,  0.0732, -0.0087, -0.0038, -0.0685]],\n",
      "\n",
      "        [[-0.0390, -0.0243,  0.0687,  0.0707,  0.0397],\n",
      "         [ 0.0051,  0.0747, -0.0436,  0.0705,  0.0654]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[-0.0066,  0.0999,  0.0190, -0.0746, -0.0891],\n",
      "         [-0.0310, -0.0455, -0.0793,  0.0783,  0.0833],\n",
      "         [ 0.0015,  0.0732, -0.0087, -0.0038, -0.0685],\n",
      "         [ 0.0973,  0.0101, -0.0998, -0.0467,  0.0832],\n",
      "         [ 0.0884, -0.0835, -0.0706, -0.0818,  0.0903],\n",
      "         [-0.0370, -0.0435, -0.0843, -0.0128,  0.0515]],\n",
      "\n",
      "        [[ 0.0041, -0.0175,  0.0553, -0.0846, -0.0412],\n",
      "         [ 0.0194, -0.0735, -0.0896,  0.0757,  0.0569],\n",
      "         [ 0.0973,  0.0101, -0.0998, -0.0467,  0.0832],\n",
      "         [ 0.0973,  0.0101, -0.0998, -0.0467,  0.0832],\n",
      "         [-0.0390, -0.0243,  0.0687,  0.0707,  0.0397],\n",
      "         [ 0.0008, -0.0334,  0.0543,  0.0660,  0.0365]],\n",
      "\n",
      "        [[ 0.0109, -0.0180, -0.0155,  0.0828, -0.0884],\n",
      "         [ 0.0008, -0.0334,  0.0543,  0.0660,  0.0365],\n",
      "         [-0.0370, -0.0435, -0.0843, -0.0128,  0.0515],\n",
      "         [-0.0339,  0.0101, -0.0273, -0.0252, -0.0259],\n",
      "         [-0.0310, -0.0455, -0.0793,  0.0783,  0.0833],\n",
      "         [ 0.0359, -0.0429,  0.0455,  0.0502,  0.0642]],\n",
      "\n",
      "        [[ 0.0015,  0.0732, -0.0087, -0.0038, -0.0685],\n",
      "         [-0.0370, -0.0435, -0.0843, -0.0128,  0.0515],\n",
      "         [ 0.0051,  0.0747, -0.0436,  0.0705,  0.0654],\n",
      "         [ 0.0359, -0.0429,  0.0455,  0.0502,  0.0642],\n",
      "         [ 0.0109, -0.0180, -0.0155,  0.0828, -0.0884],\n",
      "         [ 0.0416, -0.0650, -0.0353, -0.0202,  0.0104]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[-0.0340],\n",
      "         [ 0.0275],\n",
      "         [-0.0773],\n",
      "         [-0.0849],\n",
      "         [ 0.0384]],\n",
      "\n",
      "        [[ 0.0161],\n",
      "         [-0.0289],\n",
      "         [-0.0473],\n",
      "         [ 0.0872],\n",
      "         [-0.0112]],\n",
      "\n",
      "        [[-0.0447],\n",
      "         [ 0.0951],\n",
      "         [-0.0452],\n",
      "         [-0.0424],\n",
      "         [ 0.0406]],\n",
      "\n",
      "        [[-0.0887],\n",
      "         [ 0.0110],\n",
      "         [ 0.0481],\n",
      "         [-0.0011],\n",
      "         [-0.0559]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[[ 0.0016],\n",
      "         [-0.0043]],\n",
      "\n",
      "        [[ 0.0126],\n",
      "         [ 0.0025]],\n",
      "\n",
      "        [[-0.0070],\n",
      "         [ 0.0047]],\n",
      "\n",
      "        [[ 0.0042],\n",
      "         [-0.0055]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[ 0.0016, -0.0043],\n",
      "        [ 0.0126,  0.0025],\n",
      "        [-0.0070,  0.0047],\n",
      "        [ 0.0042, -0.0055]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[-0.0044, -0.0025, -0.0003, -0.0118, -0.0106, -0.0096],\n",
      "        [ 0.0090, -0.0126, -0.0010, -0.0010, -0.0025, -0.0038],\n",
      "        [ 0.0086,  0.0070, -0.0040, -0.0037, -0.0007,  0.0073],\n",
      "        [-0.0041,  0.0041,  0.0055,  0.0051, -0.0029,  0.0067]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[ 0.0016, -0.0043],\n",
      "        [ 0.0126,  0.0025],\n",
      "        [-0.0070,  0.0047],\n",
      "        [ 0.0042, -0.0055]], grad_fn=<SqueezeBackward0>)\n",
      "######################log_pos = #######################\n",
      "tensor([-1.3876, -1.3787, -1.3875, -1.3869], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1786, -4.1649, -4.1517, -4.1517], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5469, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.09634085 -0.05079702 -0.09764721 -0.05053148  0.02803528]\n",
      " [ 0.07910234 -0.00253747 -0.0092114   0.08016897  0.07994234]\n",
      " [-0.00411206  0.07705157  0.07703517 -0.07001527  0.01060919]\n",
      " [-0.04474556  0.09511333 -0.04521798 -0.04240331  0.0406464 ]\n",
      " [ 0.0812582  -0.05959148  0.09809767 -0.02271371 -0.00155157]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09734754 -0.09383321 -0.01098482 -0.05939215  0.0826202 ]\n",
      " [ 0.0777399   0.08806259 -0.07410274 -0.03753499  0.00243527]\n",
      " [-0.08257407  0.00867909 -0.01076714 -0.07757157 -0.09425669]\n",
      " [-0.03396392  0.02746927 -0.0773401  -0.08485603  0.03837057]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09822249  0.06757515 -0.04931423 -0.08419118  0.09988285]\n",
      " [ 0.01614852 -0.02888861 -0.04731026  0.08716858 -0.0112172 ]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.08869338  0.01102352  0.04814521 -0.00112363 -0.05590123]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.00512155  0.07467676 -0.04355403  0.07052828  0.06539901]\n",
      " [ 0.01089936 -0.01800267 -0.01550688  0.08277256 -0.08840557]\n",
      " [ 0.00083701 -0.03344972  0.05429759  0.06604428  0.03654701]\n",
      " [-0.01700632  0.04220336  0.00556344  0.07955574  0.05962846]\n",
      " [ 0.00151732  0.07319064 -0.00873277 -0.00376403 -0.06849217]\n",
      " [-0.03386819  0.01013829 -0.02733435 -0.02522949 -0.02591752]\n",
      " [ 0.00406118 -0.01747809  0.05532692 -0.08463414 -0.04119561]\n",
      " [ 0.08837286 -0.08350255 -0.07060494 -0.08184141  0.09026492]\n",
      " [ 0.09726392  0.01010766 -0.09975369 -0.04669186  0.08321423]\n",
      " [-0.0065546   0.09988397  0.01899377 -0.07461568 -0.08908959]\n",
      " [ 0.04158039 -0.06501577 -0.03525469 -0.02016342  0.01037975]\n",
      " [ 0.03592933 -0.04292066  0.04545163  0.05020288  0.06421535]\n",
      " [ 0.03265331 -0.04742007  0.01350822 -0.01186617 -0.04803563]\n",
      " [ 0.01942398 -0.0734751  -0.08958908  0.07566766  0.05688801]\n",
      " [-0.03696951 -0.04350419 -0.08427503 -0.01284425  0.05154333]\n",
      " [-0.03901384 -0.02431738  0.06870208  0.07070405  0.03974665]\n",
      " [-0.03101722 -0.04554204 -0.07931592  0.07828941  0.08329702]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.09634085 -0.05079702 -0.09764721 -0.05053148  0.02803528]\n",
      " [ 0.07910234 -0.00253747 -0.0092114   0.08016897  0.07994234]\n",
      " [-0.00411206  0.07705157  0.07703517 -0.07001527  0.01060919]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.0812582  -0.05959148  0.09809767 -0.02271371 -0.00155157]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09734754 -0.09383321 -0.01098482 -0.05939215  0.0826202 ]\n",
      " [ 0.0777399   0.08806259 -0.07410274 -0.03753499  0.00243527]\n",
      " [-0.08257407  0.00867909 -0.01076714 -0.07757157 -0.09425669]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09822249  0.06757515 -0.04931423 -0.08419118  0.09988285]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.00510944  0.07467826 -0.04354746  0.07052812  0.06539138]\n",
      " [ 0.01423378 -0.02064627 -0.01558669  0.08385622 -0.08801778]\n",
      " [ 0.00042472 -0.03270953  0.05547467  0.06385356  0.03683507]\n",
      " [-0.01700632  0.04220336  0.00556344  0.07955574  0.05962846]\n",
      " [ 0.00347239  0.07459993 -0.00913286 -0.00267175 -0.06703742]\n",
      " [-0.03274747  0.00775602 -0.0262018  -0.02416743 -0.02693557]\n",
      " [ 0.00365927 -0.01675911  0.05650437 -0.08680359 -0.04091644]\n",
      " [ 0.08922645 -0.08419292 -0.06866121 -0.07970879  0.08930059]\n",
      " [ 0.09731022  0.01086201 -0.09544205 -0.04891849  0.08281042]\n",
      " [-0.00570363  0.09919572  0.02093154 -0.0724896  -0.09005097]\n",
      " [ 0.04294194 -0.06460427 -0.03838624 -0.02225508  0.01273111]\n",
      " [ 0.03925557 -0.04556474  0.04537743  0.05128714  0.06459684]\n",
      " [ 0.03180239 -0.04673187  0.01157057 -0.01399213 -0.04707431]\n",
      " [ 0.01941888 -0.07346597 -0.08957414  0.07564013  0.05689155]\n",
      " [-0.03278269 -0.0468518  -0.08240066 -0.0096224   0.05095591]\n",
      " [-0.04122755 -0.02404054  0.06990619  0.07067049  0.03835277]\n",
      " [-0.02904803 -0.0486083  -0.07624915  0.08147391  0.08132004]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "epoch: 0, iter: 0, loss: 5.546908378601074\n",
      "input_labels = !!!\n",
      "tensor([0, 1, 2, 4])\n",
      "pos_labels = !!!\n",
      "tensor([[5, 6],\n",
      "        [0, 2],\n",
      "        [1, 3],\n",
      "        [3, 5]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 0, 11,  4,  0,  2, 13],\n",
      "        [ 1,  7,  6,  9,  2,  6],\n",
      "        [ 0,  3,  1,  9, 13, 14],\n",
      "        [ 1, 15,  6,  3,  4, 13]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.09634085 -0.05079702 -0.09764721 -0.05053148  0.02803528]\n",
      " [ 0.07910234 -0.00253747 -0.0092114   0.08016897  0.07994234]\n",
      " [-0.00411206  0.07705157  0.07703517 -0.07001527  0.01060919]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.0812582  -0.05959148  0.09809767 -0.02271371 -0.00155157]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09734754 -0.09383321 -0.01098482 -0.05939215  0.0826202 ]\n",
      " [ 0.0777399   0.08806259 -0.07410274 -0.03753499  0.00243527]\n",
      " [-0.08257407  0.00867909 -0.01076714 -0.07757157 -0.09425669]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09822249  0.06757515 -0.04931423 -0.08419118  0.09988285]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.00510944  0.07467826 -0.04354746  0.07052812  0.06539138]\n",
      " [ 0.01423378 -0.02064627 -0.01558669  0.08385622 -0.08801778]\n",
      " [ 0.00042472 -0.03270953  0.05547467  0.06385356  0.03683507]\n",
      " [-0.01700632  0.04220336  0.00556344  0.07955574  0.05962846]\n",
      " [ 0.00347239  0.07459993 -0.00913286 -0.00267175 -0.06703742]\n",
      " [-0.03274747  0.00775602 -0.0262018  -0.02416743 -0.02693557]\n",
      " [ 0.00365927 -0.01675911  0.05650437 -0.08680359 -0.04091644]\n",
      " [ 0.08922645 -0.08419292 -0.06866121 -0.07970879  0.08930059]\n",
      " [ 0.09731022  0.01086201 -0.09544205 -0.04891849  0.08281042]\n",
      " [-0.00570363  0.09919572  0.02093154 -0.0724896  -0.09005097]\n",
      " [ 0.04294194 -0.06460427 -0.03838624 -0.02225508  0.01273111]\n",
      " [ 0.03925557 -0.04556474  0.04537743  0.05128714  0.06459684]\n",
      " [ 0.03180239 -0.04673187  0.01157057 -0.01399213 -0.04707431]\n",
      " [ 0.01941888 -0.07346597 -0.08957414  0.07564013  0.05689155]\n",
      " [-0.03278269 -0.0468518  -0.08240066 -0.0096224   0.05095591]\n",
      " [-0.04122755 -0.02404054  0.06990619  0.07067049  0.03835277]\n",
      " [-0.02904803 -0.0486083  -0.07624915  0.08147391  0.08132004]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([0, 1, 2, 4])\n",
      "pos_labels = !!!\n",
      "tensor([[5, 6],\n",
      "        [0, 2],\n",
      "        [1, 3],\n",
      "        [3, 5]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 0, 11,  4,  0,  2, 13],\n",
      "        [ 1,  7,  6,  9,  2,  6],\n",
      "        [ 0,  3,  1,  9, 13, 14],\n",
      "        [ 1, 15,  6,  3,  4, 13]])\n",
      "batch_size = 4\n",
      "@@@input_embedding = @@@\n",
      "tensor([[-0.0963, -0.0508, -0.0976, -0.0505,  0.0280],\n",
      "        [ 0.0791, -0.0025, -0.0092,  0.0802,  0.0799],\n",
      "        [-0.0041,  0.0771,  0.0770, -0.0700,  0.0106],\n",
      "        [ 0.0813, -0.0596,  0.0981, -0.0227, -0.0016]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[-0.0327,  0.0078, -0.0262, -0.0242, -0.0269],\n",
      "         [ 0.0037, -0.0168,  0.0565, -0.0868, -0.0409]],\n",
      "\n",
      "        [[ 0.0051,  0.0747, -0.0435,  0.0705,  0.0654],\n",
      "         [ 0.0004, -0.0327,  0.0555,  0.0639,  0.0368]],\n",
      "\n",
      "        [[ 0.0142, -0.0206, -0.0156,  0.0839, -0.0880],\n",
      "         [-0.0170,  0.0422,  0.0056,  0.0796,  0.0596]],\n",
      "\n",
      "        [[-0.0170,  0.0422,  0.0056,  0.0796,  0.0596],\n",
      "         [-0.0327,  0.0078, -0.0262, -0.0242, -0.0269]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[ 0.0051,  0.0747, -0.0435,  0.0705,  0.0654],\n",
      "         [ 0.0393, -0.0456,  0.0454,  0.0513,  0.0646],\n",
      "         [ 0.0035,  0.0746, -0.0091, -0.0027, -0.0670],\n",
      "         [ 0.0051,  0.0747, -0.0435,  0.0705,  0.0654],\n",
      "         [ 0.0004, -0.0327,  0.0555,  0.0639,  0.0368],\n",
      "         [ 0.0194, -0.0735, -0.0896,  0.0756,  0.0569]],\n",
      "\n",
      "        [[ 0.0142, -0.0206, -0.0156,  0.0839, -0.0880],\n",
      "         [ 0.0892, -0.0842, -0.0687, -0.0797,  0.0893],\n",
      "         [ 0.0037, -0.0168,  0.0565, -0.0868, -0.0409],\n",
      "         [-0.0057,  0.0992,  0.0209, -0.0725, -0.0901],\n",
      "         [ 0.0004, -0.0327,  0.0555,  0.0639,  0.0368],\n",
      "         [ 0.0037, -0.0168,  0.0565, -0.0868, -0.0409]],\n",
      "\n",
      "        [[ 0.0051,  0.0747, -0.0435,  0.0705,  0.0654],\n",
      "         [-0.0170,  0.0422,  0.0056,  0.0796,  0.0596],\n",
      "         [ 0.0142, -0.0206, -0.0156,  0.0839, -0.0880],\n",
      "         [-0.0057,  0.0992,  0.0209, -0.0725, -0.0901],\n",
      "         [ 0.0194, -0.0735, -0.0896,  0.0756,  0.0569],\n",
      "         [-0.0328, -0.0469, -0.0824, -0.0096,  0.0510]],\n",
      "\n",
      "        [[ 0.0142, -0.0206, -0.0156,  0.0839, -0.0880],\n",
      "         [-0.0412, -0.0240,  0.0699,  0.0707,  0.0384],\n",
      "         [ 0.0037, -0.0168,  0.0565, -0.0868, -0.0409],\n",
      "         [-0.0170,  0.0422,  0.0056,  0.0796,  0.0596],\n",
      "         [ 0.0035,  0.0746, -0.0091, -0.0027, -0.0670],\n",
      "         [ 0.0194, -0.0735, -0.0896,  0.0756,  0.0569]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[-0.0963],\n",
      "         [-0.0508],\n",
      "         [-0.0976],\n",
      "         [-0.0505],\n",
      "         [ 0.0280]],\n",
      "\n",
      "        [[ 0.0791],\n",
      "         [-0.0025],\n",
      "         [-0.0092],\n",
      "         [ 0.0802],\n",
      "         [ 0.0799]],\n",
      "\n",
      "        [[-0.0041],\n",
      "         [ 0.0771],\n",
      "         [ 0.0770],\n",
      "         [-0.0700],\n",
      "         [ 0.0106]],\n",
      "\n",
      "        [[ 0.0813],\n",
      "         [-0.0596],\n",
      "         [ 0.0981],\n",
      "         [-0.0227],\n",
      "         [-0.0016]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[[ 0.0058],\n",
      "         [-0.0018]],\n",
      "\n",
      "        [[ 0.0115],\n",
      "         [ 0.0077]],\n",
      "\n",
      "        [[-0.0097],\n",
      "         [-0.0012]],\n",
      "\n",
      "        [[-0.0053],\n",
      "         [-0.0051]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[ 0.0058, -0.0018],\n",
      "        [ 0.0115,  0.0077],\n",
      "        [-0.0097, -0.0012],\n",
      "        [-0.0053, -0.0051]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[ 0.0018,  0.0067,  0.0050,  0.0018,  0.0060, -0.0084],\n",
      "        [-0.0010, -0.0087,  0.0104,  0.0139, -0.0077,  0.0104],\n",
      "        [ 0.0019,  0.0012,  0.0097, -0.0134,  0.0173,  0.0086],\n",
      "        [ 0.0009, -0.0033, -0.0089,  0.0053,  0.0049,  0.0046]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[ 0.0058, -0.0018],\n",
      "        [ 0.0115,  0.0077],\n",
      "        [-0.0097, -0.0012],\n",
      "        [-0.0053, -0.0051]], grad_fn=<SqueezeBackward0>)\n",
      "######################log_pos = #######################\n",
      "tensor([-1.3843, -1.3767, -1.3917, -1.3915], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1525, -4.1502, -4.1463, -4.1571], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5376, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.09634085 -0.05079702 -0.09764721 -0.05053148  0.02803528]\n",
      " [ 0.07910234 -0.00253747 -0.0092114   0.08016897  0.07994234]\n",
      " [-0.00411206  0.07705157  0.07703517 -0.07001527  0.01060919]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.0812582  -0.05959148  0.09809767 -0.02271371 -0.00155157]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09734754 -0.09383321 -0.01098482 -0.05939215  0.0826202 ]\n",
      " [ 0.0777399   0.08806259 -0.07410274 -0.03753499  0.00243527]\n",
      " [-0.08257407  0.00867909 -0.01076714 -0.07757157 -0.09425669]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09822249  0.06757515 -0.04931423 -0.08419118  0.09988285]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.00510944  0.07467826 -0.04354746  0.07052812  0.06539138]\n",
      " [ 0.01423378 -0.02064627 -0.01558669  0.08385622 -0.08801778]\n",
      " [ 0.00042472 -0.03270953  0.05547467  0.06385356  0.03683507]\n",
      " [-0.01700632  0.04220336  0.00556344  0.07955574  0.05962846]\n",
      " [ 0.00347239  0.07459993 -0.00913286 -0.00267175 -0.06703742]\n",
      " [-0.03274747  0.00775602 -0.0262018  -0.02416743 -0.02693557]\n",
      " [ 0.00365927 -0.01675911  0.05650437 -0.08680359 -0.04091644]\n",
      " [ 0.08922645 -0.08419292 -0.06866121 -0.07970879  0.08930059]\n",
      " [ 0.09731022  0.01086201 -0.09544205 -0.04891849  0.08281042]\n",
      " [-0.00570363  0.09919572  0.02093154 -0.0724896  -0.09005097]\n",
      " [ 0.04294194 -0.06460427 -0.03838624 -0.02225508  0.01273111]\n",
      " [ 0.03925557 -0.04556474  0.04537743  0.05128714  0.06459684]\n",
      " [ 0.03180239 -0.04673187  0.01157057 -0.01399213 -0.04707431]\n",
      " [ 0.01941888 -0.07346597 -0.08957414  0.07564013  0.05689155]\n",
      " [-0.03278269 -0.0468518  -0.08240066 -0.0096224   0.05095591]\n",
      " [-0.04122755 -0.02404054  0.06990619  0.07067049  0.03835277]\n",
      " [-0.02904803 -0.0486083  -0.07624915  0.08147391  0.08132004]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.09888365 -0.05281906 -0.09474792 -0.06153099  0.0207892 ]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09734754 -0.09383321 -0.01098482 -0.05939215  0.0826202 ]\n",
      " [ 0.0777399   0.08806259 -0.07410274 -0.03753499  0.00243527]\n",
      " [-0.08257407  0.00867909 -0.01076714 -0.07757157 -0.09425669]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09822249  0.06757515 -0.04931423 -0.08419118  0.09988285]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01199113  0.07522831 -0.04082245  0.07679392  0.06571294]\n",
      " [ 0.01022371 -0.0190751  -0.01778902  0.08240167 -0.08997601]\n",
      " [ 0.00281086 -0.03144292  0.0579103   0.0650977   0.03612096]\n",
      " [-0.01699578  0.04219782  0.0055786   0.07955068  0.05962858]\n",
      " [ 0.00384843  0.07735284 -0.00914419 -0.00084515 -0.06769786]\n",
      " [-0.03311238  0.00499618 -0.02617722 -0.02599635 -0.0262756 ]\n",
      " [-0.00472637 -0.01640756  0.05205587 -0.09148521 -0.04415226]\n",
      " [ 0.08724033 -0.08412921 -0.06842993 -0.08172169  0.08729339]\n",
      " [ 0.09731022  0.01086201 -0.09544205 -0.04891849  0.08281042]\n",
      " [-0.00756395  0.09731952  0.01922144 -0.07271779 -0.09230264]\n",
      " [ 0.04294194 -0.06460427 -0.03838624 -0.02225508  0.01273111]\n",
      " [ 0.04165604 -0.04429905  0.04781045  0.05254621  0.0638983 ]\n",
      " [ 0.03180239 -0.04673187  0.01157057 -0.01399213 -0.04707431]\n",
      " [ 0.01991266 -0.07261399 -0.09147868  0.07921045  0.0559635 ]\n",
      " [-0.03268033 -0.0487698  -0.08431825 -0.00787955  0.05069182]\n",
      " [-0.04326233 -0.02254831  0.06744973  0.07123926  0.03839162]\n",
      " [-0.02904803 -0.0486083  -0.07624915  0.08147391  0.08132004]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "input_labels = !!!\n",
      "tensor([ 9, 10,  8, 13])\n",
      "pos_labels = !!!\n",
      "tensor([[ 8, 10],\n",
      "        [ 9, 11],\n",
      "        [ 7,  9],\n",
      "        [12, 14]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 6,  6,  6, 11,  9, 14],\n",
      "        [ 7, 11, 14, 11, 14, 14],\n",
      "        [ 7, 15,  1,  2, 10, 13],\n",
      "        [ 6, 13,  6, 11, 15, 14]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.09888365 -0.05281906 -0.09474792 -0.06153099  0.0207892 ]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09734754 -0.09383321 -0.01098482 -0.05939215  0.0826202 ]\n",
      " [ 0.0777399   0.08806259 -0.07410274 -0.03753499  0.00243527]\n",
      " [-0.08257407  0.00867909 -0.01076714 -0.07757157 -0.09425669]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09822249  0.06757515 -0.04931423 -0.08419118  0.09988285]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01199113  0.07522831 -0.04082245  0.07679392  0.06571294]\n",
      " [ 0.01022371 -0.0190751  -0.01778902  0.08240167 -0.08997601]\n",
      " [ 0.00281086 -0.03144292  0.0579103   0.0650977   0.03612096]\n",
      " [-0.01699578  0.04219782  0.0055786   0.07955068  0.05962858]\n",
      " [ 0.00384843  0.07735284 -0.00914419 -0.00084515 -0.06769786]\n",
      " [-0.03311238  0.00499618 -0.02617722 -0.02599635 -0.0262756 ]\n",
      " [-0.00472637 -0.01640756  0.05205587 -0.09148521 -0.04415226]\n",
      " [ 0.08724033 -0.08412921 -0.06842993 -0.08172169  0.08729339]\n",
      " [ 0.09731022  0.01086201 -0.09544205 -0.04891849  0.08281042]\n",
      " [-0.00756395  0.09731952  0.01922144 -0.07271779 -0.09230264]\n",
      " [ 0.04294194 -0.06460427 -0.03838624 -0.02225508  0.01273111]\n",
      " [ 0.04165604 -0.04429905  0.04781045  0.05254621  0.0638983 ]\n",
      " [ 0.03180239 -0.04673187  0.01157057 -0.01399213 -0.04707431]\n",
      " [ 0.01991266 -0.07261399 -0.09147868  0.07921045  0.0559635 ]\n",
      " [-0.03268033 -0.0487698  -0.08431825 -0.00787955  0.05069182]\n",
      " [-0.04326233 -0.02254831  0.06744973  0.07123926  0.03839162]\n",
      " [-0.02904803 -0.0486083  -0.07624915  0.08147391  0.08132004]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([ 9, 10,  8, 13])\n",
      "pos_labels = !!!\n",
      "tensor([[ 8, 10],\n",
      "        [ 9, 11],\n",
      "        [ 7,  9],\n",
      "        [12, 14]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 6,  6,  6, 11,  9, 14],\n",
      "        [ 7, 11, 14, 11, 14, 14],\n",
      "        [ 7, 15,  1,  2, 10, 13],\n",
      "        [ 6, 13,  6, 11, 15, 14]])\n",
      "batch_size = 4\n",
      "@@@input_embedding = @@@\n",
      "tensor([[ 0.0777,  0.0881, -0.0741, -0.0375,  0.0024],\n",
      "        [-0.0826,  0.0087, -0.0108, -0.0776, -0.0943],\n",
      "        [-0.0973, -0.0938, -0.0110, -0.0594,  0.0826],\n",
      "        [ 0.0982,  0.0676, -0.0493, -0.0842,  0.0999]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[ 0.0973,  0.0109, -0.0954, -0.0489,  0.0828],\n",
      "         [ 0.0429, -0.0646, -0.0384, -0.0223,  0.0127]],\n",
      "\n",
      "        [[-0.0076,  0.0973,  0.0192, -0.0727, -0.0923],\n",
      "         [ 0.0417, -0.0443,  0.0478,  0.0525,  0.0639]],\n",
      "\n",
      "        [[ 0.0872, -0.0841, -0.0684, -0.0817,  0.0873],\n",
      "         [-0.0076,  0.0973,  0.0192, -0.0727, -0.0923]],\n",
      "\n",
      "        [[ 0.0318, -0.0467,  0.0116, -0.0140, -0.0471],\n",
      "         [-0.0327, -0.0488, -0.0843, -0.0079,  0.0507]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[-0.0047, -0.0164,  0.0521, -0.0915, -0.0442],\n",
      "         [-0.0047, -0.0164,  0.0521, -0.0915, -0.0442],\n",
      "         [-0.0047, -0.0164,  0.0521, -0.0915, -0.0442],\n",
      "         [ 0.0417, -0.0443,  0.0478,  0.0525,  0.0639],\n",
      "         [-0.0076,  0.0973,  0.0192, -0.0727, -0.0923],\n",
      "         [-0.0327, -0.0488, -0.0843, -0.0079,  0.0507]],\n",
      "\n",
      "        [[ 0.0872, -0.0841, -0.0684, -0.0817,  0.0873],\n",
      "         [ 0.0417, -0.0443,  0.0478,  0.0525,  0.0639],\n",
      "         [-0.0327, -0.0488, -0.0843, -0.0079,  0.0507],\n",
      "         [ 0.0417, -0.0443,  0.0478,  0.0525,  0.0639],\n",
      "         [-0.0327, -0.0488, -0.0843, -0.0079,  0.0507],\n",
      "         [-0.0327, -0.0488, -0.0843, -0.0079,  0.0507]],\n",
      "\n",
      "        [[ 0.0872, -0.0841, -0.0684, -0.0817,  0.0873],\n",
      "         [-0.0433, -0.0225,  0.0674,  0.0712,  0.0384],\n",
      "         [ 0.0102, -0.0191, -0.0178,  0.0824, -0.0900],\n",
      "         [ 0.0028, -0.0314,  0.0579,  0.0651,  0.0361],\n",
      "         [ 0.0429, -0.0646, -0.0384, -0.0223,  0.0127],\n",
      "         [ 0.0199, -0.0726, -0.0915,  0.0792,  0.0560]],\n",
      "\n",
      "        [[-0.0047, -0.0164,  0.0521, -0.0915, -0.0442],\n",
      "         [ 0.0199, -0.0726, -0.0915,  0.0792,  0.0560],\n",
      "         [-0.0047, -0.0164,  0.0521, -0.0915, -0.0442],\n",
      "         [ 0.0417, -0.0443,  0.0478,  0.0525,  0.0639],\n",
      "         [-0.0433, -0.0225,  0.0674,  0.0712,  0.0384],\n",
      "         [-0.0327, -0.0488, -0.0843, -0.0079,  0.0507]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[ 0.0777],\n",
      "         [ 0.0881],\n",
      "         [-0.0741],\n",
      "         [-0.0375],\n",
      "         [ 0.0024]],\n",
      "\n",
      "        [[-0.0826],\n",
      "         [ 0.0087],\n",
      "         [-0.0108],\n",
      "         [-0.0776],\n",
      "         [-0.0943]],\n",
      "\n",
      "        [[-0.0973],\n",
      "         [-0.0938],\n",
      "         [-0.0110],\n",
      "         [-0.0594],\n",
      "         [ 0.0826]],\n",
      "\n",
      "        [[ 0.0982],\n",
      "         [ 0.0676],\n",
      "         [-0.0493],\n",
      "         [-0.0842],\n",
      "         [ 0.0999]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[[ 0.0176],\n",
      "         [ 0.0014]],\n",
      "\n",
      "        [[ 0.0156],\n",
      "         [-0.0144]],\n",
      "\n",
      "        [[ 0.0122],\n",
      "         [-0.0119]],\n",
      "\n",
      "        [[-0.0041],\n",
      "         [ 0.0034]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[ 0.0176,  0.0014],\n",
      "        [ 0.0156, -0.0144],\n",
      "        [ 0.0122, -0.0119],\n",
      "        [-0.0041,  0.0034]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[ 0.0023,  0.0023,  0.0023,  0.0060, -0.0091,  0.0002],\n",
      "        [ 0.0091,  0.0144,  0.0010,  0.0144,  0.0010,  0.0010],\n",
      "        [-0.0122, -0.0045,  0.0113, -0.0012, -0.0047, -0.0058],\n",
      "        [ 0.0008, -0.0005,  0.0008, -0.0007,  0.0113, -0.0034]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[ 0.0176,  0.0014],\n",
      "        [ 0.0156, -0.0144],\n",
      "        [ 0.0122, -0.0119],\n",
      "        [-0.0041,  0.0034]], grad_fn=<SqueezeBackward0>)\n",
      "######################log_pos = #######################\n",
      "tensor([-1.3768, -1.3858, -1.3862, -1.3867], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1568, -4.1385, -4.1674, -4.1547], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5382, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.09888365 -0.05281906 -0.09474792 -0.06153099  0.0207892 ]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09734754 -0.09383321 -0.01098482 -0.05939215  0.0826202 ]\n",
      " [ 0.0777399   0.08806259 -0.07410274 -0.03753499  0.00243527]\n",
      " [-0.08257407  0.00867909 -0.01076714 -0.07757157 -0.09425669]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09822249  0.06757515 -0.04931423 -0.08419118  0.09988285]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01199113  0.07522831 -0.04082245  0.07679392  0.06571294]\n",
      " [ 0.01022371 -0.0190751  -0.01778902  0.08240167 -0.08997601]\n",
      " [ 0.00281086 -0.03144292  0.0579103   0.0650977   0.03612096]\n",
      " [-0.01699578  0.04219782  0.0055786   0.07955068  0.05962858]\n",
      " [ 0.00384843  0.07735284 -0.00914419 -0.00084515 -0.06769786]\n",
      " [-0.03311238  0.00499618 -0.02617722 -0.02599635 -0.0262756 ]\n",
      " [-0.00472637 -0.01640756  0.05205587 -0.09148521 -0.04415226]\n",
      " [ 0.08724033 -0.08412921 -0.06842993 -0.08172169  0.08729339]\n",
      " [ 0.09731022  0.01086201 -0.09544205 -0.04891849  0.08281042]\n",
      " [-0.00756395  0.09731952  0.01922144 -0.07271779 -0.09230264]\n",
      " [ 0.04294194 -0.06460427 -0.03838624 -0.02225508  0.01273111]\n",
      " [ 0.04165604 -0.04429905  0.04781045  0.05254621  0.0638983 ]\n",
      " [ 0.03180239 -0.04673187  0.01157057 -0.01399213 -0.04707431]\n",
      " [ 0.01991266 -0.07261399 -0.09147868  0.07921045  0.0559635 ]\n",
      " [-0.03268033 -0.0487698  -0.08431825 -0.00787955  0.05069182]\n",
      " [-0.04326233 -0.02254831  0.06744973  0.07123926  0.03839162]\n",
      " [-0.02904803 -0.0486083  -0.07624915  0.08147391  0.08132004]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.09888365 -0.05281906 -0.09474792 -0.06153099  0.0207892 ]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01199113  0.07522831 -0.04082245  0.07679392  0.06571294]\n",
      " [ 0.0126436  -0.01674256 -0.01751596  0.08387806 -0.09202981]\n",
      " [ 0.00524596 -0.02909573  0.05818509  0.06658336  0.03405426]\n",
      " [-0.01699578  0.04219782  0.0055786   0.07955068  0.05962858]\n",
      " [ 0.00384843  0.07735284 -0.00914419 -0.00084515 -0.06769786]\n",
      " [-0.03311238  0.00499618 -0.02617722 -0.02599635 -0.0262756 ]\n",
      " [-0.01545908 -0.02638184  0.06007173 -0.08446561 -0.04932672]\n",
      " [ 0.08932504 -0.08431654 -0.06815862 -0.07977306  0.08961386]\n",
      " [ 0.09923659  0.01304417 -0.09727829 -0.04984859  0.08287077]\n",
      " [-0.01401269  0.09296346  0.02053907 -0.07519297 -0.09262402]\n",
      " [ 0.04732349 -0.06005289 -0.03996228 -0.02170454  0.01072162]\n",
      " [ 0.03928162 -0.04839624  0.05115408  0.05748456  0.06364504]\n",
      " [ 0.03426302 -0.045039    0.01033517 -0.01610125 -0.04457209]\n",
      " [ 0.01989726 -0.07195114 -0.0899701   0.08280484  0.05139433]\n",
      " [-0.02844195 -0.0516275  -0.08165453 -0.00111914  0.05768828]\n",
      " [-0.04326487 -0.02187704  0.06895089  0.07482035  0.03383842]\n",
      " [-0.02904803 -0.0486083  -0.07624915  0.08147391  0.08132004]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "input_labels = !!!\n",
      "tensor([15,  0,  6,  5])\n",
      "pos_labels = !!!\n",
      "tensor([[14, 16],\n",
      "        [16,  1],\n",
      "        [ 0,  7],\n",
      "        [ 4,  0]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 4, 11,  0,  7,  8,  2],\n",
      "        [ 5, 10,  0,  6,  7, 13],\n",
      "        [ 2, 13,  4,  1,  1,  4],\n",
      "        [ 4, 16,  0,  8,  8, 11]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.09888365 -0.05281906 -0.09474792 -0.06153099  0.0207892 ]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01199113  0.07522831 -0.04082245  0.07679392  0.06571294]\n",
      " [ 0.0126436  -0.01674256 -0.01751596  0.08387806 -0.09202981]\n",
      " [ 0.00524596 -0.02909573  0.05818509  0.06658336  0.03405426]\n",
      " [-0.01699578  0.04219782  0.0055786   0.07955068  0.05962858]\n",
      " [ 0.00384843  0.07735284 -0.00914419 -0.00084515 -0.06769786]\n",
      " [-0.03311238  0.00499618 -0.02617722 -0.02599635 -0.0262756 ]\n",
      " [-0.01545908 -0.02638184  0.06007173 -0.08446561 -0.04932672]\n",
      " [ 0.08932504 -0.08431654 -0.06815862 -0.07977306  0.08961386]\n",
      " [ 0.09923659  0.01304417 -0.09727829 -0.04984859  0.08287077]\n",
      " [-0.01401269  0.09296346  0.02053907 -0.07519297 -0.09262402]\n",
      " [ 0.04732349 -0.06005289 -0.03996228 -0.02170454  0.01072162]\n",
      " [ 0.03928162 -0.04839624  0.05115408  0.05748456  0.06364504]\n",
      " [ 0.03426302 -0.045039    0.01033517 -0.01610125 -0.04457209]\n",
      " [ 0.01989726 -0.07195114 -0.0899701   0.08280484  0.05139433]\n",
      " [-0.02844195 -0.0516275  -0.08165453 -0.00111914  0.05768828]\n",
      " [-0.04326487 -0.02187704  0.06895089  0.07482035  0.03383842]\n",
      " [-0.02904803 -0.0486083  -0.07624915  0.08147391  0.08132004]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([15,  0,  6,  5])\n",
      "pos_labels = !!!\n",
      "tensor([[14, 16],\n",
      "        [16,  1],\n",
      "        [ 0,  7],\n",
      "        [ 4,  0]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 4, 11,  0,  7,  8,  2],\n",
      "        [ 5, 10,  0,  6,  7, 13],\n",
      "        [ 2, 13,  4,  1,  1,  4],\n",
      "        [ 4, 16,  0,  8,  8, 11]])\n",
      "batch_size = 4\n",
      "@@@input_embedding = @@@\n",
      "tensor([[-0.0275, -0.0261, -0.0184,  0.0803, -0.0522],\n",
      "        [-0.0989, -0.0528, -0.0947, -0.0615,  0.0208],\n",
      "        [ 0.0163,  0.0835, -0.0322, -0.0180, -0.0363],\n",
      "        [-0.0692,  0.0585,  0.0740,  0.0007,  0.0132]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[-0.0284, -0.0516, -0.0817, -0.0011,  0.0577],\n",
      "         [-0.0290, -0.0486, -0.0762,  0.0815,  0.0813]],\n",
      "\n",
      "        [[-0.0290, -0.0486, -0.0762,  0.0815,  0.0813],\n",
      "         [ 0.0126, -0.0167, -0.0175,  0.0839, -0.0920]],\n",
      "\n",
      "        [[ 0.0120,  0.0752, -0.0408,  0.0768,  0.0657],\n",
      "         [ 0.0893, -0.0843, -0.0682, -0.0798,  0.0896]],\n",
      "\n",
      "        [[ 0.0038,  0.0774, -0.0091, -0.0008, -0.0677],\n",
      "         [ 0.0120,  0.0752, -0.0408,  0.0768,  0.0657]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[ 0.0038,  0.0774, -0.0091, -0.0008, -0.0677],\n",
      "         [ 0.0393, -0.0484,  0.0512,  0.0575,  0.0636],\n",
      "         [ 0.0120,  0.0752, -0.0408,  0.0768,  0.0657],\n",
      "         [ 0.0893, -0.0843, -0.0682, -0.0798,  0.0896],\n",
      "         [ 0.0992,  0.0130, -0.0973, -0.0498,  0.0829],\n",
      "         [ 0.0052, -0.0291,  0.0582,  0.0666,  0.0341]],\n",
      "\n",
      "        [[-0.0331,  0.0050, -0.0262, -0.0260, -0.0263],\n",
      "         [ 0.0473, -0.0601, -0.0400, -0.0217,  0.0107],\n",
      "         [ 0.0120,  0.0752, -0.0408,  0.0768,  0.0657],\n",
      "         [-0.0155, -0.0264,  0.0601, -0.0845, -0.0493],\n",
      "         [ 0.0893, -0.0843, -0.0682, -0.0798,  0.0896],\n",
      "         [ 0.0199, -0.0720, -0.0900,  0.0828,  0.0514]],\n",
      "\n",
      "        [[ 0.0052, -0.0291,  0.0582,  0.0666,  0.0341],\n",
      "         [ 0.0199, -0.0720, -0.0900,  0.0828,  0.0514],\n",
      "         [ 0.0038,  0.0774, -0.0091, -0.0008, -0.0677],\n",
      "         [ 0.0126, -0.0167, -0.0175,  0.0839, -0.0920],\n",
      "         [ 0.0126, -0.0167, -0.0175,  0.0839, -0.0920],\n",
      "         [ 0.0038,  0.0774, -0.0091, -0.0008, -0.0677]],\n",
      "\n",
      "        [[ 0.0038,  0.0774, -0.0091, -0.0008, -0.0677],\n",
      "         [-0.0290, -0.0486, -0.0762,  0.0815,  0.0813],\n",
      "         [ 0.0120,  0.0752, -0.0408,  0.0768,  0.0657],\n",
      "         [ 0.0992,  0.0130, -0.0973, -0.0498,  0.0829],\n",
      "         [ 0.0992,  0.0130, -0.0973, -0.0498,  0.0829],\n",
      "         [ 0.0393, -0.0484,  0.0512,  0.0575,  0.0636]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[-0.0275],\n",
      "         [-0.0261],\n",
      "         [-0.0184],\n",
      "         [ 0.0803],\n",
      "         [-0.0522]],\n",
      "\n",
      "        [[-0.0989],\n",
      "         [-0.0528],\n",
      "         [-0.0947],\n",
      "         [-0.0615],\n",
      "         [ 0.0208]],\n",
      "\n",
      "        [[ 0.0163],\n",
      "         [ 0.0835],\n",
      "         [-0.0322],\n",
      "         [-0.0180],\n",
      "         [-0.0363]],\n",
      "\n",
      "        [[-0.0692],\n",
      "         [ 0.0585],\n",
      "         [ 0.0740],\n",
      "         [ 0.0007],\n",
      "         [ 0.0132]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[[ 0.0005],\n",
      "         [ 0.0058]],\n",
      "\n",
      "        [[ 0.0093],\n",
      "         [-0.0058]],\n",
      "\n",
      "        [[ 0.0040],\n",
      "         [-0.0052]],\n",
      "\n",
      "        [[ 0.0027],\n",
      "         [ 0.0015]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[ 0.0005,  0.0058],\n",
      "        [ 0.0093, -0.0058],\n",
      "        [ 0.0040, -0.0052],\n",
      "        [ 0.0027,  0.0015]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[-0.0015, -0.0005, -0.0012,  0.0101,  0.0096, -0.0031],\n",
      "        [-0.0065, -0.0038,  0.0047, -0.0014, -0.0089, -0.0063],\n",
      "        [ 0.0067,  0.0061, -0.0093, -0.0012, -0.0012, -0.0093],\n",
      "        [-0.0027,  0.0053, -0.0015,  0.0122,  0.0122,  0.0009]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[ 0.0005,  0.0058],\n",
      "        [ 0.0093, -0.0058],\n",
      "        [ 0.0040, -0.0052],\n",
      "        [ 0.0027,  0.0015]], grad_fn=<SqueezeBackward0>)\n",
      "######################log_pos = #######################\n",
      "tensor([-1.3832, -1.3845, -1.3869, -1.3842], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1522, -4.1701, -4.1630, -4.1457], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5424, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.09888365 -0.05281906 -0.09474792 -0.06153099  0.0207892 ]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.0691928   0.05849885  0.07399582  0.00072781  0.01321235]\n",
      " [ 0.01627956  0.08347691 -0.03223564 -0.01795088 -0.03631908]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.02747226 -0.02614786 -0.01838891  0.08027954 -0.05224437]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01199113  0.07522831 -0.04082245  0.07679392  0.06571294]\n",
      " [ 0.0126436  -0.01674256 -0.01751596  0.08387806 -0.09202981]\n",
      " [ 0.00524596 -0.02909573  0.05818509  0.06658336  0.03405426]\n",
      " [-0.01699578  0.04219782  0.0055786   0.07955068  0.05962858]\n",
      " [ 0.00384843  0.07735284 -0.00914419 -0.00084515 -0.06769786]\n",
      " [-0.03311238  0.00499618 -0.02617722 -0.02599635 -0.0262756 ]\n",
      " [-0.01545908 -0.02638184  0.06007173 -0.08446561 -0.04932672]\n",
      " [ 0.08932504 -0.08431654 -0.06815862 -0.07977306  0.08961386]\n",
      " [ 0.09923659  0.01304417 -0.09727829 -0.04984859  0.08287077]\n",
      " [-0.01401269  0.09296346  0.02053907 -0.07519297 -0.09262402]\n",
      " [ 0.04732349 -0.06005289 -0.03996228 -0.02170454  0.01072162]\n",
      " [ 0.03928162 -0.04839624  0.05115408  0.05748456  0.06364504]\n",
      " [ 0.03426302 -0.045039    0.01033517 -0.01610125 -0.04457209]\n",
      " [ 0.01989726 -0.07195114 -0.0899701   0.08280484  0.05139433]\n",
      " [-0.02844195 -0.0516275  -0.08165453 -0.00111914  0.05768828]\n",
      " [-0.04326487 -0.02187704  0.06895089  0.07482035  0.03383842]\n",
      " [-0.02904803 -0.0486083  -0.07624915  0.08147391  0.08132004]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.10229864 -0.05036409 -0.09194364 -0.05608074  0.01695164]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01736518  0.08271069 -0.03283432 -0.02591122 -0.02655793]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01555341  0.07928037 -0.03880626  0.07587254  0.06559467]\n",
      " [ 0.00934989 -0.02224323 -0.01827874  0.08323342 -0.08969153]\n",
      " [ 0.0055282  -0.030521    0.05944873  0.06502052  0.03626736]\n",
      " [-0.01699578  0.04219782  0.0055786   0.07955068  0.05962858]\n",
      " [ 0.00372265  0.07380987 -0.00706983 -0.00195199 -0.06456727]\n",
      " [-0.0306322   0.00632097 -0.02380077 -0.02445305 -0.02679704]\n",
      " [-0.01298525 -0.02506044  0.06244209 -0.08292626 -0.04984681]\n",
      " [ 0.09289946 -0.08024745 -0.06613003 -0.08067479  0.08948102]\n",
      " [ 0.10335855  0.01078768 -0.10049792 -0.05188211  0.08351403]\n",
      " [-0.01401269  0.09296346  0.02053907 -0.07519297 -0.09262402]\n",
      " [ 0.04980033 -0.05872988 -0.03758904 -0.02016332  0.01020089]\n",
      " [ 0.04169767 -0.04920419  0.04976485  0.05545885  0.06462133]\n",
      " [ 0.03426302 -0.045039    0.01033517 -0.01610125 -0.04457209]\n",
      " [ 0.02197143 -0.072707   -0.08679049  0.08479539  0.05177815]\n",
      " [-0.02912858 -0.05228103 -0.08211414  0.00088732  0.05638252]\n",
      " [-0.04326487 -0.02187704  0.06895089  0.07482035  0.03383842]\n",
      " [-0.0304682  -0.05203299 -0.08091014  0.08192588  0.08020557]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "input_labels = !!!\n",
      "tensor([12,  7])\n",
      "pos_labels = !!!\n",
      "tensor([[11, 13],\n",
      "        [ 6,  8]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 2,  5, 10, 16,  4,  8],\n",
      "        [ 3, 10,  8,  5, 15,  7]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.10229864 -0.05036409 -0.09194364 -0.05608074  0.01695164]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01736518  0.08271069 -0.03283432 -0.02591122 -0.02655793]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01555341  0.07928037 -0.03880626  0.07587254  0.06559467]\n",
      " [ 0.00934989 -0.02224323 -0.01827874  0.08323342 -0.08969153]\n",
      " [ 0.0055282  -0.030521    0.05944873  0.06502052  0.03626736]\n",
      " [-0.01699578  0.04219782  0.0055786   0.07955068  0.05962858]\n",
      " [ 0.00372265  0.07380987 -0.00706983 -0.00195199 -0.06456727]\n",
      " [-0.0306322   0.00632097 -0.02380077 -0.02445305 -0.02679704]\n",
      " [-0.01298525 -0.02506044  0.06244209 -0.08292626 -0.04984681]\n",
      " [ 0.09289946 -0.08024745 -0.06613003 -0.08067479  0.08948102]\n",
      " [ 0.10335855  0.01078768 -0.10049792 -0.05188211  0.08351403]\n",
      " [-0.01401269  0.09296346  0.02053907 -0.07519297 -0.09262402]\n",
      " [ 0.04980033 -0.05872988 -0.03758904 -0.02016332  0.01020089]\n",
      " [ 0.04169767 -0.04920419  0.04976485  0.05545885  0.06462133]\n",
      " [ 0.03426302 -0.045039    0.01033517 -0.01610125 -0.04457209]\n",
      " [ 0.02197143 -0.072707   -0.08679049  0.08479539  0.05177815]\n",
      " [-0.02912858 -0.05228103 -0.08211414  0.00088732  0.05638252]\n",
      " [-0.04326487 -0.02187704  0.06895089  0.07482035  0.03383842]\n",
      " [-0.0304682  -0.05203299 -0.08091014  0.08192588  0.08020557]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([12,  7])\n",
      "pos_labels = !!!\n",
      "tensor([[11, 13],\n",
      "        [ 6,  8]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 2,  5, 10, 16,  4,  8],\n",
      "        [ 3, 10,  8,  5, 15,  7]])\n",
      "batch_size = 2\n",
      "@@@input_embedding = @@@\n",
      "tensor([[ 0.0994, -0.0436, -0.0498, -0.0341,  0.0926],\n",
      "        [ 0.0866, -0.0006,  0.0957,  0.0752, -0.0553]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[ 0.0417, -0.0492,  0.0498,  0.0555,  0.0646],\n",
      "         [ 0.0220, -0.0727, -0.0868,  0.0848,  0.0518]],\n",
      "\n",
      "        [[-0.0130, -0.0251,  0.0624, -0.0829, -0.0498],\n",
      "         [ 0.1034,  0.0108, -0.1005, -0.0519,  0.0835]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[ 0.0055, -0.0305,  0.0594,  0.0650,  0.0363],\n",
      "         [-0.0306,  0.0063, -0.0238, -0.0245, -0.0268],\n",
      "         [ 0.0498, -0.0587, -0.0376, -0.0202,  0.0102],\n",
      "         [-0.0305, -0.0520, -0.0809,  0.0819,  0.0802],\n",
      "         [ 0.0037,  0.0738, -0.0071, -0.0020, -0.0646],\n",
      "         [ 0.1034,  0.0108, -0.1005, -0.0519,  0.0835]],\n",
      "\n",
      "        [[-0.0170,  0.0422,  0.0056,  0.0796,  0.0596],\n",
      "         [ 0.0498, -0.0587, -0.0376, -0.0202,  0.0102],\n",
      "         [ 0.1034,  0.0108, -0.1005, -0.0519,  0.0835],\n",
      "         [-0.0306,  0.0063, -0.0238, -0.0245, -0.0268],\n",
      "         [-0.0433, -0.0219,  0.0690,  0.0748,  0.0338],\n",
      "         [ 0.0929, -0.0802, -0.0661, -0.0807,  0.0895]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[ 0.0994],\n",
      "         [-0.0436],\n",
      "         [-0.0498],\n",
      "         [-0.0341],\n",
      "         [ 0.0926]],\n",
      "\n",
      "        [[ 0.0866],\n",
      "         [-0.0006],\n",
      "         [ 0.0957],\n",
      "         [ 0.0752],\n",
      "         [-0.0553]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[[ 0.0079],\n",
      "         [ 0.0116]],\n",
      "\n",
      "        [[ 0.0014],\n",
      "         [-0.0092]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[ 0.0079,  0.0116],\n",
      "        [ 0.0014, -0.0092]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[-6.7011e-05,  3.7863e-03, -1.1015e-02, -7.9040e-03,  8.4115e-03,\n",
      "         -2.4310e-02],\n",
      "        [-1.7170e-03,  1.3298e-03,  9.1963e-03,  5.2872e-03, -6.6162e-03,\n",
      "          9.2534e-03]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[ 0.0079,  0.0116],\n",
      "        [ 0.0014, -0.0092]], grad_fn=<SqueezeBackward0>)\n",
      "######################log_pos = #######################\n",
      "tensor([-1.3766, -1.3902], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1745, -4.1505], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5459, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.10229864 -0.05036409 -0.09194364 -0.05608074  0.01695164]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01736518  0.08271069 -0.03283432 -0.02591122 -0.02655793]\n",
      " [ 0.08655737 -0.00060815  0.09565913  0.0751775  -0.05534681]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09942765 -0.04361018 -0.04975622 -0.03407041  0.09263287]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01555341  0.07928037 -0.03880626  0.07587254  0.06559467]\n",
      " [ 0.00934989 -0.02224323 -0.01827874  0.08323342 -0.08969153]\n",
      " [ 0.0055282  -0.030521    0.05944873  0.06502052  0.03626736]\n",
      " [-0.01699578  0.04219782  0.0055786   0.07955068  0.05962858]\n",
      " [ 0.00372265  0.07380987 -0.00706983 -0.00195199 -0.06456727]\n",
      " [-0.0306322   0.00632097 -0.02380077 -0.02445305 -0.02679704]\n",
      " [-0.01298525 -0.02506044  0.06244209 -0.08292626 -0.04984681]\n",
      " [ 0.09289946 -0.08024745 -0.06613003 -0.08067479  0.08948102]\n",
      " [ 0.10335855  0.01078768 -0.10049792 -0.05188211  0.08351403]\n",
      " [-0.01401269  0.09296346  0.02053907 -0.07519297 -0.09262402]\n",
      " [ 0.04980033 -0.05872988 -0.03758904 -0.02016332  0.01020089]\n",
      " [ 0.04169767 -0.04920419  0.04976485  0.05545885  0.06462133]\n",
      " [ 0.03426302 -0.045039    0.01033517 -0.01610125 -0.04457209]\n",
      " [ 0.02197143 -0.072707   -0.08679049  0.08479539  0.05177815]\n",
      " [-0.02912858 -0.05228103 -0.08211414  0.00088732  0.05638252]\n",
      " [-0.04326487 -0.02187704  0.06895089  0.07482035  0.03383842]\n",
      " [-0.0304682  -0.05203299 -0.08091014  0.08192588  0.08020557]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.10229864 -0.05036409 -0.09194364 -0.05608074  0.01695164]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01736518  0.08271069 -0.03283432 -0.02591122 -0.02655793]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09745839 -0.04712062 -0.04198769 -0.02949995  0.09239831]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01555341  0.07928037 -0.03880626  0.07587254  0.06559467]\n",
      " [ 0.00934989 -0.02224323 -0.01827874  0.08323342 -0.08969153]\n",
      " [ 0.00055665 -0.02834042  0.06193663  0.0667241   0.03163556]\n",
      " [-0.02132736  0.04222826  0.00079154  0.07578858  0.06239829]\n",
      " [-0.00122783  0.07598121 -0.00459248 -0.00025564 -0.06917943]\n",
      " [-0.0399106   0.00852768 -0.02608798 -0.02650169 -0.02865989]\n",
      " [-0.00866039 -0.02509082  0.06722172 -0.07917    -0.05261223]\n",
      " [ 0.08859162 -0.08021719 -0.07089085 -0.08441627  0.09223556]\n",
      " [ 0.09836654  0.01299442 -0.09793589 -0.05012331  0.07880064]\n",
      " [-0.01401269  0.09296346  0.02053907 -0.07519297 -0.09262402]\n",
      " [ 0.04047657 -0.05650697 -0.03986731 -0.02220679  0.00830924]\n",
      " [ 0.04664939 -0.05137607  0.04728688  0.05376207  0.06923465]\n",
      " [ 0.03426302 -0.045039    0.01033517 -0.01610125 -0.04457209]\n",
      " [ 0.02691403 -0.07487489 -0.08926389  0.08310173  0.05638297]\n",
      " [-0.02912858 -0.05228103 -0.08211414  0.00088732  0.05638252]\n",
      " [-0.04760705 -0.02184653  0.06415211  0.07104903  0.03661492]\n",
      " [-0.03545923 -0.04984386 -0.0784125   0.08363613  0.07555562]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "input_labels = !!!\n",
      "tensor([ 3, 12,  0,  4])\n",
      "pos_labels = !!!\n",
      "tensor([[ 2,  4],\n",
      "        [11, 13],\n",
      "        [ 5,  6],\n",
      "        [ 3,  5]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 0,  4, 10,  1,  5,  5],\n",
      "        [ 3,  3, 13,  1,  8, 12],\n",
      "        [ 8, 15,  9, 10, 13,  3],\n",
      "        [15,  1,  6,  7, 12,  7]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.10229864 -0.05036409 -0.09194364 -0.05608074  0.01695164]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01736518  0.08271069 -0.03283432 -0.02591122 -0.02655793]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09745839 -0.04712062 -0.04198769 -0.02949995  0.09239831]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01555341  0.07928037 -0.03880626  0.07587254  0.06559467]\n",
      " [ 0.00934989 -0.02224323 -0.01827874  0.08323342 -0.08969153]\n",
      " [ 0.00055665 -0.02834042  0.06193663  0.0667241   0.03163556]\n",
      " [-0.02132736  0.04222826  0.00079154  0.07578858  0.06239829]\n",
      " [-0.00122783  0.07598121 -0.00459248 -0.00025564 -0.06917943]\n",
      " [-0.0399106   0.00852768 -0.02608798 -0.02650169 -0.02865989]\n",
      " [-0.00866039 -0.02509082  0.06722172 -0.07917    -0.05261223]\n",
      " [ 0.08859162 -0.08021719 -0.07089085 -0.08441627  0.09223556]\n",
      " [ 0.09836654  0.01299442 -0.09793589 -0.05012331  0.07880064]\n",
      " [-0.01401269  0.09296346  0.02053907 -0.07519297 -0.09262402]\n",
      " [ 0.04047657 -0.05650697 -0.03986731 -0.02220679  0.00830924]\n",
      " [ 0.04664939 -0.05137607  0.04728688  0.05376207  0.06923465]\n",
      " [ 0.03426302 -0.045039    0.01033517 -0.01610125 -0.04457209]\n",
      " [ 0.02691403 -0.07487489 -0.08926389  0.08310173  0.05638297]\n",
      " [-0.02912858 -0.05228103 -0.08211414  0.00088732  0.05638252]\n",
      " [-0.04760705 -0.02184653  0.06415211  0.07104903  0.03661492]\n",
      " [-0.03545923 -0.04984386 -0.0784125   0.08363613  0.07555562]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([ 3, 12,  0,  4])\n",
      "pos_labels = !!!\n",
      "tensor([[ 2,  4],\n",
      "        [11, 13],\n",
      "        [ 5,  6],\n",
      "        [ 3,  5]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 0,  4, 10,  1,  5,  5],\n",
      "        [ 3,  3, 13,  1,  8, 12],\n",
      "        [ 8, 15,  9, 10, 13,  3],\n",
      "        [15,  1,  6,  7, 12,  7]])\n",
      "batch_size = 4\n",
      "@@@input_embedding = @@@\n",
      "tensor([[-0.0433,  0.1004, -0.0414, -0.0468,  0.0368],\n",
      "        [ 0.0975, -0.0471, -0.0420, -0.0295,  0.0924],\n",
      "        [-0.1023, -0.0504, -0.0919, -0.0561,  0.0170],\n",
      "        [ 0.0804, -0.0579,  0.0971, -0.0268,  0.0003]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[ 0.0006, -0.0283,  0.0619,  0.0667,  0.0316],\n",
      "         [-0.0012,  0.0760, -0.0046, -0.0003, -0.0692]],\n",
      "\n",
      "        [[ 0.0466, -0.0514,  0.0473,  0.0538,  0.0692],\n",
      "         [ 0.0269, -0.0749, -0.0893,  0.0831,  0.0564]],\n",
      "\n",
      "        [[-0.0399,  0.0085, -0.0261, -0.0265, -0.0287],\n",
      "         [-0.0087, -0.0251,  0.0672, -0.0792, -0.0526]],\n",
      "\n",
      "        [[-0.0213,  0.0422,  0.0008,  0.0758,  0.0624],\n",
      "         [-0.0399,  0.0085, -0.0261, -0.0265, -0.0287]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[ 0.0156,  0.0793, -0.0388,  0.0759,  0.0656],\n",
      "         [-0.0012,  0.0760, -0.0046, -0.0003, -0.0692],\n",
      "         [ 0.0405, -0.0565, -0.0399, -0.0222,  0.0083],\n",
      "         [ 0.0093, -0.0222, -0.0183,  0.0832, -0.0897],\n",
      "         [-0.0399,  0.0085, -0.0261, -0.0265, -0.0287],\n",
      "         [-0.0399,  0.0085, -0.0261, -0.0265, -0.0287]],\n",
      "\n",
      "        [[-0.0213,  0.0422,  0.0008,  0.0758,  0.0624],\n",
      "         [-0.0213,  0.0422,  0.0008,  0.0758,  0.0624],\n",
      "         [ 0.0269, -0.0749, -0.0893,  0.0831,  0.0564],\n",
      "         [ 0.0093, -0.0222, -0.0183,  0.0832, -0.0897],\n",
      "         [ 0.0984,  0.0130, -0.0979, -0.0501,  0.0788],\n",
      "         [ 0.0343, -0.0450,  0.0103, -0.0161, -0.0446]],\n",
      "\n",
      "        [[ 0.0984,  0.0130, -0.0979, -0.0501,  0.0788],\n",
      "         [-0.0476, -0.0218,  0.0642,  0.0710,  0.0366],\n",
      "         [-0.0140,  0.0930,  0.0205, -0.0752, -0.0926],\n",
      "         [ 0.0405, -0.0565, -0.0399, -0.0222,  0.0083],\n",
      "         [ 0.0269, -0.0749, -0.0893,  0.0831,  0.0564],\n",
      "         [-0.0213,  0.0422,  0.0008,  0.0758,  0.0624]],\n",
      "\n",
      "        [[-0.0476, -0.0218,  0.0642,  0.0710,  0.0366],\n",
      "         [ 0.0093, -0.0222, -0.0183,  0.0832, -0.0897],\n",
      "         [-0.0087, -0.0251,  0.0672, -0.0792, -0.0526],\n",
      "         [ 0.0886, -0.0802, -0.0709, -0.0844,  0.0922],\n",
      "         [ 0.0343, -0.0450,  0.0103, -0.0161, -0.0446],\n",
      "         [ 0.0886, -0.0802, -0.0709, -0.0844,  0.0922]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[-0.0433],\n",
      "         [ 0.1004],\n",
      "         [-0.0414],\n",
      "         [-0.0468],\n",
      "         [ 0.0368]],\n",
      "\n",
      "        [[ 0.0975],\n",
      "         [-0.0471],\n",
      "         [-0.0420],\n",
      "         [-0.0295],\n",
      "         [ 0.0924]],\n",
      "\n",
      "        [[-0.1023],\n",
      "         [-0.0504],\n",
      "         [-0.0919],\n",
      "         [-0.0561],\n",
      "         [ 0.0170]],\n",
      "\n",
      "        [[ 0.0804],\n",
      "         [-0.0579],\n",
      "         [ 0.0971],\n",
      "         [-0.0268],\n",
      "         [ 0.0003]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0074],\n",
      "         [ 0.0053]],\n",
      "\n",
      "        [[ 0.0098],\n",
      "         [ 0.0127]],\n",
      "\n",
      "        [[ 0.0071],\n",
      "         [-0.0005]],\n",
      "\n",
      "        [[-0.0061],\n",
      "         [-0.0055]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[-0.0074,  0.0053],\n",
      "        [ 0.0098,  0.0127],\n",
      "        [ 0.0071, -0.0005],\n",
      "        [-0.0061, -0.0055]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[-0.0078, -0.0053,  0.0044,  0.0091, -0.0039, -0.0039],\n",
      "        [ 0.0006,  0.0006, -0.0127,  0.0080, -0.0218, -0.0014],\n",
      "        [-0.0024,  0.0033,  0.0025, -0.0038, -0.0055,  0.0032],\n",
      "        [-0.0018,  0.0020, -0.0094, -0.0072, -0.0068, -0.0072]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[-0.0074,  0.0053],\n",
      "        [ 0.0098,  0.0127],\n",
      "        [ 0.0071, -0.0005],\n",
      "        [-0.0061, -0.0055]], grad_fn=<SqueezeBackward0>)\n",
      "######################log_pos = #######################\n",
      "tensor([-1.3873, -1.3751, -1.3830, -1.3921], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1626, -4.1723, -4.1603, -4.1741], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5517, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.10229864 -0.05036409 -0.09194364 -0.05608074  0.01695164]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.0433238   0.10042502 -0.04139317 -0.04680085  0.0368204 ]\n",
      " [ 0.08044905 -0.05788083  0.0971237  -0.02681437  0.0003005 ]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01736518  0.08271069 -0.03283432 -0.02591122 -0.02655793]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09745839 -0.04712062 -0.04198769 -0.02949995  0.09239831]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01555341  0.07928037 -0.03880626  0.07587254  0.06559467]\n",
      " [ 0.00934989 -0.02224323 -0.01827874  0.08323342 -0.08969153]\n",
      " [ 0.00055665 -0.02834042  0.06193663  0.0667241   0.03163556]\n",
      " [-0.02132736  0.04222826  0.00079154  0.07578858  0.06239829]\n",
      " [-0.00122783  0.07598121 -0.00459248 -0.00025564 -0.06917943]\n",
      " [-0.0399106   0.00852768 -0.02608798 -0.02650169 -0.02865989]\n",
      " [-0.00866039 -0.02509082  0.06722172 -0.07917    -0.05261223]\n",
      " [ 0.08859162 -0.08021719 -0.07089085 -0.08441627  0.09223556]\n",
      " [ 0.09836654  0.01299442 -0.09793589 -0.05012331  0.07880064]\n",
      " [-0.01401269  0.09296346  0.02053907 -0.07519297 -0.09262402]\n",
      " [ 0.04047657 -0.05650697 -0.03986731 -0.02220679  0.00830924]\n",
      " [ 0.04664939 -0.05137607  0.04728688  0.05376207  0.06923465]\n",
      " [ 0.03426302 -0.045039    0.01033517 -0.01610125 -0.04457209]\n",
      " [ 0.02691403 -0.07487489 -0.08926389  0.08310173  0.05638297]\n",
      " [-0.02912858 -0.05228103 -0.08211414  0.00088732  0.05638252]\n",
      " [-0.04760705 -0.02184653  0.06415211  0.07104903  0.03661492]\n",
      " [-0.03545923 -0.04984386 -0.0784125   0.08363613  0.07555562]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.1055897  -0.0506419  -0.08735865 -0.06078062  0.01116944]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01736518  0.08271069 -0.03283432 -0.02591122 -0.02655793]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.0166407   0.07676    -0.03776741  0.0770471   0.06467059]\n",
      " [ 0.00599215 -0.02212358 -0.0186288   0.08580237 -0.09291606]\n",
      " [-0.00053044 -0.02582051  0.06089797  0.06554975  0.03255947]\n",
      " [-0.02162817  0.04438926  0.00762072  0.07799052  0.05736412]\n",
      " [-0.00122205  0.0759678  -0.00458695 -0.00024939 -0.06918435]\n",
      " [-0.03827189  0.00079108 -0.02387001 -0.02622643 -0.03007462]\n",
      " [-0.01323914 -0.02489641  0.06248308 -0.07989885 -0.05219588]\n",
      " [ 0.08455473 -0.07731277 -0.07576446 -0.08307074  0.09222048]\n",
      " [ 0.09846405  0.01544593 -0.09457334 -0.04797403  0.07604115]\n",
      " [-0.01145841  0.09422099  0.0228348  -0.0737927  -0.09304728]\n",
      " [ 0.04411954 -0.05775057 -0.03653186 -0.01963471  0.00696618]\n",
      " [ 0.04907392 -0.05254832  0.04624232  0.05302818  0.07153329]\n",
      " [ 0.02980683 -0.04240824  0.00894926 -0.01469061 -0.04689118]\n",
      " [ 0.02944772 -0.07359739 -0.08694568  0.08451695  0.05592877]\n",
      " [-0.02912858 -0.05228103 -0.08211414  0.00088732  0.05638252]\n",
      " [-0.0470668  -0.0191412   0.06401668  0.0731197   0.03618431]\n",
      " [-0.03545923 -0.04984386 -0.0784125   0.08363613  0.07555562]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "epoch: 1, iter: 0, loss: 5.551693916320801\n",
      "input_labels = !!!\n",
      "tensor([ 2, 13,  6, 11])\n",
      "pos_labels = !!!\n",
      "tensor([[ 1,  3],\n",
      "        [12, 14],\n",
      "        [ 0,  7],\n",
      "        [10, 12]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 0,  5, 10,  7, 11,  2],\n",
      "        [ 2,  9,  2,  6,  1, 15],\n",
      "        [ 8, 13, 12,  4, 15,  8],\n",
      "        [10, 14, 15,  3,  6, 15]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.1055897  -0.0506419  -0.08735865 -0.06078062  0.01116944]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01736518  0.08271069 -0.03283432 -0.02591122 -0.02655793]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.0166407   0.07676    -0.03776741  0.0770471   0.06467059]\n",
      " [ 0.00599215 -0.02212358 -0.0186288   0.08580237 -0.09291606]\n",
      " [-0.00053044 -0.02582051  0.06089797  0.06554975  0.03255947]\n",
      " [-0.02162817  0.04438926  0.00762072  0.07799052  0.05736412]\n",
      " [-0.00122205  0.0759678  -0.00458695 -0.00024939 -0.06918435]\n",
      " [-0.03827189  0.00079108 -0.02387001 -0.02622643 -0.03007462]\n",
      " [-0.01323914 -0.02489641  0.06248308 -0.07989885 -0.05219588]\n",
      " [ 0.08455473 -0.07731277 -0.07576446 -0.08307074  0.09222048]\n",
      " [ 0.09846405  0.01544593 -0.09457334 -0.04797403  0.07604115]\n",
      " [-0.01145841  0.09422099  0.0228348  -0.0737927  -0.09304728]\n",
      " [ 0.04411954 -0.05775057 -0.03653186 -0.01963471  0.00696618]\n",
      " [ 0.04907392 -0.05254832  0.04624232  0.05302818  0.07153329]\n",
      " [ 0.02980683 -0.04240824  0.00894926 -0.01469061 -0.04689118]\n",
      " [ 0.02944772 -0.07359739 -0.08694568  0.08451695  0.05592877]\n",
      " [-0.02912858 -0.05228103 -0.08211414  0.00088732  0.05638252]\n",
      " [-0.0470668  -0.0191412   0.06401668  0.0731197   0.03618431]\n",
      " [-0.03545923 -0.04984386 -0.0784125   0.08363613  0.07555562]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([ 2, 13,  6, 11])\n",
      "pos_labels = !!!\n",
      "tensor([[ 1,  3],\n",
      "        [12, 14],\n",
      "        [ 0,  7],\n",
      "        [10, 12]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 0,  5, 10,  7, 11,  2],\n",
      "        [ 2,  9,  2,  6,  1, 15],\n",
      "        [ 8, 13, 12,  4, 15,  8],\n",
      "        [10, 14, 15,  3,  6, 15]])\n",
      "batch_size = 4\n",
      "@@@input_embedding = @@@\n",
      "tensor([[-0.0038,  0.0757,  0.0819, -0.0716,  0.0085],\n",
      "        [ 0.0988,  0.0707, -0.0522, -0.0850,  0.0970],\n",
      "        [ 0.0174,  0.0827, -0.0328, -0.0259, -0.0266],\n",
      "        [-0.0349,  0.0244, -0.0698, -0.0821,  0.0336]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[ 0.0060, -0.0221, -0.0186,  0.0858, -0.0929],\n",
      "         [-0.0216,  0.0444,  0.0076,  0.0780,  0.0574]],\n",
      "\n",
      "        [[ 0.0298, -0.0424,  0.0089, -0.0147, -0.0469],\n",
      "         [-0.0291, -0.0523, -0.0821,  0.0009,  0.0564]],\n",
      "\n",
      "        [[ 0.0166,  0.0768, -0.0378,  0.0770,  0.0647],\n",
      "         [ 0.0846, -0.0773, -0.0758, -0.0831,  0.0922]],\n",
      "\n",
      "        [[ 0.0441, -0.0578, -0.0365, -0.0196,  0.0070],\n",
      "         [ 0.0298, -0.0424,  0.0089, -0.0147, -0.0469]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[ 0.0166,  0.0768, -0.0378,  0.0770,  0.0647],\n",
      "         [-0.0383,  0.0008, -0.0239, -0.0262, -0.0301],\n",
      "         [ 0.0441, -0.0578, -0.0365, -0.0196,  0.0070],\n",
      "         [ 0.0846, -0.0773, -0.0758, -0.0831,  0.0922],\n",
      "         [ 0.0491, -0.0525,  0.0462,  0.0530,  0.0715],\n",
      "         [-0.0005, -0.0258,  0.0609,  0.0655,  0.0326]],\n",
      "\n",
      "        [[-0.0005, -0.0258,  0.0609,  0.0655,  0.0326],\n",
      "         [-0.0115,  0.0942,  0.0228, -0.0738, -0.0930],\n",
      "         [-0.0005, -0.0258,  0.0609,  0.0655,  0.0326],\n",
      "         [-0.0132, -0.0249,  0.0625, -0.0799, -0.0522],\n",
      "         [ 0.0060, -0.0221, -0.0186,  0.0858, -0.0929],\n",
      "         [-0.0471, -0.0191,  0.0640,  0.0731,  0.0362]],\n",
      "\n",
      "        [[ 0.0985,  0.0154, -0.0946, -0.0480,  0.0760],\n",
      "         [ 0.0294, -0.0736, -0.0869,  0.0845,  0.0559],\n",
      "         [ 0.0298, -0.0424,  0.0089, -0.0147, -0.0469],\n",
      "         [-0.0012,  0.0760, -0.0046, -0.0002, -0.0692],\n",
      "         [-0.0471, -0.0191,  0.0640,  0.0731,  0.0362],\n",
      "         [ 0.0985,  0.0154, -0.0946, -0.0480,  0.0760]],\n",
      "\n",
      "        [[ 0.0441, -0.0578, -0.0365, -0.0196,  0.0070],\n",
      "         [-0.0291, -0.0523, -0.0821,  0.0009,  0.0564],\n",
      "         [-0.0471, -0.0191,  0.0640,  0.0731,  0.0362],\n",
      "         [-0.0216,  0.0444,  0.0076,  0.0780,  0.0574],\n",
      "         [-0.0132, -0.0249,  0.0625, -0.0799, -0.0522],\n",
      "         [-0.0471, -0.0191,  0.0640,  0.0731,  0.0362]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[-0.0038],\n",
      "         [ 0.0757],\n",
      "         [ 0.0819],\n",
      "         [-0.0716],\n",
      "         [ 0.0085]],\n",
      "\n",
      "        [[ 0.0988],\n",
      "         [ 0.0707],\n",
      "         [-0.0522],\n",
      "         [-0.0850],\n",
      "         [ 0.0970]],\n",
      "\n",
      "        [[ 0.0174],\n",
      "         [ 0.0827],\n",
      "         [-0.0328],\n",
      "         [-0.0259],\n",
      "         [-0.0266]],\n",
      "\n",
      "        [[-0.0349],\n",
      "         [ 0.0244],\n",
      "         [-0.0698],\n",
      "         [-0.0821],\n",
      "         [ 0.0336]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[[-0.0102],\n",
      "         [-0.0010]],\n",
      "\n",
      "        [[-0.0038],\n",
      "         [ 0.0031]],\n",
      "\n",
      "        [[ 0.0042],\n",
      "         [-0.0027]],\n",
      "\n",
      "        [[ 0.0014],\n",
      "         [-0.0031]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[-0.0102, -0.0010],\n",
      "        [-0.0038,  0.0031],\n",
      "        [ 0.0042, -0.0027],\n",
      "        [ 0.0014, -0.0031]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[ 0.0023,  0.0001,  0.0061,  0.0056,  0.0036,  0.0014],\n",
      "        [ 0.0075, -0.0016,  0.0075,  0.0046,  0.0163,  0.0121],\n",
      "        [-0.0053,  0.0064,  0.0017, -0.0083,  0.0074, -0.0053],\n",
      "        [-0.0014, -0.0073,  0.0081,  0.0032, -0.0003,  0.0081]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[-0.0102, -0.0010],\n",
      "        [-0.0038,  0.0031],\n",
      "        [ 0.0042, -0.0027],\n",
      "        [ 0.0014, -0.0031]], grad_fn=<SqueezeBackward0>)\n",
      "######################log_pos = #######################\n",
      "tensor([-1.3919, -1.3867, -1.3856, -1.3871], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1494, -4.1358, -4.1606, -4.1538], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5377, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.1055897  -0.0506419  -0.08735865 -0.06078062  0.01116944]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00375843  0.0756731   0.08186354 -0.07156494  0.00854443]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01736518  0.08271069 -0.03283432 -0.02591122 -0.02655793]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.0349428   0.02440172 -0.06975438 -0.08209933  0.03363008]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.09879393  0.07071277 -0.05220384 -0.08503453  0.09695413]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.0166407   0.07676    -0.03776741  0.0770471   0.06467059]\n",
      " [ 0.00599215 -0.02212358 -0.0186288   0.08580237 -0.09291606]\n",
      " [-0.00053044 -0.02582051  0.06089797  0.06554975  0.03255947]\n",
      " [-0.02162817  0.04438926  0.00762072  0.07799052  0.05736412]\n",
      " [-0.00122205  0.0759678  -0.00458695 -0.00024939 -0.06918435]\n",
      " [-0.03827189  0.00079108 -0.02387001 -0.02622643 -0.03007462]\n",
      " [-0.01323914 -0.02489641  0.06248308 -0.07989885 -0.05219588]\n",
      " [ 0.08455473 -0.07731277 -0.07576446 -0.08307074  0.09222048]\n",
      " [ 0.09846405  0.01544593 -0.09457334 -0.04797403  0.07604115]\n",
      " [-0.01145841  0.09422099  0.0228348  -0.0737927  -0.09304728]\n",
      " [ 0.04411954 -0.05775057 -0.03653186 -0.01963471  0.00696618]\n",
      " [ 0.04907392 -0.05254832  0.04624232  0.05302818  0.07153329]\n",
      " [ 0.02980683 -0.04240824  0.00894926 -0.01469061 -0.04689118]\n",
      " [ 0.02944772 -0.07359739 -0.08694568  0.08451695  0.05592877]\n",
      " [-0.02912858 -0.05228103 -0.08211414  0.00088732  0.05638252]\n",
      " [-0.0470668  -0.0191412   0.06401668  0.0731197   0.03618431]\n",
      " [-0.03545923 -0.04984386 -0.0784125   0.08363613  0.07555562]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.1055897  -0.0506419  -0.08735865 -0.06078062  0.01116944]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01716778  0.07693382 -0.04063079  0.07818773  0.06379466]\n",
      " [ 0.003448   -0.02197555 -0.01527736  0.08611269 -0.09510546]\n",
      " [-0.00535778 -0.03123346  0.06145323  0.07157348  0.02751642]\n",
      " [-0.02084999  0.04567298  0.01140946  0.07824972  0.05673842]\n",
      " [-0.00165797  0.0738915  -0.00376271  0.00040106 -0.06851766]\n",
      " [-0.03817793 -0.00110063 -0.02591646 -0.02443742 -0.03028822]\n",
      " [-0.01482961 -0.02727031  0.0655293  -0.07572508 -0.05545504]\n",
      " [ 0.08508315 -0.07712866 -0.07862727 -0.08193532  0.09134261]\n",
      " [ 0.09759349  0.01129941 -0.09292726 -0.04667503  0.07737257]\n",
      " [-0.01393022  0.09245176  0.02414093 -0.07166514 -0.09547306]\n",
      " [ 0.04421447 -0.05963754 -0.03856973 -0.01784805  0.006752  ]\n",
      " [ 0.04916771 -0.05443678  0.04419938  0.05481412  0.07132006]\n",
      " [ 0.03097271 -0.04209212  0.00671531 -0.01822893 -0.04295726]\n",
      " [ 0.02901497 -0.07565854 -0.08612745  0.08516266  0.0565906 ]\n",
      " [-0.02578581 -0.05112822 -0.08166699  0.00082472  0.0579588 ]\n",
      " [-0.04821421 -0.02417368  0.06960539  0.07996655  0.03276185]\n",
      " [-0.03545923 -0.04984386 -0.0784125   0.08363613  0.07555562]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "input_labels = !!!\n",
      "tensor([ 0,  1,  5, 15])\n",
      "pos_labels = !!!\n",
      "tensor([[16,  1],\n",
      "        [ 0,  2],\n",
      "        [ 4,  0],\n",
      "        [14, 16]])\n",
      "neg_labels = !!!\n",
      "tensor([[10, 13,  0,  2,  6, 16],\n",
      "        [ 8,  0,  6,  1,  2, 12],\n",
      "        [12, 10,  7,  7,  0,  5],\n",
      "        [10, 12,  5, 13, 10,  0]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.1055897  -0.0506419  -0.08735865 -0.06078062  0.01116944]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01716778  0.07693382 -0.04063079  0.07818773  0.06379466]\n",
      " [ 0.003448   -0.02197555 -0.01527736  0.08611269 -0.09510546]\n",
      " [-0.00535778 -0.03123346  0.06145323  0.07157348  0.02751642]\n",
      " [-0.02084999  0.04567298  0.01140946  0.07824972  0.05673842]\n",
      " [-0.00165797  0.0738915  -0.00376271  0.00040106 -0.06851766]\n",
      " [-0.03817793 -0.00110063 -0.02591646 -0.02443742 -0.03028822]\n",
      " [-0.01482961 -0.02727031  0.0655293  -0.07572508 -0.05545504]\n",
      " [ 0.08508315 -0.07712866 -0.07862727 -0.08193532  0.09134261]\n",
      " [ 0.09759349  0.01129941 -0.09292726 -0.04667503  0.07737257]\n",
      " [-0.01393022  0.09245176  0.02414093 -0.07166514 -0.09547306]\n",
      " [ 0.04421447 -0.05963754 -0.03856973 -0.01784805  0.006752  ]\n",
      " [ 0.04916771 -0.05443678  0.04419938  0.05481412  0.07132006]\n",
      " [ 0.03097271 -0.04209212  0.00671531 -0.01822893 -0.04295726]\n",
      " [ 0.02901497 -0.07565854 -0.08612745  0.08516266  0.0565906 ]\n",
      " [-0.02578581 -0.05112822 -0.08166699  0.00082472  0.0579588 ]\n",
      " [-0.04821421 -0.02417368  0.06960539  0.07996655  0.03276185]\n",
      " [-0.03545923 -0.04984386 -0.0784125   0.08363613  0.07555562]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([ 0,  1,  5, 15])\n",
      "pos_labels = !!!\n",
      "tensor([[16,  1],\n",
      "        [ 0,  2],\n",
      "        [ 4,  0],\n",
      "        [14, 16]])\n",
      "neg_labels = !!!\n",
      "tensor([[10, 13,  0,  2,  6, 16],\n",
      "        [ 8,  0,  6,  1,  2, 12],\n",
      "        [12, 10,  7,  7,  0,  5],\n",
      "        [10, 12,  5, 13, 10,  0]])\n",
      "batch_size = 4\n",
      "@@@input_embedding = @@@\n",
      "tensor([[-0.1056, -0.0506, -0.0874, -0.0608,  0.0112],\n",
      "        [ 0.0766,  0.0003, -0.0115,  0.0879,  0.0858],\n",
      "        [-0.0744,  0.0603,  0.0795, -0.0003,  0.0055],\n",
      "        [-0.0351, -0.0288, -0.0197,  0.0805, -0.0555]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[-0.0355, -0.0498, -0.0784,  0.0836,  0.0756],\n",
      "         [ 0.0034, -0.0220, -0.0153,  0.0861, -0.0951]],\n",
      "\n",
      "        [[ 0.0172,  0.0769, -0.0406,  0.0782,  0.0638],\n",
      "         [-0.0054, -0.0312,  0.0615,  0.0716,  0.0275]],\n",
      "\n",
      "        [[-0.0017,  0.0739, -0.0038,  0.0004, -0.0685],\n",
      "         [ 0.0172,  0.0769, -0.0406,  0.0782,  0.0638]],\n",
      "\n",
      "        [[-0.0258, -0.0511, -0.0817,  0.0008,  0.0580],\n",
      "         [-0.0355, -0.0498, -0.0784,  0.0836,  0.0756]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[ 0.0442, -0.0596, -0.0386, -0.0178,  0.0068],\n",
      "         [ 0.0290, -0.0757, -0.0861,  0.0852,  0.0566],\n",
      "         [ 0.0172,  0.0769, -0.0406,  0.0782,  0.0638],\n",
      "         [-0.0054, -0.0312,  0.0615,  0.0716,  0.0275],\n",
      "         [-0.0148, -0.0273,  0.0655, -0.0757, -0.0555],\n",
      "         [-0.0355, -0.0498, -0.0784,  0.0836,  0.0756]],\n",
      "\n",
      "        [[ 0.0976,  0.0113, -0.0929, -0.0467,  0.0774],\n",
      "         [ 0.0172,  0.0769, -0.0406,  0.0782,  0.0638],\n",
      "         [-0.0148, -0.0273,  0.0655, -0.0757, -0.0555],\n",
      "         [ 0.0034, -0.0220, -0.0153,  0.0861, -0.0951],\n",
      "         [-0.0054, -0.0312,  0.0615,  0.0716,  0.0275],\n",
      "         [ 0.0310, -0.0421,  0.0067, -0.0182, -0.0430]],\n",
      "\n",
      "        [[ 0.0310, -0.0421,  0.0067, -0.0182, -0.0430],\n",
      "         [ 0.0442, -0.0596, -0.0386, -0.0178,  0.0068],\n",
      "         [ 0.0851, -0.0771, -0.0786, -0.0819,  0.0913],\n",
      "         [ 0.0851, -0.0771, -0.0786, -0.0819,  0.0913],\n",
      "         [ 0.0172,  0.0769, -0.0406,  0.0782,  0.0638],\n",
      "         [-0.0382, -0.0011, -0.0259, -0.0244, -0.0303]],\n",
      "\n",
      "        [[ 0.0442, -0.0596, -0.0386, -0.0178,  0.0068],\n",
      "         [ 0.0310, -0.0421,  0.0067, -0.0182, -0.0430],\n",
      "         [-0.0382, -0.0011, -0.0259, -0.0244, -0.0303],\n",
      "         [ 0.0290, -0.0757, -0.0861,  0.0852,  0.0566],\n",
      "         [ 0.0442, -0.0596, -0.0386, -0.0178,  0.0068],\n",
      "         [ 0.0172,  0.0769, -0.0406,  0.0782,  0.0638]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[-0.1056],\n",
      "         [-0.0506],\n",
      "         [-0.0874],\n",
      "         [-0.0608],\n",
      "         [ 0.0112]],\n",
      "\n",
      "        [[ 0.0766],\n",
      "         [ 0.0003],\n",
      "         [-0.0115],\n",
      "         [ 0.0879],\n",
      "         [ 0.0858]],\n",
      "\n",
      "        [[-0.0744],\n",
      "         [ 0.0603],\n",
      "         [ 0.0795],\n",
      "         [-0.0003],\n",
      "         [ 0.0055]],\n",
      "\n",
      "        [[-0.0351],\n",
      "         [-0.0288],\n",
      "         [-0.0197],\n",
      "         [ 0.0805],\n",
      "         [-0.0555]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[[ 0.0089],\n",
      "         [-0.0042]],\n",
      "\n",
      "        [[ 0.0142],\n",
      "         [ 0.0075]],\n",
      "\n",
      "        [[ 0.0039],\n",
      "         [ 0.0005]],\n",
      "\n",
      "        [[ 0.0008],\n",
      "         [ 0.0068]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[ 0.0089, -0.0042],\n",
      "        [ 0.0142,  0.0075],\n",
      "        [ 0.0039,  0.0005],\n",
      "        [ 0.0008,  0.0068]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[-0.0029, -0.0037,  0.0062,  0.0073, -0.0012, -0.0089],\n",
      "        [-0.0111, -0.0142,  0.0133,  0.0002, -0.0075,  0.0030],\n",
      "        [ 0.0045,  0.0099,  0.0167,  0.0167, -0.0005, -0.0006],\n",
      "        [ 0.0009, -0.0009, -0.0016, -0.0066,  0.0009, -0.0007]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[ 0.0089, -0.0042],\n",
      "        [ 0.0142,  0.0075],\n",
      "        [ 0.0039,  0.0005],\n",
      "        [ 0.0008,  0.0068]], grad_fn=<SqueezeBackward0>)\n",
      "######################log_pos = #######################\n",
      "tensor([-1.3840, -1.3755, -1.3841, -1.3825], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1605, -4.1671, -4.1356, -4.1629], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5380, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.1055897  -0.0506419  -0.08735865 -0.06078062  0.01116944]\n",
      " [ 0.07659252  0.00032632 -0.01152079  0.08793088  0.08579083]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07438231  0.060264    0.07945493 -0.00026568  0.0054783 ]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03510799 -0.02875551 -0.01970096  0.08050252 -0.05546057]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01716778  0.07693382 -0.04063079  0.07818773  0.06379466]\n",
      " [ 0.003448   -0.02197555 -0.01527736  0.08611269 -0.09510546]\n",
      " [-0.00535778 -0.03123346  0.06145323  0.07157348  0.02751642]\n",
      " [-0.02084999  0.04567298  0.01140946  0.07824972  0.05673842]\n",
      " [-0.00165797  0.0738915  -0.00376271  0.00040106 -0.06851766]\n",
      " [-0.03817793 -0.00110063 -0.02591646 -0.02443742 -0.03028822]\n",
      " [-0.01482961 -0.02727031  0.0655293  -0.07572508 -0.05545504]\n",
      " [ 0.08508315 -0.07712866 -0.07862727 -0.08193532  0.09134261]\n",
      " [ 0.09759349  0.01129941 -0.09292726 -0.04667503  0.07737257]\n",
      " [-0.01393022  0.09245176  0.02414093 -0.07166514 -0.09547306]\n",
      " [ 0.04421447 -0.05963754 -0.03856973 -0.01784805  0.006752  ]\n",
      " [ 0.04916771 -0.05443678  0.04419938  0.05481412  0.07132006]\n",
      " [ 0.03097271 -0.04209212  0.00671531 -0.01822893 -0.04295726]\n",
      " [ 0.02901497 -0.07565854 -0.08612745  0.08516266  0.0565906 ]\n",
      " [-0.02578581 -0.05112822 -0.08166699  0.00082472  0.0579588 ]\n",
      " [-0.04821421 -0.02417368  0.06960539  0.07996655  0.03276185]\n",
      " [-0.03545923 -0.04984386 -0.0784125   0.08363613  0.07555562]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.10725258 -0.0482505  -0.08675849 -0.06216525  0.00629544]\n",
      " [ 0.07364302  0.0022982  -0.01059497  0.089246    0.08864496]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07956024  0.06849203  0.0847001   0.00531577  0.00089992]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03982215 -0.02724058 -0.01811096  0.08047485 -0.0536498 ]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.02065112  0.07891428 -0.03795772  0.07765811  0.06487289]\n",
      " [-0.00111197 -0.02325243 -0.01717793  0.08239187 -0.09697025]\n",
      " [-0.00274204 -0.02997207  0.06363143  0.07307094  0.02722206]\n",
      " [-0.02084999  0.04567298  0.01140946  0.07824972  0.05673842]\n",
      " [-0.0035139   0.07539516 -0.00178021  0.00039444 -0.06838097]\n",
      " [-0.03543946 -0.00188818 -0.02741047 -0.02644495 -0.02903759]\n",
      " [-0.01409034 -0.0260116   0.06800069 -0.07638828 -0.05786494]\n",
      " [ 0.08877121 -0.0801167  -0.08256683 -0.08192215  0.09107099]\n",
      " [ 0.09566806  0.0112912  -0.09263764 -0.04888548  0.07521592]\n",
      " [-0.01393022  0.09245176  0.02414093 -0.07166514 -0.09547306]\n",
      " [ 0.05046298 -0.05843167 -0.03737454 -0.02034307  0.00910788]\n",
      " [ 0.04916771 -0.05443678  0.04419938  0.05481412  0.07132006]\n",
      " [ 0.03179421 -0.04288423  0.00551378 -0.02243074 -0.04384831]\n",
      " [ 0.03254025 -0.07366887 -0.08344524  0.08466585  0.05770191]\n",
      " [-0.02666314 -0.0518468  -0.08215931  0.00283645  0.05657286]\n",
      " [-0.04821421 -0.02417368  0.06960539  0.07996655  0.03276185]\n",
      " [-0.03631052 -0.05054908 -0.07888396  0.08565538  0.07417132]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "input_labels = !!!\n",
      "tensor([10,  9, 14,  7])\n",
      "pos_labels = !!!\n",
      "tensor([[ 9, 11],\n",
      "        [ 8, 10],\n",
      "        [13, 15],\n",
      "        [ 6,  8]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 3,  7,  1, 10,  9, 12],\n",
      "        [12,  2, 10,  9,  0,  4],\n",
      "        [16, 16,  8,  0, 12,  3],\n",
      "        [14,  0,  2, 12, 11,  1]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.10725258 -0.0482505  -0.08675849 -0.06216525  0.00629544]\n",
      " [ 0.07364302  0.0022982  -0.01059497  0.089246    0.08864496]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07956024  0.06849203  0.0847001   0.00531577  0.00089992]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03982215 -0.02724058 -0.01811096  0.08047485 -0.0536498 ]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.02065112  0.07891428 -0.03795772  0.07765811  0.06487289]\n",
      " [-0.00111197 -0.02325243 -0.01717793  0.08239187 -0.09697025]\n",
      " [-0.00274204 -0.02997207  0.06363143  0.07307094  0.02722206]\n",
      " [-0.02084999  0.04567298  0.01140946  0.07824972  0.05673842]\n",
      " [-0.0035139   0.07539516 -0.00178021  0.00039444 -0.06838097]\n",
      " [-0.03543946 -0.00188818 -0.02741047 -0.02644495 -0.02903759]\n",
      " [-0.01409034 -0.0260116   0.06800069 -0.07638828 -0.05786494]\n",
      " [ 0.08877121 -0.0801167  -0.08256683 -0.08192215  0.09107099]\n",
      " [ 0.09566806  0.0112912  -0.09263764 -0.04888548  0.07521592]\n",
      " [-0.01393022  0.09245176  0.02414093 -0.07166514 -0.09547306]\n",
      " [ 0.05046298 -0.05843167 -0.03737454 -0.02034307  0.00910788]\n",
      " [ 0.04916771 -0.05443678  0.04419938  0.05481412  0.07132006]\n",
      " [ 0.03179421 -0.04288423  0.00551378 -0.02243074 -0.04384831]\n",
      " [ 0.03254025 -0.07366887 -0.08344524  0.08466585  0.05770191]\n",
      " [-0.02666314 -0.0518468  -0.08215931  0.00283645  0.05657286]\n",
      " [-0.04821421 -0.02417368  0.06960539  0.07996655  0.03276185]\n",
      " [-0.03631052 -0.05054908 -0.07888396  0.08565538  0.07417132]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([10,  9, 14,  7])\n",
      "pos_labels = !!!\n",
      "tensor([[ 9, 11],\n",
      "        [ 8, 10],\n",
      "        [13, 15],\n",
      "        [ 6,  8]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 3,  7,  1, 10,  9, 12],\n",
      "        [12,  2, 10,  9,  0,  4],\n",
      "        [16, 16,  8,  0, 12,  3],\n",
      "        [14,  0,  2, 12, 11,  1]])\n",
      "batch_size = 4\n",
      "@@@input_embedding = @@@\n",
      "tensor([[-0.0835,  0.0179, -0.0034, -0.0780, -0.1041],\n",
      "        [ 0.0815,  0.0878, -0.0809, -0.0317,  0.0076],\n",
      "        [ 0.0112, -0.0281, -0.0450,  0.0899, -0.0153],\n",
      "        [ 0.0834,  0.0037,  0.1014,  0.0695, -0.0661]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[-0.0139,  0.0925,  0.0241, -0.0717, -0.0955],\n",
      "         [ 0.0492, -0.0544,  0.0442,  0.0548,  0.0713]],\n",
      "\n",
      "        [[ 0.0957,  0.0113, -0.0926, -0.0489,  0.0752],\n",
      "         [ 0.0505, -0.0584, -0.0374, -0.0203,  0.0091]],\n",
      "\n",
      "        [[ 0.0325, -0.0737, -0.0834,  0.0847,  0.0577],\n",
      "         [-0.0482, -0.0242,  0.0696,  0.0800,  0.0328]],\n",
      "\n",
      "        [[-0.0141, -0.0260,  0.0680, -0.0764, -0.0579],\n",
      "         [ 0.0957,  0.0113, -0.0926, -0.0489,  0.0752]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[-0.0208,  0.0457,  0.0114,  0.0782,  0.0567],\n",
      "         [ 0.0888, -0.0801, -0.0826, -0.0819,  0.0911],\n",
      "         [-0.0011, -0.0233, -0.0172,  0.0824, -0.0970],\n",
      "         [ 0.0505, -0.0584, -0.0374, -0.0203,  0.0091],\n",
      "         [-0.0139,  0.0925,  0.0241, -0.0717, -0.0955],\n",
      "         [ 0.0318, -0.0429,  0.0055, -0.0224, -0.0438]],\n",
      "\n",
      "        [[ 0.0318, -0.0429,  0.0055, -0.0224, -0.0438],\n",
      "         [-0.0027, -0.0300,  0.0636,  0.0731,  0.0272],\n",
      "         [ 0.0505, -0.0584, -0.0374, -0.0203,  0.0091],\n",
      "         [-0.0139,  0.0925,  0.0241, -0.0717, -0.0955],\n",
      "         [ 0.0207,  0.0789, -0.0380,  0.0777,  0.0649],\n",
      "         [-0.0035,  0.0754, -0.0018,  0.0004, -0.0684]],\n",
      "\n",
      "        [[-0.0363, -0.0505, -0.0789,  0.0857,  0.0742],\n",
      "         [-0.0363, -0.0505, -0.0789,  0.0857,  0.0742],\n",
      "         [ 0.0957,  0.0113, -0.0926, -0.0489,  0.0752],\n",
      "         [ 0.0207,  0.0789, -0.0380,  0.0777,  0.0649],\n",
      "         [ 0.0318, -0.0429,  0.0055, -0.0224, -0.0438],\n",
      "         [-0.0208,  0.0457,  0.0114,  0.0782,  0.0567]],\n",
      "\n",
      "        [[-0.0267, -0.0518, -0.0822,  0.0028,  0.0566],\n",
      "         [ 0.0207,  0.0789, -0.0380,  0.0777,  0.0649],\n",
      "         [-0.0027, -0.0300,  0.0636,  0.0731,  0.0272],\n",
      "         [ 0.0318, -0.0429,  0.0055, -0.0224, -0.0438],\n",
      "         [ 0.0492, -0.0544,  0.0442,  0.0548,  0.0713],\n",
      "         [-0.0011, -0.0233, -0.0172,  0.0824, -0.0970]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[-0.0835],\n",
      "         [ 0.0179],\n",
      "         [-0.0034],\n",
      "         [-0.0780],\n",
      "         [-0.1041]],\n",
      "\n",
      "        [[ 0.0815],\n",
      "         [ 0.0878],\n",
      "         [-0.0809],\n",
      "         [-0.0317],\n",
      "         [ 0.0076]],\n",
      "\n",
      "        [[ 0.0112],\n",
      "         [-0.0281],\n",
      "         [-0.0450],\n",
      "         [ 0.0899],\n",
      "         [-0.0153]],\n",
      "\n",
      "        [[ 0.0834],\n",
      "         [ 0.0037],\n",
      "         [ 0.1014],\n",
      "         [ 0.0695],\n",
      "         [-0.0661]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[[ 0.0183],\n",
      "         [-0.0169]],\n",
      "\n",
      "        [[ 0.0184],\n",
      "         [ 0.0027]],\n",
      "\n",
      "        [[ 0.0129],\n",
      "         [ 0.0037]],\n",
      "\n",
      "        [[ 0.0041],\n",
      "         [-0.0097]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[ 0.0183, -0.0169],\n",
      "        [ 0.0184,  0.0027],\n",
      "        [ 0.0129,  0.0037],\n",
      "        [ 0.0041, -0.0097]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[ 9.4912e-03,  1.1651e-02, -3.3983e-03,  4.4933e-03, -1.8267e-02,\n",
      "         -2.8716e-03],\n",
      "        [ 1.2398e-03,  1.0116e-02, -2.7212e-03, -6.5824e-03, -9.7119e-03,\n",
      "         -5.9489e-03],\n",
      "        [-1.1136e-02, -1.1136e-02,  6.2437e-04, -5.7142e-03,  3.5351e-05,\n",
      "         -4.1396e-03],\n",
      "        [ 1.4287e-02,  7.1943e-04, -9.3884e-03, -4.3890e-03, -7.4719e-03,\n",
      "         -1.0217e-02]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[ 0.0183, -0.0169],\n",
      "        [ 0.0184,  0.0027],\n",
      "        [ 0.0129,  0.0037],\n",
      "        [ 0.0041, -0.0097]], grad_fn=<SqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################log_pos = #######################\n",
      "tensor([-1.3857, -1.3758, -1.3780, -1.3891], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1584, -4.1657, -4.1747, -4.1672], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5486, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.10725258 -0.0482505  -0.08675849 -0.06216525  0.00629544]\n",
      " [ 0.07364302  0.0022982  -0.01059497  0.089246    0.08864496]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07956024  0.06849203  0.0847001   0.00531577  0.00089992]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08339272  0.00374357  0.10135104  0.06951786 -0.06610727]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08154672  0.08782614 -0.08089297 -0.03173636  0.00757106]\n",
      " [-0.08350182  0.0179262  -0.00343599 -0.0780361  -0.10408156]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.01115721 -0.02809668 -0.04503313  0.08992811 -0.01529059]\n",
      " [-0.03982215 -0.02724058 -0.01811096  0.08047485 -0.0536498 ]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.02065112  0.07891428 -0.03795772  0.07765811  0.06487289]\n",
      " [-0.00111197 -0.02325243 -0.01717793  0.08239187 -0.09697025]\n",
      " [-0.00274204 -0.02997207  0.06363143  0.07307094  0.02722206]\n",
      " [-0.02084999  0.04567298  0.01140946  0.07824972  0.05673842]\n",
      " [-0.0035139   0.07539516 -0.00178021  0.00039444 -0.06838097]\n",
      " [-0.03543946 -0.00188818 -0.02741047 -0.02644495 -0.02903759]\n",
      " [-0.01409034 -0.0260116   0.06800069 -0.07638828 -0.05786494]\n",
      " [ 0.08877121 -0.0801167  -0.08256683 -0.08192215  0.09107099]\n",
      " [ 0.09566806  0.0112912  -0.09263764 -0.04888548  0.07521592]\n",
      " [-0.01393022  0.09245176  0.02414093 -0.07166514 -0.09547306]\n",
      " [ 0.05046298 -0.05843167 -0.03737454 -0.02034307  0.00910788]\n",
      " [ 0.04916771 -0.05443678  0.04419938  0.05481412  0.07132006]\n",
      " [ 0.03179421 -0.04288423  0.00551378 -0.02243074 -0.04384831]\n",
      " [ 0.03254025 -0.07366887 -0.08344524  0.08466585  0.05770191]\n",
      " [-0.02666314 -0.0518468  -0.08215931  0.00283645  0.05657286]\n",
      " [-0.04821421 -0.02417368  0.06960539  0.07996655  0.03276185]\n",
      " [-0.03631052 -0.05054908 -0.07888396  0.08565538  0.07417132]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.10725258 -0.0482505  -0.08675849 -0.06216525  0.00629544]\n",
      " [ 0.07364302  0.0022982  -0.01059497  0.089246    0.08864496]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07956024  0.06849203  0.0847001   0.00531577  0.00089992]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08365669  0.0064706   0.10129436  0.05965371 -0.06762496]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08310555  0.08373493 -0.08451287 -0.03436622  0.01233155]\n",
      " [-0.08597033  0.02047984  0.00065999 -0.07752654 -0.10261613]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.00940675 -0.03032099 -0.0385595   0.08759516 -0.02060052]\n",
      " [-0.03982215 -0.02724058 -0.01811096  0.08047485 -0.0536498 ]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01623875  0.07731884 -0.03732939  0.07446343  0.06671814]\n",
      " [-0.00111634 -0.02379541 -0.0196386   0.08259927 -0.09270266]\n",
      " [-0.006865   -0.03225065  0.06309786  0.07211423  0.02869418]\n",
      " [-0.01905186  0.04593083  0.01262311  0.0779385   0.05971117]\n",
      " [-0.00555863  0.07319298  0.00024813  0.0011902  -0.06857081]\n",
      " [-0.03543946 -0.00188818 -0.02741047 -0.02644495 -0.02903759]\n",
      " [-0.01200984 -0.0259182   0.07052923 -0.07465393 -0.0595142 ]\n",
      " [ 0.09084659 -0.08056225 -0.08248144 -0.07998262  0.09365787]\n",
      " [ 0.0995041   0.01426289 -0.09096976 -0.05017268  0.07412487]\n",
      " [-0.01593747  0.09024069  0.02617148 -0.07083348 -0.09561543]\n",
      " [ 0.05254029 -0.0588848  -0.03728333 -0.01839439  0.01170356]\n",
      " [ 0.04496988 -0.05407877  0.04156951  0.05110226  0.07035485]\n",
      " [ 0.02947903 -0.04491872  0.00620734 -0.02367404 -0.03939313]\n",
      " [ 0.03281737 -0.07436676 -0.0845638   0.08689953  0.05732211]\n",
      " [-0.02873306 -0.05193972 -0.08467498  0.00111091  0.05821374]\n",
      " [-0.0479358  -0.0248748   0.06848164  0.08221059  0.03238029]\n",
      " [-0.03687149 -0.04913642 -0.07661977  0.08113393  0.07494011]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "input_labels = !!!\n",
      "tensor([ 8, 16])\n",
      "pos_labels = !!!\n",
      "tensor([[ 7,  9],\n",
      "        [15,  0]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 9,  5,  1,  8,  1, 14],\n",
      "        [ 1,  3, 13,  0,  2,  8]])\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "[[-0.10725258 -0.0482505  -0.08675849 -0.06216525  0.00629544]\n",
      " [ 0.07364302  0.0022982  -0.01059497  0.089246    0.08864496]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07956024  0.06849203  0.0847001   0.00531577  0.00089992]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08365669  0.0064706   0.10129436  0.05965371 -0.06762496]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08310555  0.08373493 -0.08451287 -0.03436622  0.01233155]\n",
      " [-0.08597033  0.02047984  0.00065999 -0.07752654 -0.10261613]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.00940675 -0.03032099 -0.0385595   0.08759516 -0.02060052]\n",
      " [-0.03982215 -0.02724058 -0.01811096  0.08047485 -0.0536498 ]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01623875  0.07731884 -0.03732939  0.07446343  0.06671814]\n",
      " [-0.00111634 -0.02379541 -0.0196386   0.08259927 -0.09270266]\n",
      " [-0.006865   -0.03225065  0.06309786  0.07211423  0.02869418]\n",
      " [-0.01905186  0.04593083  0.01262311  0.0779385   0.05971117]\n",
      " [-0.00555863  0.07319298  0.00024813  0.0011902  -0.06857081]\n",
      " [-0.03543946 -0.00188818 -0.02741047 -0.02644495 -0.02903759]\n",
      " [-0.01200984 -0.0259182   0.07052923 -0.07465393 -0.0595142 ]\n",
      " [ 0.09084659 -0.08056225 -0.08248144 -0.07998262  0.09365787]\n",
      " [ 0.0995041   0.01426289 -0.09096976 -0.05017268  0.07412487]\n",
      " [-0.01593747  0.09024069  0.02617148 -0.07083348 -0.09561543]\n",
      " [ 0.05254029 -0.0588848  -0.03728333 -0.01839439  0.01170356]\n",
      " [ 0.04496988 -0.05407877  0.04156951  0.05110226  0.07035485]\n",
      " [ 0.02947903 -0.04491872  0.00620734 -0.02367404 -0.03939313]\n",
      " [ 0.03281737 -0.07436676 -0.0845638   0.08689953  0.05732211]\n",
      " [-0.02873306 -0.05193972 -0.08467498  0.00111091  0.05821374]\n",
      " [-0.0479358  -0.0248748   0.06848164  0.08221059  0.03238029]\n",
      " [-0.03687149 -0.04913642 -0.07661977  0.08113393  0.07494011]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "EmbeddingModel forward\n",
      "input_labels = !!!\n",
      "tensor([ 8, 16])\n",
      "pos_labels = !!!\n",
      "tensor([[ 7,  9],\n",
      "        [15,  0]])\n",
      "neg_labels = !!!\n",
      "tensor([[ 9,  5,  1,  8,  1, 14],\n",
      "        [ 1,  3, 13,  0,  2,  8]])\n",
      "batch_size = 2\n",
      "@@@input_embedding = @@@\n",
      "tensor([[-0.0984, -0.0861, -0.0099, -0.0681,  0.0789],\n",
      "        [-0.0910,  0.0128,  0.0523, -0.0018, -0.0541]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@pos_embedding = @@@\n",
      "tensor([[[ 0.0908, -0.0806, -0.0825, -0.0800,  0.0937],\n",
      "         [-0.0159,  0.0902,  0.0262, -0.0708, -0.0956]],\n",
      "\n",
      "        [[-0.0479, -0.0249,  0.0685,  0.0822,  0.0324],\n",
      "         [ 0.0162,  0.0773, -0.0373,  0.0745,  0.0667]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "@@@neg_embedding = @@@\n",
      "tensor([[[-0.0159,  0.0902,  0.0262, -0.0708, -0.0956],\n",
      "         [-0.0354, -0.0019, -0.0274, -0.0264, -0.0290],\n",
      "         [-0.0011, -0.0238, -0.0196,  0.0826, -0.0927],\n",
      "         [ 0.0995,  0.0143, -0.0910, -0.0502,  0.0741],\n",
      "         [-0.0011, -0.0238, -0.0196,  0.0826, -0.0927],\n",
      "         [-0.0287, -0.0519, -0.0847,  0.0011,  0.0582]],\n",
      "\n",
      "        [[-0.0011, -0.0238, -0.0196,  0.0826, -0.0927],\n",
      "         [-0.0191,  0.0459,  0.0126,  0.0779,  0.0597],\n",
      "         [ 0.0328, -0.0744, -0.0846,  0.0869,  0.0573],\n",
      "         [ 0.0162,  0.0773, -0.0373,  0.0745,  0.0667],\n",
      "         [-0.0069, -0.0323,  0.0631,  0.0721,  0.0287],\n",
      "         [ 0.0995,  0.0143, -0.0910, -0.0502,  0.0741]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "input_embedding.unsqueeze(2) = ***\n",
      "tensor([[[-0.0984],\n",
      "         [-0.0861],\n",
      "         [-0.0099],\n",
      "         [-0.0681],\n",
      "         [ 0.0789]],\n",
      "\n",
      "        [[-0.0910],\n",
      "         [ 0.0128],\n",
      "         [ 0.0523],\n",
      "         [-0.0018],\n",
      "         [-0.0541]]], grad_fn=<UnsqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[[ 0.0117],\n",
      "         [-0.0092]],\n",
      "\n",
      "        [[ 0.0057],\n",
      "         [-0.0062]]], grad_fn=<BmmBackward>)\n",
      "@@@log_pos.squeeze() == ***\n",
      "tensor([[ 0.0117, -0.0092],\n",
      "        [ 0.0057, -0.0062]], grad_fn=<SqueezeBackward0>)\n",
      "@@@log_neg = @@@\n",
      "tensor([[ 0.0092, -0.0034,  0.0106,  0.0008,  0.0106, -0.0127],\n",
      "        [-0.0036,  0.0004,  0.0116,  0.0062, -0.0018,  0.0176]],\n",
      "       grad_fn=<SqueezeBackward0>)\n",
      "@@@log_pos = @@@\n",
      "tensor([[ 0.0117, -0.0092],\n",
      "        [ 0.0057, -0.0062]], grad_fn=<SqueezeBackward0>)\n",
      "######################log_pos = #######################\n",
      "tensor([-1.3851, -1.3865], grad_fn=<SumBackward1>)\n",
      "######################log_neg = #######################\n",
      "tensor([-4.1514, -4.1438], grad_fn=<SumBackward1>)\n",
      "loss = !!!\n",
      "tensor(5.5334, grad_fn=<MeanBackward0>)\n",
      "#################################after backward and step#######################################################\n",
      "[[-0.10725258 -0.0482505  -0.08675849 -0.06216525  0.00629544]\n",
      " [ 0.07364302  0.0022982  -0.01059497  0.089246    0.08864496]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07956024  0.06849203  0.0847001   0.00531577  0.00089992]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08365669  0.0064706   0.10129436  0.05965371 -0.06762496]\n",
      " [-0.09838018 -0.08609484 -0.00992148 -0.06808605  0.07892119]\n",
      " [ 0.08310555  0.08373493 -0.08451287 -0.03436622  0.01233155]\n",
      " [-0.08597033  0.02047984  0.00065999 -0.07752654 -0.10261613]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.00940675 -0.03032099 -0.0385595   0.08759516 -0.02060052]\n",
      " [-0.03982215 -0.02724058 -0.01811096  0.08047485 -0.0536498 ]\n",
      " [-0.09098653  0.0128199   0.0523078  -0.00175718 -0.05411736]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01623875  0.07731884 -0.03732939  0.07446343  0.06671814]\n",
      " [-0.00111634 -0.02379541 -0.0196386   0.08259927 -0.09270266]\n",
      " [-0.006865   -0.03225065  0.06309786  0.07211423  0.02869418]\n",
      " [-0.01905186  0.04593083  0.01262311  0.0779385   0.05971117]\n",
      " [-0.00555863  0.07319298  0.00024813  0.0011902  -0.06857081]\n",
      " [-0.03543946 -0.00188818 -0.02741047 -0.02644495 -0.02903759]\n",
      " [-0.01200984 -0.0259182   0.07052923 -0.07465393 -0.0595142 ]\n",
      " [ 0.09084659 -0.08056225 -0.08248144 -0.07998262  0.09365787]\n",
      " [ 0.0995041   0.01426289 -0.09096976 -0.05017268  0.07412487]\n",
      " [-0.01593747  0.09024069  0.02617148 -0.07083348 -0.09561543]\n",
      " [ 0.05254029 -0.0588848  -0.03728333 -0.01839439  0.01170356]\n",
      " [ 0.04496988 -0.05407877  0.04156951  0.05110226  0.07035485]\n",
      " [ 0.02947903 -0.04491872  0.00620734 -0.02367404 -0.03939313]\n",
      " [ 0.03281737 -0.07436676 -0.0845638   0.08689953  0.05732211]\n",
      " [-0.02873306 -0.05193972 -0.08467498  0.00111091  0.05821374]\n",
      " [-0.0479358  -0.0248748   0.06848164  0.08221059  0.03238029]\n",
      " [-0.03687149 -0.04913642 -0.07661977  0.08113393  0.07494011]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n",
      "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
      "[[-0.10725258 -0.0482505  -0.08675849 -0.06216525  0.00629544]\n",
      " [ 0.07364302  0.0022982  -0.01059497  0.089246    0.08864496]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07956024  0.06849203  0.0847001   0.00531577  0.00089992]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08365669  0.0064706   0.10129436  0.05965371 -0.06762496]\n",
      " [-0.09551293 -0.08569594 -0.00187606 -0.07653442  0.08757468]\n",
      " [ 0.08310555  0.08373493 -0.08451287 -0.03436622  0.01233155]\n",
      " [-0.08597033  0.02047984  0.00065999 -0.07752654 -0.10261613]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.00940675 -0.03032099 -0.0385595   0.08759516 -0.02060052]\n",
      " [-0.03982215 -0.02724058 -0.01811096  0.08047485 -0.0536498 ]\n",
      " [-0.09858242  0.01510332  0.06161764 -0.0111112  -0.05878298]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "[[ 0.01621064  0.0773228  -0.03731323  0.07446288  0.06670141]\n",
      " [ 0.01322721 -0.01587366 -0.02127186  0.08945985 -0.09784221]\n",
      " [-0.0023115  -0.03289223  0.06048007  0.07220217  0.03140252]\n",
      " [-0.01450341  0.04528995  0.01000822  0.07802635  0.06241652]\n",
      " [-0.00555863  0.07319298  0.00024813  0.0011902  -0.06857081]\n",
      " [-0.03051202  0.00242394 -0.02691355 -0.02303481 -0.03299042]\n",
      " [-0.01200984 -0.0259182   0.07052923 -0.07465393 -0.0595142 ]\n",
      " [ 0.08595625 -0.08484191 -0.08297462 -0.08336708  0.09758093]\n",
      " [ 0.10893042  0.01793044 -0.09306633 -0.04668273  0.07286261]\n",
      " [-0.01598264  0.09020116  0.02616693 -0.07086474 -0.09557919]\n",
      " [ 0.05254029 -0.0588848  -0.03728333 -0.01839439  0.01170356]\n",
      " [ 0.04496988 -0.05407877  0.04156951  0.05110226  0.07035485]\n",
      " [ 0.02947903 -0.04491872  0.00620734 -0.02367404 -0.03939313]\n",
      " [ 0.03734028 -0.07500403 -0.087164    0.08698688  0.06001227]\n",
      " [-0.02378292 -0.04760774 -0.08417577  0.00453676  0.0542427 ]\n",
      " [-0.05247209 -0.02423564  0.07108954  0.08212299  0.02968218]\n",
      " [-0.03687149 -0.04913642 -0.07661977  0.08113393  0.07494011]\n",
      " [-0.05682831  0.03508819  0.01669896  0.09786547  0.08000911]\n",
      " [ 0.04595221 -0.09996235 -0.0082884   0.01977041  0.06560748]\n",
      " [ 0.07173664  0.07786878 -0.05261946  0.0753581   0.04336775]]\n"
     ]
    }
   ],
   "source": [
    "for e in range(NUM_EPOCHS):\n",
    "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
    "        #这里的enumerate(dataloader)使用__getitem__(self,idx)提取对应的值\n",
    "        #!!!注意这里是dataloader，而不是对应的dataset数组\n",
    "        #这里面是按照batch提取的，一次提取多个batch，这里的batch应该是打乱顺序进行随机抽取的\n",
    "        \n",
    "        #本来dataset中的数值的形状为([],[6],[60])\n",
    "        #经过dataloader处理之后的对应数值为([128],[128,6],[128,60])\n",
    "        #(打乱顺序随机取出对应的128个数值)\n",
    "        input_labels = input_labels.long()\n",
    "        pos_labels = pos_labels.long()\n",
    "        neg_labels = neg_labels.long()\n",
    "        print('input_labels = !!!')\n",
    "        print(input_labels)\n",
    "        print('pos_labels = !!!')\n",
    "        print(pos_labels)\n",
    "        print('neg_labels = !!!')\n",
    "        print(neg_labels)\n",
    "        print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!before backward and step!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "        print(model.input_embeddings())\n",
    "        print('----------------------------------------------------------------------------------------------------------------')\n",
    "        print(model.output_embeddings())\n",
    "        optimizer.zero_grad()\n",
    "        #!!!注意每次optimizer需要将梯度降为0\n",
    "        loss = model(input_labels, pos_labels, neg_labels).mean() \n",
    "        # model返回的是一个batch所有样本的损失，需要求个平均\n",
    "        #这里求.mean()损失函数的平均的时候\n",
    "        print('loss = !!!')\n",
    "        print(loss)\n",
    "        print('#################################after backward and step#######################################################')\n",
    "        print(model.input_embeddings())\n",
    "        print('---------------------------------------------------------------------------------------------------------------')\n",
    "        print(model.output_embeddings())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #注意这里w不是直接减去ita*loss\n",
    "        #而是w减去ita*(损失函数对自变量的导)\n",
    "        #这里有个反向求导的对应的过程loss.backward(),反向求导之后再进行相应的\n",
    "        #优化\n",
    "        #出现次数较多的关联项下降的比较快，出席次数较少的关联项下降的比较慢\n",
    "        #本质上就是当前这个对应的变量上升相应的梯度，其他变量下降相应的梯度\n",
    "        print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@after backward and step@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "        print(model.input_embeddings())\n",
    "        print('--------------------------------------------------------------------------------------------------------------')\n",
    "        print(model.output_embeddings())\n",
    "        #函数之中定义了对应的input_embeddings()的内容为in_embed的相应的内容\n",
    "        if i % 100 == 0:\n",
    "            with open(LOG_FILE, \"a\") as fout: \n",
    "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
    "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
    "        \n",
    "        #if i % 2000 == 0: \n",
    "        #    embedding_weights = model.input_embeddings()  # 取出训练中的in_embed词向量\n",
    "            # 在三个词文本上评估词向量\n",
    "        #    sim_simlex = evaluate(\"simlex-999.txt\", embedding_weights)\n",
    "        #    sim_men = evaluate(\"men.txt\", embedding_weights)\n",
    "        #    sim_353 = evaluate(\"wordsim353.csv\", embedding_weights)\n",
    "            \n",
    "            #with open(LOG_FILE, \"a\") as fout:\n",
    "            #    print(\"epoch: {}, iter: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "            #        e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "            #    fout.write(\"epoch: {}, iter: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
    "            #        e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
    "                \n",
    "    embedding_weights = model.input_embeddings() # 调用最终训练好的embeding词向量\n",
    "    np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights) # 保存参数\n",
    "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE)) # 保存参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 5)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10725258 -0.0482505  -0.08675849 -0.06216525  0.00629544]\n",
      " [ 0.07364302  0.0022982  -0.01059497  0.089246    0.08864496]\n",
      " [-0.00802662  0.07961414  0.08324979 -0.06912722  0.00171028]\n",
      " [-0.04294299  0.09924994 -0.03610822 -0.04722053  0.0394378 ]\n",
      " [ 0.074784   -0.04972069  0.09695041 -0.02280641  0.00028184]\n",
      " [-0.07956024  0.06849203  0.0847001   0.00531577  0.00089992]\n",
      " [ 0.01468543  0.08337899 -0.03046825 -0.0272178  -0.025835  ]\n",
      " [ 0.08365669  0.0064706   0.10129436  0.05965371 -0.06762496]\n",
      " [-0.09551293 -0.08569594 -0.00187606 -0.07653442  0.08757468]\n",
      " [ 0.08310555  0.08373493 -0.08451287 -0.03436622  0.01233155]\n",
      " [-0.08597033  0.02047984  0.00065999 -0.07752654 -0.10261613]\n",
      " [-0.03025249  0.02512149 -0.07240909 -0.08607884  0.02911239]\n",
      " [ 0.09610046 -0.04913365 -0.03815091 -0.03238004  0.09234068]\n",
      " [ 0.10047776  0.06891955 -0.06032153 -0.08875097  0.10059984]\n",
      " [ 0.00940675 -0.03032099 -0.0385595   0.08759516 -0.02060052]\n",
      " [-0.03982215 -0.02724058 -0.01811096  0.08047485 -0.0536498 ]\n",
      " [-0.09858242  0.01510332  0.06161764 -0.0111112  -0.05878298]\n",
      " [ 0.04802681  0.09893809  0.07467414  0.01711309  0.09297407]\n",
      " [-0.02750699 -0.05791422 -0.04138596 -0.09514274 -0.06904038]\n",
      " [-0.01448403  0.01326855  0.0960081   0.02331076 -0.00194278]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_to_idx为单词对应的编号，embedding_weight为单词对应编号的相应的矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 计算两个文本对应的余弦向量的值\n",
    " from sklearn.metrics.pairwise import cosine_similarity #余弦相似度函数\n",
    " word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    " word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    " model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "  human_similarity.append(float(data.iloc[i, 2]))\n",
    "            # 这个是人类统计得到的相似度\n",
    "  return scipy.stats.spearmanr(human_similarity, model_similarity)# , model_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果判断两个句子是否相同，可以用model_similarity中的所有数平方加和得到最终的内容"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
